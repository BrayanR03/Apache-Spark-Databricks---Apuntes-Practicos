{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5a20393c-40f7-4656-a26f-ba04955d9b47",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### üìò Talleres de Ingenier√≠a de Datos con Pyspark en Databricks üêçüß±"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0cfb42a0-7319-463b-a13c-60f10cc2cd04",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "üë®‚Äçüíª Autor: Brayan Neciosup  \n",
    "üìç Portafolio: [brayanneciosup](https://bryanneciosup626.wixsite.com/brayandataanalitics)  \n",
    "üîó LinkedIn: [linkedin.com/brayanneciosup](https://www.linkedin.com/in/brayan-rafael-neciosup-bola%C3%B1os-407a59246/)  \n",
    "üíª GitHub: [github.com/BrayanR03](https://github.com/BrayanR03)  \n",
    "üìö Serie: Fundamentos de Apache Spark - PySpark \n",
    "üìì Estos talleres constar√°n de 3 niveles (B√°sico-Intermedio-Avanzado)   \n",
    "üîç Abarcar√° temas desde Fundamentos de Data Wrangling hacia Casos de Uso Avanzado   \n",
    "üìù Cada ejercicio presenta su enunciado, dataset, resultado esperado y soluci√≥n.   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c4522c20-2e22-46d0-88c6-0f0bebfe08d6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### FUNDAMENTOS DE DATA WRANGLING (MANIPULACI√ìN DE DATOS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25f1d0b0-f046-42d2-a32a-92e388f9a21d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "spark = SparkSession.builder.appName(\"TalleresPySparkDE\").getOrCreate()\n",
    "### üí° CABE RESALTAR QUE LOS DATASETS UTILIZADOS ESTAN ALMACENADOS EN UNITY CATALOG."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d953466b-3ec1-4626-ae09-9f261d383d85",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### ü•â NIVEL B√ÅSICO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73350ec6-4165-41c6-80d6-63f997fd376c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "1. Detecci√≥n de valores nulos en columnas principales\n",
    "\n",
    "üóÉÔ∏è Dataset: TITANIC\n",
    "üóíÔ∏è Enunciado: Identifica cu√°ntos valores faltantes hay en las columnas age, embarked y deck.\n",
    "‚úçÔ∏è Resultado esperado: un conteo por columna con la cantidad de valores nulos.\n",
    "\n",
    "\"\"\"\n",
    "## ‚úîÔ∏è SOLUCI√ìN\n",
    "df_uno = spark.sql(\"SELECT * FROM workspace.exercises.titanic\")\n",
    "# df_uno.show(5)\n",
    "cantidad_datos_nulos = df_uno.select([\n",
    "  sum(when(col(c).isNull(),1).otherwise(0)).alias(c)\n",
    "  for c in df_uno.columns\n",
    "])\n",
    "cantidad_datos_nulos.show() ## ‚û°Ô∏è Cantidad de datos nulos: age(177) - embarked(2) - deck(688)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "52bd62dc-6436-4eb7-8a94-8848908e2b5f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "2. Eliminaci√≥n de filas duplicadas\n",
    "\n",
    "üóÉÔ∏è Dataset: Diccionario\n",
    "üóíÔ∏è Enunciado: Elimina las filas duplicadas y conserva solo la primera aparici√≥n de cada registro.\n",
    "‚úçÔ∏è Resultado esperado: un DataFrame sin filas repetidas.\n",
    "\n",
    "\"\"\"\n",
    "## ‚úîÔ∏è Soluci√≥n\n",
    "dict_data = {\n",
    " \"id\": [1,2,2,3,4,4,5],\n",
    " \"nombre\": [\"Ana\",\"Luis\",\"Luis\",\"Mar√≠a\",\"Pedro\",\"Pedro\",\"Sof√≠a\"],\n",
    " \"edad\": [23,30,30,22,40,40,29]\n",
    "}\n",
    "df_dos = spark.createDataFrame(data=list(zip(*dict_data.values())),schema=list(dict_data.keys()))\n",
    "# df_dos.show()\n",
    "print(df_dos.count()) ## Cantidad de datos originales: 7\n",
    "df_dos_clean = df_dos.drop_duplicates(subset=[\"id\",\"nombre\",\"edad\"])\n",
    "# df_dos_clean.show()\n",
    "print(df_dos_clean.count()) ## Cantidad de datos despu√©s de eliminar nulos: 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f3794ff3-1222-4443-8da6-19838d6847fd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "3. Reemplazo simple de valores faltantes\n",
    "\n",
    "üóÉÔ∏è Dataset: PENGUINS\n",
    "üóíÔ∏è Enunciado: Reemplaza los valores nulos en la columna bill_length_mm con la media de esa misma columna.\n",
    "‚úçÔ∏è Resultado esperado: columna sin valores nulos en bill_length_mm.\n",
    "\n",
    "\"\"\"\n",
    "## ‚úîÔ∏è Soluci√≥n\n",
    "df_tres = spark.sql(\"SELECT * FROM workspace.exercises.penguins\")\n",
    "# df_tres.show(5)\n",
    "cantidad_nulos_df_tres = df_tres.select([\n",
    "    sum(when(col(c).isNull(),1).otherwise(0)).alias(c)\n",
    "    for c in df_tres.columns\n",
    "    if c=='bill_length_mm'\n",
    "])\n",
    "# cantidad_nulos_df_tres.show() ## Cantidad de nulos: 2\n",
    "media_columna_bill_length_mm = df_tres.select(round(mean(col(\"bill_length_mm\")),2)).collect()[0][0]\n",
    "media_columna_bill_length_mm ## Valor de la media: 43.92\n",
    "df_tres = df_tres.fillna(value=media_columna_bill_length_mm,subset=[\"bill_length_mm\"])\n",
    "\n",
    "cantidad_nulos_df_tres = df_tres.select([\n",
    "    sum(when(col(c).isNull(),1).otherwise(0)).alias(c)\n",
    "    for c in df_tres.columns\n",
    "    if c=='bill_length_mm'\n",
    "])\n",
    "cantidad_nulos_df_tres.show() ## Cantidad de nulos: 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "98c26d0a-42cc-44b5-8239-4700a606ca1f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### ü•à NIVEL INTERMEDIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33860441-6be8-4518-8e9e-a0ec71ac8a42",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\"\"\"\n",
    "4. Imputaci√≥n condicional de valores faltantes\n",
    "\n",
    "üóÉÔ∏è Dataset: TITANIC\n",
    "üóíÔ∏è Enunciado: Completa los valores faltantes de age con la edad promedio por clase (pclass).\n",
    "‚úçÔ∏è Resultado esperado: columna age sin valores nulos, imputada seg√∫n clase de pasajero.\n",
    "\n",
    "\"\"\"\n",
    "## ‚úîÔ∏è Soluci√≥n\n",
    "df_cuatro = spark.sql(\"SELECT * FROM workspace.exercises.titanic\")\n",
    "# df_cuatro.show()\n",
    "cantidad_datos_nulos_df_cuatro = df_cuatro.select([\n",
    "    sum(when(col(c).isNull(),1).otherwise(0)).alias(c)\n",
    "    for c in df_cuatro.columns\n",
    "])\n",
    "# cantidad_datos_nulos_df_cuatro.select(col(\"age\")).show() ## Cantidad de nulos: 177\n",
    "promedio_edad_pclass = df_cuatro.groupBy(col(\"pclass\")).agg(\n",
    "    round(avg(col(\"age\")),2).alias(\"avg_edad_pclass\")\n",
    ")\n",
    "# promedio_edad_pclass.show()\n",
    "def valor_avg_edad_pclass(pclass):\n",
    "    return promedio_edad_pclass.filter(\n",
    "        col(\"pclass\")==pclass\n",
    "    ).collect()[0][1]\n",
    "df_cuatro_clean = df_cuatro.withColumn(\n",
    "    \"age\",\n",
    "    when(\n",
    "        (col(\"age\").isNull()==True) & (col(\"pclass\")==1),lit(valor_avg_edad_pclass(1))\n",
    "    ).when(\n",
    "        (col(\"age\").isNull()==True) & (col(\"pclass\")==2),lit(valor_avg_edad_pclass(2))\n",
    "    ).when(\n",
    "        (col(\"age\").isNull()==True) & (col(\"pclass\")==3),lit(valor_avg_edad_pclass(3))\n",
    "    ).otherwise(col(\"age\"))\n",
    ")\n",
    "# df_cuatro_clean.show(5)\n",
    "cantidad_datos_nulos_df_cuatro_clean = df_cuatro_clean.select([\n",
    "    sum(when(col(c).isNull(),1).otherwise(0)).alias(c)\n",
    "    for c in df_cuatro_clean.columns\n",
    "])\n",
    "# cantidad_datos_nulos_df_cuatro_clean.show() ## Cantidad de nulos: 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b60910a-9452-4715-b378-2bae2427f9ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "5. Detecci√≥n de outliers usando IQR\n",
    "\n",
    "üóÉÔ∏è Dataset: Diccionario\n",
    "üóíÔ∏è Enunciado: Identifica los valores de ventas que son outliers seg√∫n el rango intercuart√≠lico (IQR).\n",
    "‚úçÔ∏è Resultado esperado: listado de los productos que presentan valores an√≥malos.\n",
    "\n",
    "\"\"\"\n",
    "## ‚úîÔ∏è Soluci√≥n\n",
    "diccionario_cinco = {\n",
    " \"producto\": [\"A\",\"B\",\"C\",\"D\",\"E\",\"F\"],\n",
    " \"ventas\": [120, 130, 115, 1000, 140, 135]\n",
    "}\n",
    "\n",
    "df_cinco = spark.createDataFrame(data=list(zip(*diccionario_cinco.values())),schema=list(diccionario_cinco.keys()))\n",
    "# df_cinco.show()\n",
    "df_cinco_quartiles = df_cinco.agg(\n",
    "    expr('percentile(ventas,array(0.25))')[0].alias(\"q1\"),\n",
    "    expr('percentile(ventas,array(0.75))')[0].alias(\"q3\")\n",
    ")\n",
    "# df_cinco_quartiles.show()\n",
    "q1_ventas = df_cinco_quartiles.select(col(\"q1\")).collect()[0][0]\n",
    "# q1_ventas\n",
    "q3_ventas = df_cinco_quartiles.select(col(\"q3\")).collect()[0][0]\n",
    "# q3_ventas\n",
    "iqr_ventas = q3_ventas - q1_ventas\n",
    "lower_bound_ventas = q1_ventas - 1.5 * iqr_ventas\n",
    "upper_bound_ventas = q3_ventas + 1.5 * iqr_ventas\n",
    "# print(lower_bound_ventas)\n",
    "# print(upper_bound_ventas)\n",
    "df_cinco_outliers = df_cinco.filter(\n",
    "    (col(\"ventas\")<lower_bound_ventas) |\n",
    "    (col(\"ventas\")>upper_bound_ventas) \n",
    ")\n",
    "# df_cinco_outliers.show() ## Valores outilers\n",
    "# df_cinco_outliers.count() ## Cantidad de valores outilers: 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5f10427-dffd-4ee0-b174-8c293117bf0d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "6. Eliminaci√≥n selectiva de duplicados\n",
    "\n",
    "üóÉÔ∏è Dataset: Diamonds\n",
    "üóíÔ∏è Enunciado: En el dataset de diamantes, elimina duplicados basados solo en las columnas carat y price.\n",
    "‚úçÔ∏è Resultado esperado: DataFrame sin duplicados en esas dos columnas, pero manteniendo el resto de filas.\n",
    "\n",
    "\"\"\"\n",
    "## ‚úîÔ∏è Soluci√≥n\n",
    "df_seis = spark.sql(\"SELECT * FROM workspace.exercises.df_diamonds\")\n",
    "# df_seis.show(5)\n",
    "# df_seis.count() ## Cantidad inicial de datos: 53940\n",
    "df_seis_clean = df_seis.dropDuplicates(subset=[\"carat\",\"price\"])\n",
    "# df_seis_clean.show(5)\n",
    "df_seis_clean.count() ## Cantidad despu√©s de eliminar duplicados: 28988"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "08765b5b-f587-41e4-80d5-3cc9455817bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "7. Relleno de valores faltantes con interpolaci√≥n\n",
    "\n",
    "üóÉÔ∏è Dataset: Diccionario\n",
    "üóíÔ∏è Enunciado: Rellena los valores nulos de la columna temperatura mediante la mediana de las temepraturas.\n",
    "‚úçÔ∏è Resultado esperado: columna completa sin valores nulos.\n",
    "\n",
    "\"\"\"\n",
    "## ‚úîÔ∏è Soluci√≥n\n",
    "## Utilizaremos pandas para generar fechas\n",
    "diccionario_siete  = {\n",
    " \"fecha\": [\"2024-01-01\",\"2024-01-02\",\"2024-01-03\",\"2024-01-04\",\"2024-01-05\",\"2024-01-06\",\"2024-01-07\",\"2024-01-08\",\"2024-01-09\",\"2024-01-10\"],\n",
    " \"temperatura\": [21,22,None,24,25,None,None,28,29,30]\n",
    "}\n",
    "df_siete = spark.createDataFrame(data=list(zip(*diccionario_siete.values())),schema=list(diccionario_siete.keys()))\n",
    "# df_siete.show()\n",
    "mediana_ventas = df_siete.select(median(col(\"temperatura\")).alias(\"mediana\")).collect()[0][0]\n",
    "# mediana_ventas\n",
    "df_siete = df_siete.fillna({\"temperatura\":mediana_ventas})\n",
    "df_siete.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "52cb0da3-7514-4180-9b4c-c23c2b156c54",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "8. Conteo de valores faltantes combinados\n",
    "\n",
    "üóÉÔ∏è Dataset: Penguins\n",
    "üóíÔ∏è Enunciado: Calcula el n√∫mero de registros que tienen valores nulos simult√°neamente\n",
    "    en las columnas bill_length_mm y bill_depth_mm.\n",
    "‚úçÔ∏è Resultado esperado: un n√∫mero entero que indique cu√°ntos registros cumplen esta condici√≥n.\n",
    "\n",
    "\"\"\"\n",
    "## ‚úîÔ∏è Soluci√≥n\n",
    "df_ocho = spark.sql(\"SELECT * FROM workspace.exercises.penguins\")\n",
    "# df_ocho.show(5)\n",
    "df_ocho_nulos_simultaneos = df_ocho.filter(\n",
    "    (col(\"bill_length_mm\").isNull() & col(\"bill_depth_mm\").isNull())\n",
    ")\n",
    "# df_ocho_nulos_simultaneos.show() ## Valores nulos simult√°neos en: \"bill_length_mm\" y \"bill_depth_mm\"\n",
    "df_ocho_nulos_simultaneos.count() ## Cantidad de datos que tengan valores nulos simult√°neos: 2"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "talleres_pyspark_databricks_de",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

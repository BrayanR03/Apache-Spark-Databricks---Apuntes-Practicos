{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5a20393c-40f7-4656-a26f-ba04955d9b47",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### üìò Talleres de Ingenier√≠a de Datos con Pyspark en Databricks üêçüß±"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0cfb42a0-7319-463b-a13c-60f10cc2cd04",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "üë®‚Äçüíª Autor: Brayan Neciosup  \n",
    "üìç Portafolio: [brayanneciosup](https://bryanneciosup626.wixsite.com/brayandataanalitics)  \n",
    "üîó LinkedIn: [linkedin.com/brayanneciosup](https://www.linkedin.com/in/brayan-rafael-neciosup-bola%C3%B1os-407a59246/)  \n",
    "üíª GitHub: [github.com/BrayanR03](https://github.com/BrayanR03)  \n",
    "üìö Serie: Fundamentos de Apache Spark - PySpark \n",
    "üìì Estos talleres constar√°n de 3 niveles (B√°sico-Intermedio-Avanzado)   \n",
    "üîç Abarcar√° temas desde Fundamentos de Data Wrangling hacia Casos de Uso Avanzado   \n",
    "üìù Cada ejercicio presenta su enunciado, dataset, resultado esperado y soluci√≥n.   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "23a24d8b-d185-4fab-9dca-d9509b90efce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "spark = SparkSession.builder.appName(\"TalleresPySparkDE\").getOrCreate()\n",
    "### üí° CABE RESALTAR QUE LOS DATASETS UTILIZADOS ESTAN ALMACENADOS EN UNITY CATALOG."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c4522c20-2e22-46d0-88c6-0f0bebfe08d6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### FUNDAMENTOS DE DATA WRANGLING (MANIPULACI√ìN DE DATOS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d953466b-3ec1-4626-ae09-9f261d383d85",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### ü•â NIVEL B√ÅSICO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73350ec6-4165-41c6-80d6-63f997fd376c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "1. Detecci√≥n de valores nulos en columnas principales\n",
    "\n",
    "üóÉÔ∏è Dataset: TITANIC\n",
    "üóíÔ∏è Enunciado: Identifica cu√°ntos valores faltantes hay en las columnas age, embarked y deck.\n",
    "‚úçÔ∏è Resultado esperado: un conteo por columna con la cantidad de valores nulos.\n",
    "\n",
    "\"\"\"\n",
    "## ‚úîÔ∏è SOLUCI√ìN\n",
    "df_uno = spark.sql(\"SELECT * FROM workspace.exercises.titanic\")\n",
    "# df_uno.show(5)\n",
    "cantidad_datos_nulos = df_uno.select([\n",
    "  sum(when(col(c).isNull(),1).otherwise(0)).alias(c)\n",
    "  for c in df_uno.columns\n",
    "])\n",
    "cantidad_datos_nulos.show() ## ‚û°Ô∏è Cantidad de datos nulos: age(177) - embarked(2) - deck(688)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "52bd62dc-6436-4eb7-8a94-8848908e2b5f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "2. Eliminaci√≥n de filas duplicadas\n",
    "\n",
    "üóÉÔ∏è Dataset: Diccionario\n",
    "üóíÔ∏è Enunciado: Elimina las filas duplicadas y conserva solo la primera aparici√≥n de cada registro.\n",
    "‚úçÔ∏è Resultado esperado: un DataFrame sin filas repetidas.\n",
    "\n",
    "\"\"\"\n",
    "## ‚úîÔ∏è Soluci√≥n\n",
    "dict_data = {\n",
    " \"id\": [1,2,2,3,4,4,5],\n",
    " \"nombre\": [\"Ana\",\"Luis\",\"Luis\",\"Mar√≠a\",\"Pedro\",\"Pedro\",\"Sof√≠a\"],\n",
    " \"edad\": [23,30,30,22,40,40,29]\n",
    "}\n",
    "df_dos = spark.createDataFrame(data=list(zip(*dict_data.values())),schema=list(dict_data.keys()))\n",
    "# df_dos.show()\n",
    "print(df_dos.count()) ## Cantidad de datos originales: 7\n",
    "df_dos_clean = df_dos.drop_duplicates(subset=[\"id\",\"nombre\",\"edad\"])\n",
    "# df_dos_clean.show()\n",
    "print(df_dos_clean.count()) ## Cantidad de datos despu√©s de eliminar nulos: 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f3794ff3-1222-4443-8da6-19838d6847fd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "3. Reemplazo simple de valores faltantes\n",
    "\n",
    "üóÉÔ∏è Dataset: PENGUINS\n",
    "üóíÔ∏è Enunciado: Reemplaza los valores nulos en la columna bill_length_mm con la media de esa misma columna.\n",
    "‚úçÔ∏è Resultado esperado: columna sin valores nulos en bill_length_mm.\n",
    "\n",
    "\"\"\"\n",
    "## ‚úîÔ∏è Soluci√≥n\n",
    "df_tres = spark.sql(\"SELECT * FROM workspace.exercises.penguins\")\n",
    "# df_tres.show(5)\n",
    "cantidad_nulos_df_tres = df_tres.select([\n",
    "    sum(when(col(c).isNull(),1).otherwise(0)).alias(c)\n",
    "    for c in df_tres.columns\n",
    "    if c=='bill_length_mm'\n",
    "])\n",
    "# cantidad_nulos_df_tres.show() ## Cantidad de nulos: 2\n",
    "media_columna_bill_length_mm = df_tres.select(round(mean(col(\"bill_length_mm\")),2)).collect()[0][0]\n",
    "media_columna_bill_length_mm ## Valor de la media: 43.92\n",
    "df_tres = df_tres.fillna(value=media_columna_bill_length_mm,subset=[\"bill_length_mm\"])\n",
    "\n",
    "cantidad_nulos_df_tres = df_tres.select([\n",
    "    sum(when(col(c).isNull(),1).otherwise(0)).alias(c)\n",
    "    for c in df_tres.columns\n",
    "    if c=='bill_length_mm'\n",
    "])\n",
    "cantidad_nulos_df_tres.show() ## Cantidad de nulos: 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "98c26d0a-42cc-44b5-8239-4700a606ca1f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### ü•à NIVEL INTERMEDIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33860441-6be8-4518-8e9e-a0ec71ac8a42",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\"\"\"\n",
    "4. Imputaci√≥n condicional de valores faltantes\n",
    "\n",
    "üóÉÔ∏è Dataset: TITANIC\n",
    "üóíÔ∏è Enunciado: Completa los valores faltantes de age con la edad promedio por clase (pclass).\n",
    "‚úçÔ∏è Resultado esperado: columna age sin valores nulos, imputada seg√∫n clase de pasajero.\n",
    "\n",
    "\"\"\"\n",
    "## ‚úîÔ∏è Soluci√≥n\n",
    "df_cuatro = spark.sql(\"SELECT * FROM workspace.exercises.titanic\")\n",
    "# df_cuatro.show()\n",
    "cantidad_datos_nulos_df_cuatro = df_cuatro.select([\n",
    "    sum(when(col(c).isNull(),1).otherwise(0)).alias(c)\n",
    "    for c in df_cuatro.columns\n",
    "])\n",
    "# cantidad_datos_nulos_df_cuatro.select(col(\"age\")).show() ## Cantidad de nulos: 177\n",
    "promedio_edad_pclass = df_cuatro.groupBy(col(\"pclass\")).agg(\n",
    "    round(avg(col(\"age\")),2).alias(\"avg_edad_pclass\")\n",
    ")\n",
    "# promedio_edad_pclass.show()\n",
    "def valor_avg_edad_pclass(pclass):\n",
    "    return promedio_edad_pclass.filter(\n",
    "        col(\"pclass\")==pclass\n",
    "    ).collect()[0][1]\n",
    "df_cuatro_clean = df_cuatro.withColumn(\n",
    "    \"age\",\n",
    "    when(\n",
    "        (col(\"age\").isNull()==True) & (col(\"pclass\")==1),lit(valor_avg_edad_pclass(1))\n",
    "    ).when(\n",
    "        (col(\"age\").isNull()==True) & (col(\"pclass\")==2),lit(valor_avg_edad_pclass(2))\n",
    "    ).when(\n",
    "        (col(\"age\").isNull()==True) & (col(\"pclass\")==3),lit(valor_avg_edad_pclass(3))\n",
    "    ).otherwise(col(\"age\"))\n",
    ")\n",
    "# df_cuatro_clean.show(5)\n",
    "cantidad_datos_nulos_df_cuatro_clean = df_cuatro_clean.select([\n",
    "    sum(when(col(c).isNull(),1).otherwise(0)).alias(c)\n",
    "    for c in df_cuatro_clean.columns\n",
    "])\n",
    "# cantidad_datos_nulos_df_cuatro_clean.show() ## Cantidad de nulos: 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b60910a-9452-4715-b378-2bae2427f9ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "5. Detecci√≥n de outliers usando IQR\n",
    "\n",
    "üóÉÔ∏è Dataset: Diccionario\n",
    "üóíÔ∏è Enunciado: Identifica los valores de ventas que son outliers seg√∫n el rango intercuart√≠lico (IQR).\n",
    "‚úçÔ∏è Resultado esperado: listado de los productos que presentan valores an√≥malos.\n",
    "\n",
    "\"\"\"\n",
    "## ‚úîÔ∏è Soluci√≥n\n",
    "diccionario_cinco = {\n",
    " \"producto\": [\"A\",\"B\",\"C\",\"D\",\"E\",\"F\"],\n",
    " \"ventas\": [120, 130, 115, 1000, 140, 135]\n",
    "}\n",
    "\n",
    "df_cinco = spark.createDataFrame(data=list(zip(*diccionario_cinco.values())),schema=list(diccionario_cinco.keys()))\n",
    "# df_cinco.show()\n",
    "df_cinco_quartiles = df_cinco.agg(\n",
    "    expr('percentile(ventas,array(0.25))')[0].alias(\"q1\"),\n",
    "    expr('percentile(ventas,array(0.75))')[0].alias(\"q3\")\n",
    ")\n",
    "# df_cinco_quartiles.show()\n",
    "q1_ventas = df_cinco_quartiles.select(col(\"q1\")).collect()[0][0]\n",
    "# q1_ventas\n",
    "q3_ventas = df_cinco_quartiles.select(col(\"q3\")).collect()[0][0]\n",
    "# q3_ventas\n",
    "iqr_ventas = q3_ventas - q1_ventas\n",
    "lower_bound_ventas = q1_ventas - 1.5 * iqr_ventas\n",
    "upper_bound_ventas = q3_ventas + 1.5 * iqr_ventas\n",
    "# print(lower_bound_ventas)\n",
    "# print(upper_bound_ventas)\n",
    "df_cinco_outliers = df_cinco.filter(\n",
    "    (col(\"ventas\")<lower_bound_ventas) |\n",
    "    (col(\"ventas\")>upper_bound_ventas) \n",
    ")\n",
    "# df_cinco_outliers.show() ## Valores outilers\n",
    "# df_cinco_outliers.count() ## Cantidad de valores outilers: 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5f10427-dffd-4ee0-b174-8c293117bf0d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "6. Eliminaci√≥n selectiva de duplicados\n",
    "\n",
    "üóÉÔ∏è Dataset: Diamonds\n",
    "üóíÔ∏è Enunciado: En el dataset de diamantes, elimina duplicados basados solo en las columnas carat y price.\n",
    "‚úçÔ∏è Resultado esperado: DataFrame sin duplicados en esas dos columnas, pero manteniendo el resto de filas.\n",
    "\n",
    "\"\"\"\n",
    "## ‚úîÔ∏è Soluci√≥n\n",
    "df_seis = spark.sql(\"SELECT * FROM workspace.exercises.df_diamonds\")\n",
    "# df_seis.show(5)\n",
    "# df_seis.count() ## Cantidad inicial de datos: 53940\n",
    "df_seis_clean = df_seis.dropDuplicates(subset=[\"carat\",\"price\"])\n",
    "# df_seis_clean.show(5)\n",
    "df_seis_clean.count() ## Cantidad despu√©s de eliminar duplicados: 28988"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "08765b5b-f587-41e4-80d5-3cc9455817bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "7. Relleno de valores faltantes con interpolaci√≥n\n",
    "\n",
    "üóÉÔ∏è Dataset: Diccionario\n",
    "üóíÔ∏è Enunciado: Rellena los valores nulos de la columna temperatura mediante la mediana de las temepraturas.\n",
    "‚úçÔ∏è Resultado esperado: columna completa sin valores nulos.\n",
    "\n",
    "\"\"\"\n",
    "## ‚úîÔ∏è Soluci√≥n\n",
    "## Utilizaremos pandas para generar fechas\n",
    "diccionario_siete  = {\n",
    " \"fecha\": [\"2024-01-01\",\"2024-01-02\",\"2024-01-03\",\"2024-01-04\",\"2024-01-05\",\"2024-01-06\",\"2024-01-07\",\"2024-01-08\",\"2024-01-09\",\"2024-01-10\"],\n",
    " \"temperatura\": [21,22,None,24,25,None,None,28,29,30]\n",
    "}\n",
    "df_siete = spark.createDataFrame(data=list(zip(*diccionario_siete.values())),schema=list(diccionario_siete.keys()))\n",
    "# df_siete.show()\n",
    "mediana_ventas = df_siete.select(median(col(\"temperatura\")).alias(\"mediana\")).collect()[0][0]\n",
    "# mediana_ventas\n",
    "df_siete = df_siete.fillna({\"temperatura\":mediana_ventas})\n",
    "df_siete.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "52cb0da3-7514-4180-9b4c-c23c2b156c54",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "8. Conteo de valores faltantes combinados\n",
    "\n",
    "üóÉÔ∏è Dataset: Penguins\n",
    "üóíÔ∏è Enunciado: Calcula el n√∫mero de registros que tienen valores nulos simult√°neamente\n",
    "    en las columnas bill_length_mm y bill_depth_mm.\n",
    "‚úçÔ∏è Resultado esperado: un n√∫mero entero que indique cu√°ntos registros cumplen esta condici√≥n.\n",
    "\n",
    "\"\"\"\n",
    "## ‚úîÔ∏è Soluci√≥n\n",
    "df_ocho = spark.sql(\"SELECT * FROM workspace.exercises.penguins\")\n",
    "# df_ocho.show(5)\n",
    "df_ocho_nulos_simultaneos = df_ocho.filter(\n",
    "    (col(\"bill_length_mm\").isNull() & col(\"bill_depth_mm\").isNull())\n",
    ")\n",
    "# df_ocho_nulos_simultaneos.show() ## Valores nulos simult√°neos en: \"bill_length_mm\" y \"bill_depth_mm\"\n",
    "df_ocho_nulos_simultaneos.count() ## Cantidad de datos que tengan valores nulos simult√°neos: 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e1812251-c0c1-4362-ab41-b4e96c24fb88",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### ü•á NIVEL AVANZADO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "23fff3bb-5b1f-49cf-ae6d-bad30bc271d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "9. Winsorizaci√≥n de outliers\n",
    "\n",
    "üóÉÔ∏è Dataset: Diccionario\n",
    "üóíÔ∏è Enunciado: Aplica winsorizaci√≥n al 5% superior e inferior en la\n",
    "   columna nota para reducir el impacto de valores extremos.\n",
    "‚úçÔ∏è Resultado esperado: columna nota ajustada, sin eliminar registros.\n",
    "\n",
    "üí° La winsorinizaci√≥n permite mejorar la integridad y confiabilidad de datos\n",
    "    evitando valores extremos a los limites de quartil que tiene cada columna.\n",
    "    Por ejemplo: valores mayores a 10, se establecen como 10 y valores\n",
    "    menores a 5, se establecen como 5.\n",
    "\n",
    "\"\"\"\n",
    "## ‚úîÔ∏è Soluci√≥n\n",
    "diccionario_nueve = {\n",
    " \"alumno\": [\"A\",\"B\",\"C\",\"D\",\"E\",\"F\",\"G\"],\n",
    " \"nota\": [10, 11, 75, 100, 11, 13, 200]\n",
    "}\n",
    "df_nueve = spark.createDataFrame(data=list(zip(*diccionario_nueve.values())),schema=list(diccionario_nueve.keys()))\n",
    "# df_nueve.show()\n",
    "df_nueve_quartiles = df_nueve.agg(\n",
    "    expr('percentile(nota,array(0.05))')[0].alias(\"q005\"),\n",
    "    round(expr('percentile(nota,array(0.95))')[0],2).alias(\"q95\")\n",
    ")\n",
    "# df_nueve_quartiles.show()\n",
    "limite_inferior_5_pct = df_nueve_quartiles.select(col(\"q005\")).collect()[0][0]\n",
    "limite_superior_95_pct = df_nueve_quartiles.select(col(\"q95\")).collect()[0][0]\n",
    "# limite_inferior_5_pct\n",
    "# limite_superior_95_pct\n",
    "\n",
    "df_nueve_final = df_nueve.withColumn(\n",
    "    \"nota\",\n",
    "    when(\n",
    "        col(\"nota\")<limite_inferior_5_pct,lit(limite_inferior_5_pct)\n",
    "    ).when(\n",
    "        col(\"nota\")>limite_superior_95_pct,lit(limite_superior_95_pct)\n",
    "    ).otherwise(col(\"nota\"))\n",
    ")\n",
    "df_nueve_final.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "283e103d-125f-4334-bf78-f3b086da9337",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "10. Detecci√≥n de inconsistencias de tipado en columnas\n",
    "\n",
    "üóÉÔ∏è Dataset: Diccionario\n",
    "üóíÔ∏è Enunciado: Identifica las filas con tipos incorrectos  y convi√©rtelas al tipo correcto.\n",
    "‚úçÔ∏è Resultado esperado: Un dataset con tipos de datos correcto en cada columna\n",
    "\n",
    "\"\"\"\n",
    "from pyspark.sql.types import LongType,FloatType\n",
    "## ‚úîÔ∏è Soluci√≥n\n",
    "diccionario_diez = {\n",
    "    \"ID\":[1,\"2\",3,\"004\",\"5\"],\n",
    "    \"Venta\":[\"12254\",1450,1200.00,\"300\",120.00]\n",
    "}\n",
    "df_diez = spark.createDataFrame(data=list(zip(*diccionario_diez.values())),schema=list(diccionario_diez.keys()))\n",
    "# df_diez.show()\n",
    "df_diez_estandarizado = df_diez.withColumns({\n",
    "    \"ID\":\n",
    "    col(\"ID\").cast(LongType()),\n",
    "    \"Venta\":\n",
    "    col(\"Venta\").cast(FloatType())\n",
    "})\n",
    "df_diez.printSchema()\n",
    "df_diez_estandarizado.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b62d21e-0710-4974-9134-fc4194bfb4b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "11. Identificaci√≥n de duplicados aproximados (fuzzy matching)\n",
    "\n",
    "üóÉÔ∏è Dataset: Diccionario\n",
    "üóíÔ∏è Enunciado: Detecta nombres de clientes que parecen duplicados\n",
    "    por errores tipogr√°ficos (ejemplo: \"Luis\" vs \"luiz\").\n",
    "‚úçÔ∏è Resultado esperado: listado de pares de valores sospechosos de ser duplicados.\n",
    "\n",
    "\"\"\"\n",
    "## ‚úîÔ∏è Soluci√≥n\n",
    "diccionario_once = {\n",
    " \"cliente\": [\"Ana\", \"Ana \", \"Luis\", \"Luz\", \"luiz\", \"Pedro\", \"pedroo\"]\n",
    "}\n",
    "import difflib\n",
    "lista_clientes = [i.replace(' ','') for i in diccionario_once[\"cliente\"]]\n",
    "# lista_clientes\n",
    "sospechosos_duplicados = {}\n",
    "for nombre in lista_clientes:\n",
    "    sospechosos = difflib.get_close_matches(nombre,lista_clientes,n=3,cutoff=0.8)\n",
    "    sospechosos_duplicados[nombre]=sospechosos\n",
    "sospechosos_duplicados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "edb42f36-653c-476c-8eb7-123749413d40",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "12. Validaci√≥n y limpieza de rangos v√°lidos\n",
    "\n",
    "üóÉÔ∏è Dataset: Titanic\n",
    "üóíÔ∏è Enunciado: Valida que la columna \"age\" est√© en un rango l√≥gico (0 a 100 a√±os).\n",
    "               Detecta y corrige/descarta valores fuera de rango.\n",
    "‚úçÔ∏è Resultado esperado: columna age sin valores inv√°lidos, garantizando integridad de negocio.\n",
    "\n",
    "\"\"\"\n",
    "## ‚úîÔ∏è Soluci√≥n\n",
    "df_doce = spark.sql(\"SELECT * FROM workspace.exercises.titanic\")\n",
    "# df_doce.show(5)\n",
    "df_doce_clean = df_doce.filter(\n",
    "    (col(\"age\")>0) &\n",
    "    (col(\"age\")<100) \n",
    ")\n",
    "# df_doce.count() ## Cantidad de datos originales: 891\n",
    "# df_doce_clean.count() ## Cantidad de datos filtrados: 714"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be92cbf0-3a8a-437e-bcc0-577692dba0f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "13. Normalizaci√≥n de categor√≠as inconsistentes\n",
    "\n",
    "üóÉÔ∏è Dataset: Diccionario\n",
    "üóíÔ∏è Enunciado: Detecta y unifica las categor√≠as inconsistentes aplicando reglas \n",
    "              de limpieza (case folding, correcci√≥n de errores).\n",
    "‚úçÔ∏è Resultado esperado: Dataset con categor√≠as √∫nicas y estandarizadas.\n",
    "\n",
    "\"\"\"\n",
    "## ‚úîÔ∏è Soluci√≥n\n",
    "diccionario_trece = {\n",
    "    \"Ciudad\":[\"Lima\",\"lima\",\"LIMA\",\"Lma\",]\n",
    "}\n",
    "df_trece = spark.createDataFrame(data=list(zip(*diccionario_trece.values())),schema=list(diccionario_trece.keys()))\n",
    "ciudades_validas = [\"Lima\"]\n",
    "import difflib\n",
    "def ciudad_estandarizada(ciudad):\n",
    "    similar = difflib.get_close_matches(ciudad,ciudades_validas,n=3,cutoff=0.6)\n",
    "    return similar[0] if similar else ciudad\n",
    "from pyspark.sql.types import StringType\n",
    "udf_ciudad = udf(ciudad_estandarizada,StringType())\n",
    "df_trece = df_trece.withColumn(\n",
    "    \"Ciudad_Estandarizada\",\n",
    "    initcap(udf_ciudad(col(\"Ciudad\")))\n",
    ")\n",
    "df_trece.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "790fb074-41a8-4804-a336-2f5039d99005",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### FEATURE ENGINEERING (INGENIER√çA DE CARACTER√çSTICAS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bbfd3ab4-06d6-47f6-9740-b26b8bd72906",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### ü•â NIVEL B√ÅSICO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "59ea1780-6a35-4995-9718-e5909a770803",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "1. Variables dummies simples\n",
    "\n",
    "üóÉÔ∏è Dataset: Titanic\n",
    "üóíÔ∏è Enunciado: Convierte la columna sex en variables dummies\n",
    "‚úçÔ∏è Resultado esperado: Dos columnas adicionales (sex_male, sex_female) con valores binarios 0/1.\n",
    "\n",
    "\"\"\"\n",
    "## ‚úîÔ∏è Soluci√≥n\n",
    "df_uno = spark.sql(\"SELECT * FROM workspace.exercises.titanic\")\n",
    "# df_uno.show(5)\n",
    "# Esto siempre funciona y es m√°s directo\n",
    "df_with_dummies = df_uno.withColumn(\n",
    "    \"sex_male\", \n",
    "    when(\n",
    "        col(\"sex\") == \"male\", 1\n",
    "    ).otherwise(0)\n",
    ").withColumn(\n",
    "    \"sex_female\",\n",
    "    when(\n",
    "        col(\"sex\") == \"female\", 1\n",
    "    ).otherwise(0)\n",
    ")\n",
    "# df_with_dummies.show(5)\n",
    "## üí° Esta es una t√©ncica de pre-procesamiento de datos llamada \"OneHot-Encoding\"\n",
    "##    la cu√°l permite transformar datos en valores √≥ptimos para modelos de Machine Learning (ML)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "acb7787f-bba3-42a0-a3f1-06925b69ee95",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "2. Binning por intervalos fijos\n",
    "\n",
    "üóÉÔ∏è Dataset: Tips\n",
    "üóíÔ∏è Enunciado: Agrupa la columna total_bill en 3 intervalos: bajo(<10), medio(>=10 y <20), alto(>=20).\n",
    "‚úçÔ∏è Resultado esperado: Nueva columna total_bill_bin con categor√≠as: \"Bajo\", \"Medio\", \"Alto\".\n",
    "\n",
    "\"\"\"\n",
    "## ‚úîÔ∏è Soluci√≥n\n",
    "df_dos = spark.sql(\"SELECT * FROM workspace.exercises.df_tips\")\n",
    "# df_dos.show(5)\n",
    "df_dos = df_dos.withColumn(\n",
    "    \"total_bill_bin\",\n",
    "    when(\n",
    "        col(\"total_bill\")<10,lit(\"Bajo\")\n",
    "    ).when(\n",
    "        (col(\"total_bill\")>=10) & (col(\"total_bill\")<20),lit(\"Medio\")\n",
    "    ).otherwise(lit(\"Alto\"))\n",
    ")\n",
    "df_dos.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a2544e82-96b6-415d-93d4-4e5ae0372ec5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "3. Normalizaci√≥n min-max\n",
    "\n",
    "üóÉÔ∏è Dataset: Diccionario\n",
    "üóíÔ∏è Enunciado: Normaliza la columna ventas entre 0 y 1 usando min-max.\n",
    "‚úçÔ∏è Resultado esperado: Nueva columna ventas_norm con valores escalados entre 0 y 1.\n",
    "\n",
    "\"\"\"\n",
    "## ‚úîÔ∏è Soluci√≥n\n",
    "diccionario_tres = {\n",
    "    \"producto\":[\"A\",\"B\",\"C\"],\n",
    "    \"ventas\":[100,300,500]\n",
    "}\n",
    "df_tres = spark.createDataFrame(data=list(zip(*diccionario_tres.values())),schema=list(diccionario_tres.keys()))\n",
    "# df_tres.show()\n",
    "df_tres_max_min = df_tres.agg(\n",
    "    max(col(\"ventas\")).alias(\"max_ventas\"),\n",
    "    min(col(\"ventas\")).alias(\"min_ventas\")\n",
    ")\n",
    "# df_tres_max_min.show()\n",
    "df_tres = df_tres.withColumn(\n",
    "    \"ventas_normalizadas\",\n",
    "    (\n",
    "        (col(\"ventas\")-df_tres_max_min.select(col(\"min_ventas\")).collect()[0][0])/\n",
    "        (df_tres_max_min.select(col(\"max_ventas\")).collect()[0][0] - df_tres_max_min.select(col(\"min_ventas\")).collect()[0][0])\n",
    "    )\n",
    ")\n",
    "df_tres.show()\n",
    "# ## üí° Esta es una t√©ncica de pre-procesamiento de datos llamada \"Normalizaci√≥n Min-Max\"\n",
    "# ##    la cu√°l permite establecer en un rango de 0 y 1 valores que permitan a modelos de ML\n",
    "# ##    aprener los patrones de los datos, pero con variables en escalas comparables mejorando su rendimiento.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "728d3cc0-8784-4e54-8ac7-383c0fa58616",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "4. Extracci√≥n de a√±o y mes de fechas\n",
    "\n",
    "üóÉÔ∏è Dataset: Diccionario\n",
    "üóíÔ∏è Enunciado: Enunciado: Extrae el a√±o y mes de la columna fecha.\n",
    "‚úçÔ∏è Resultado esperado: Dos nuevas columnas: a√±o y mes.\n",
    "\n",
    "\"\"\"\n",
    "## ‚úîÔ∏è Soluci√≥n\n",
    "diccionario_cuatro = {\n",
    "    \"id\":[1,2,3],\n",
    "    \"fecha\":[\"2021-05-12\",\"2022-01-20\",\"2023-07-15\"]\n",
    "}\n",
    "df_cuatro = spark.createDataFrame(data=list(zip(*diccionario_cuatro.values())),schema=list(diccionario_cuatro.keys()))\n",
    "# df_cuatro.show()\n",
    "df_cuatro = df_cuatro.withColumn(\n",
    "    \"fecha\",\n",
    "    to_date(col(\"fecha\")) ## Primero convertimos a un tipo de dato fecha la columna.\n",
    ")\n",
    "df_cuatro = df_cuatro.withColumn(\n",
    "    \"a√±o\",\n",
    "    year(col(\"fecha\"))\n",
    ")\n",
    "# df_cuatro.show()\n",
    "df_cuatro = df_cuatro.withColumn(\n",
    "    \"mes\",\n",
    "    month(col(\"fecha\"))\n",
    ")\n",
    "df_cuatro.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "04f7432b-3ac1-4884-b915-b7d63cb4fb13",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "5. Longitud de cadenas de texto\n",
    "\n",
    "üóÉÔ∏è Dataset: Diccionario\n",
    "üóíÔ∏è Enunciado: Crea una columna con la longitud de caracteres de cada comentario.\n",
    "‚úçÔ∏è Resultado esperado: Columna longitud con n√∫mero de caracteres por fila.\n",
    "\n",
    "\"\"\"\n",
    "## ‚úîÔ∏è Soluci√≥n\n",
    "diccionario_cinco = {\n",
    "    \"comentario\":[\"Excelente servicio\",\"Muy caro\",\"Aceptable\"]\n",
    "}\n",
    "df_cinco = spark.createDataFrame(data=list(zip(*diccionario_cinco.values())),schema=list(diccionario_cinco.keys()))\n",
    "# df_cinco.show()\n",
    "df_cinco = df_cinco.withColumn(\n",
    "    \"longitud_comentario\",\n",
    "    length(col(\"comentario\"))\n",
    ")\n",
    "df_cinco.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c4f8f481-1fe3-4816-b71b-d55376add5aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### ü•à NIVEL INTERMEDIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e84852d-8242-4cce-9b7a-a43e67440ed5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "6. Variables dummies m√∫ltiples\n",
    "\n",
    "üóÉÔ∏è Dataset: Titanic\n",
    "üóíÔ∏è Enunciado: Convierte embarked en variables dummies.\n",
    "‚úçÔ∏è Resultado esperado: Nuevas columnas (embarked_C, embarked_Q, embarked_S) con valores 0/1.\n",
    "\n",
    "\"\"\"\n",
    "## ‚úîÔ∏è Soluci√≥n\n",
    "df_seis = spark.sql(\"SELECT * FROM workspace.exercises.titanic\")\n",
    "# df_seis.show(5)\n",
    "df_seis = df_seis.withColumns({\n",
    "    \"embarked_C\":\n",
    "        when(\n",
    "            col(\"embarked\")==\"C\",lit(1)\n",
    "        ).otherwise(0),\n",
    "    \"embarked_Q\":\n",
    "        when(\n",
    "            col(\"embarked\")==\"Q\",lit(1)\n",
    "        ).otherwise(0),\n",
    "    \"embarked_S\":\n",
    "        when(\n",
    "            col(\"embarked\")==\"S\",lit(1)\n",
    "        ).otherwise(0)\n",
    "})\n",
    "display(df_seis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5762011d-4955-4454-aff5-0b6e87773af8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "7. Binning por cuantiles\n",
    "\n",
    "üóÉÔ∏è Dataset: Diamonds\n",
    "üóíÔ∏è Enunciado: Divide la columna price en 4 categor√≠as seg√∫n sus cuartiles.\n",
    "‚úçÔ∏è Resultado esperado: Nueva columna price_bin con categor√≠as Q1, Q2, Q3, Q4.\n",
    "\n",
    "\"\"\"\n",
    "## ‚úîÔ∏è Soluci√≥n\n",
    "df_siete = spark.sql(\"SELECT * FROM workspace.exercises.df_diamonds\")\n",
    "# df_siete.show(5)\n",
    "valores_quartiles = df_siete.approxQuantile(\"price\",[0.25,0.50,0.75],0.01)\n",
    "# valores_quartiles\n",
    "df_siete = df_siete.withColumn(\n",
    "    \"price_bin\",\n",
    "    when(\n",
    "        col(\"price\")<valores_quartiles[0],lit(\"Q1\")\n",
    "    ).when(\n",
    "        col(\"price\")<valores_quartiles[1],lit(\"Q2\")\n",
    "    ).when(\n",
    "        col(\"price\")<valores_quartiles[2],lit(\"Q3\")\n",
    "    ).otherwise(lit(\"Q4\"))\n",
    ")\n",
    "df_siete.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b7a72556-f915-4652-94ee-9b8890690292",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "8. Estandarizaci√≥n (Z-score)\n",
    "\n",
    "üóÉÔ∏è Dataset: Diccionario\n",
    "üóíÔ∏è Enunciado: Calcula el Z-score de las notas.\n",
    "‚úçÔ∏è Resultado esperado: Nueva columna notas_zscore con valores centrados en media 0 y desviaci√≥n est√°ndar 1.\n",
    "\n",
    "\"\"\"\n",
    "## ‚úîÔ∏è Soluci√≥n\n",
    "\n",
    "lista_ocho = [\n",
    "    12, 14, 13, 15, 16, 11, 14, 13, 15, 12,  # Notas normales\n",
    "    14, 16, 13, 15, 14, 12, 13, 16, 15, 14,  # M√°s notas normales\n",
    "    3,   # Nota muy baja (Outlier)\n",
    "    19   # Nota muy lta (Outlier)\n",
    "]\n",
    "diccionario_ocho = {\n",
    "    \"Estudiante\": [\"Estudiante \"+str(i+1) for i in range(len(lista_ocho))],\n",
    "    \"Nota\":[i for i in lista_ocho]\n",
    "}\n",
    "# diccionario_ocho\n",
    "df_ocho = spark.createDataFrame(data=list(zip(*diccionario_ocho.values())),schema=list(diccionario_ocho.keys()))\n",
    "# df_ocho.show(5)\n",
    "\n",
    "# 1. Calculamos el promedio y la desviaci√≥n est√°ndar de la columna \"nota\"\n",
    "# Esto se hace sobre todo el DataFrame\n",
    "promedio_nota = df_ocho.select(avg(\"Nota\")).first()[0]\n",
    "stddev_nota = df_ocho.select(stddev(\"Nota\")).first()[0]\n",
    "\n",
    "# 2. Realizamos el c√°lculo z-score\n",
    "df_ocho = df_ocho.withColumn(\n",
    "    \"nota-zscore\",\n",
    "    round((col(\"Nota\")-promedio_nota)/stddev_nota,2)\n",
    ")\n",
    "df_ocho.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee71347d-c7e6-445b-8a8d-3e93e87363eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "9. D√≠a de la semana desde fecha\n",
    "\n",
    "üóÉÔ∏è Dataset: Diccionario\n",
    "üóíÔ∏è Enunciado: Extrae el d√≠a de la semana de cada fecha.\n",
    "‚úçÔ∏è Resultado esperado: Columna dia_semana con valores tipo: \"Viernes\", \"S√°bado\", \"Domingo\".\n",
    "\n",
    "\"\"\"\n",
    "## ‚úîÔ∏è Soluci√≥n\n",
    "diccionario_nueve = {\n",
    "    \"id\":[1,2,3],\n",
    "    \"fecha\":[\"2023-05-12\",\"2023-05-13\",\"2023-05-14\"]\n",
    "}\n",
    "df_nueve = spark.createDataFrame(data=list(zip(*diccionario_nueve.values())),schema=list(diccionario_nueve.keys()))\n",
    "# df_nueve.show(5)\n",
    "df_nueve = df_nueve.withColumn(\n",
    "    \"fecha\",\n",
    "    to_date(col(\"fecha\"))\n",
    ")\n",
    "df_nueve = df_nueve.withColumn(\n",
    "    \"dia_semana\",\n",
    "    date_format(col(\"fecha\"),\"EEEE\")\n",
    ")\n",
    "df_nueve.show()\n",
    "## üí° Es importante conocer los diversos formatos de fechas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ca10568-fe8c-4a08-a8f5-38cce28e2433",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "10. Conteo de palabras en texto\n",
    "\n",
    "üóÉÔ∏è Dataset: Diccionario\n",
    "üóíÔ∏è Enunciado: Crea una columna con el n√∫mero de palabras por comentario.\n",
    "‚úçÔ∏è Resultado esperado: Columna num_palabras con valores 4, 4, 3.\n",
    "\n",
    "\"\"\"\n",
    "## ‚úîÔ∏è Soluci√≥n\n",
    "\n",
    "diccionario_diez = {\n",
    "    \"comentario\":[\"Me gusta el servicio\",\"Precio alto pero bueno\",\"No lo recomiendo\"]\n",
    "}\n",
    "df_diez = spark.createDataFrame(data=list(zip(*diccionario_diez.values())),schema=list(diccionario_diez.keys()))\n",
    "df_diez = df_diez.withColumn(\n",
    "    \"cantidad_palabras\",\n",
    "    size(split(col(\"comentario\"),\" \"))\n",
    ")\n",
    "df_diez.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "be26ec8b-699c-4342-9fc4-9055c9681470",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### ü•á NIVEL AVANZADO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "911e032f-557d-498a-b488-1d5c8e8e2f59",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StringType\n",
    "\"\"\"\n",
    "11. Variables categ√≥ricas cruzadas\n",
    "\n",
    "üóÉÔ∏è Dataset: Titanic\n",
    "üóíÔ∏è Enunciado: Crea una nueva variable categ√≥rica que combine pclass y sex (ejemplo: \"1_female\", \"3_male\").\n",
    "‚úçÔ∏è Resultado esperado: Columna clase_sexo con las combinaciones √∫nicas.\n",
    "\n",
    "\"\"\"\n",
    "## ‚úîÔ∏è Soluci√≥n\n",
    "df_once = spark.sql(\"SELECT * FROM workspace.exercises.titanic\")\n",
    "# df_once.show(5)\n",
    "df_once = df_once.withColumn(\n",
    "    \"clase_sexo\",\n",
    "    concat_ws(\"-\",col(\"pclass\"),col(\"sex\"))\n",
    ")\n",
    "def funcion_dummies_clase_sexo(df,columna):\n",
    "    df_temp = df.select(col(columna))\n",
    "    df_temp = df_temp.dropDuplicates(subset=[columna]).orderBy(col(columna))\n",
    "    for i in df_temp.collect():\n",
    "        df = df.withColumn(\n",
    "            i[0],\n",
    "            when(\n",
    "                col(\"clase_sexo\")==i[0],lit(1)\n",
    "            ).otherwise(lit(0))\n",
    "        )\n",
    "    return df\n",
    "df_once_final = funcion_dummies_clase_sexo(df_once,\"clase_sexo\")\n",
    "display(df_once_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f4c5be39-c3d3-440a-8007-fc812acc80cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "12. Binning desigual basado en reglas\n",
    "\n",
    "üóÉÔ∏è Dataset: Tips\n",
    "üóíÔ∏è Enunciado: Clasifica tip en categor√≠as: \"Bajo\" (<2), \"Medio\" (2 - 5), \"Alto\" (>5).\n",
    "‚úçÔ∏è Resultado esperado: Nueva columna tip_categoria con esas 3 categor√≠as.\n",
    "\n",
    "\"\"\"\n",
    "## ‚úîÔ∏è Soluci√≥n\n",
    "df_doce = spark.sql(\"SELECT * FROM workspace.exercises.df_tips\")\n",
    "# df_doce.show()\n",
    "df_doce = df_doce.withColumn(\n",
    "    \"tip_categoria\",\n",
    "    when(\n",
    "        col(\"tip\")<2,lit(\"Bajo\")\n",
    "    ).when(\n",
    "        (col(\"tip\")>=2) & (col(\"tip\")<=5),lit(\"Medio\")\n",
    "    ).otherwise(lit(\"Alto\"))\n",
    ")\n",
    "df_doce.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c9457fe7-2b50-4217-99eb-f4f538b71452",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "13. Normalizaci√≥n robusta\n",
    "\n",
    "üóÉÔ∏è Dataset: Diccionario\n",
    "üóíÔ∏è Enunciado: Aplica una normalizaci√≥n robusta (usando mediana e IQR).\n",
    "‚úçÔ∏è Resultado esperado: Columna ventas_robust que reduzca la influencia de outliers.\n",
    "\n",
    "\"\"\"\n",
    "## ‚úîÔ∏è Soluci√≥n\n",
    "diccionario_trece = {\n",
    "    \"producto\":[\"A\",\"B\",\"C\",\"D\"],\n",
    "    \"ventas\":[10,200,1000,10000]\n",
    "}\n",
    "df_trece = spark.createDataFrame(data=list(zip(*diccionario_trece.values())),schema=list(diccionario_trece.keys()))\n",
    "# df_trece.show()\n",
    "\n",
    "##  ‚úÖ Hallamos los Quartiles (columna ventas)\n",
    "df_trece_quartiles = df_trece.agg(\n",
    "    expr('percentile(ventas,array(0.25))')[0].alias(\"q1\"),\n",
    "    expr('percentile(ventas,array(0.75))')[0].alias(\"q3\")\n",
    ")\n",
    "# df_trece_quartiles.show()\n",
    "\n",
    "## ‚úÖ Hallamos IQR (Rango Interquartil - columnas ventas)\n",
    "iqr_ventas = df_trece_quartiles.select(col(\"q3\")).collect()[0][0] - df_trece_quartiles.select(col(\"q1\")).collect()[0][0]\n",
    "# iqr_ventas\n",
    "\n",
    "## ‚úÖ Hallamos mediana (columna ventas)\n",
    "mediana_ventas = df_trece.select(median(col(\"ventas\"))).collect()[0][0]\n",
    "# mediana_ventas\n",
    "\n",
    "## ‚úÖ Calculamos Normalizaci√≥n robusta\n",
    "df_trece = df_trece.withColumn(\n",
    "    \"ventas_robus\",\n",
    "    round((col(\"ventas\") - mediana_ventas)/iqr_ventas,2)\n",
    ")\n",
    "df_trece.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "397a0264-344c-4e40-aec1-bbe967a18b87",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import TimestampType\n",
    "\"\"\"\n",
    "14. Extracci√≥n de partes avanzadas de fecha\n",
    "\n",
    "üóÉÔ∏è Dataset: Diccionario\n",
    "üóíÔ∏è Enunciado: Extrae la hora, el minuto y el nombre del d√≠a de la semana.\n",
    "‚úçÔ∏è Resultado esperado: Tres nuevas columnas: hora, minuto, dia_semana.\n",
    "\n",
    "\"\"\"\n",
    "## ‚úîÔ∏è Soluci√≥n\n",
    "diccionario_catorce = {\n",
    "    \"id\":[1,2,3],\n",
    "    \"fecha\":[\"2021-05-12 14:35:00\",\"2021-05-12 20:10:00\",\"2021-05-13 08:45:00\"]\n",
    "}\n",
    "df_catorce = spark.createDataFrame(data=list(zip(*diccionario_catorce.values())),schema=list(diccionario_catorce.keys()))\n",
    "# df_catorce.show()\n",
    "# df_catorce.printSchema()\n",
    "df_catorce = df_catorce.withColumn(\n",
    "    \"fecha\",\n",
    "    col(\"fecha\").cast(TimestampType())\n",
    ")\n",
    "# df_catorce.show()\n",
    "# df_catorce.printSchema()\n",
    "df_catorce = df_catorce.withColumns({\n",
    "    \"hora\":\n",
    "    hour(col(\"fecha\")),\n",
    "    \"minuto\":\n",
    "        minute(col(\"fecha\")),\n",
    "    \"dia_semana\":\n",
    "    date_format(col(\"fecha\"),\"EEEE\")\n",
    "})\n",
    "df_catorce.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "433e8130-7602-4a68-aa3a-a09fc32c77d4",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1758928100456}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "\"\"\"\n",
    "15. Extracci√≥n de features de texto (tokens √∫nicos)\n",
    "\n",
    "üóÉÔ∏è Dataset: Diccionario\n",
    "üóíÔ∏è Enunciado: Crea una columna con el n√∫mero de palabras √∫nicas en cada comentario.\n",
    "‚úçÔ∏è Resultado esperado: Columna palabras_unicas con valores (3, 2, 4).\n",
    "\n",
    "\"\"\"\n",
    "## ‚úîÔ∏è Soluci√≥n\n",
    "diccionario_quince = {\n",
    "    \"comentario\":[\"Muy buen buen servicio\",\"Servicio aceptable aceptable\",\"No me gust√≥ el servicio\"]\n",
    "}\n",
    "df_quince = spark.createDataFrame(data=list(zip(*diccionario_quince.values())),schema=list(diccionario_quince.keys()))\n",
    "df_quince = df_quince.select(\n",
    "    \"*\",\n",
    "    size(array_distinct(split(col(\"comentario\"),\" \",0))).alias(\"palabras_unicas\")\n",
    ")\n",
    "display(df_quince)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "talleres_pyspark_databricks_de",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5a20393c-40f7-4656-a26f-ba04955d9b47",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### üìò Talleres de Ingenier√≠a de Datos con Pyspark en Databricks üêçüß±"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0cfb42a0-7319-463b-a13c-60f10cc2cd04",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "üë®‚Äçüíª Autor: Brayan Neciosup  \n",
    "üìç Portafolio: [brayanneciosup](https://bryanneciosup626.wixsite.com/brayandataanalitics)  \n",
    "üîó LinkedIn: [linkedin.com/brayanneciosup](https://www.linkedin.com/in/brayan-rafael-neciosup-bola%C3%B1os-407a59246/)  \n",
    "üíª GitHub: [github.com/BrayanR03](https://github.com/BrayanR03)  \n",
    "üìö Serie: Fundamentos de Apache Spark - PySpark \n",
    "üìì Estos talleres constar√°n de 3 niveles (B√°sico-Intermedio-Avanzado)   \n",
    "üîç Abarcar√° temas desde Fundamentos de Data Wrangling hacia Casos de Uso Avanzado   \n",
    "üìù Cada ejercicio presenta su enunciado, dataset, resultado esperado y soluci√≥n.   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c4522c20-2e22-46d0-88c6-0f0bebfe08d6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### FUNDAMENTOS DE DATA WRANGLING (MANIPULACI√ìN DE DATOS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25f1d0b0-f046-42d2-a32a-92e388f9a21d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "spark = SparkSession.builder.appName(\"TalleresPySparkDE\").getOrCreate()\n",
    "### üí° CABE RESALTAR QUE LOS DATASETS UTILIZADOS ESTAN ALMACENADOS EN UNITY CATALOG."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d953466b-3ec1-4626-ae09-9f261d383d85",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### ü•â NIVEL B√ÅSICO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73350ec6-4165-41c6-80d6-63f997fd376c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "1. Detecci√≥n de valores nulos en columnas principales\n",
    "\n",
    "üóÉÔ∏è Dataset: TITANIC\n",
    "üóíÔ∏è Enunciado: Identifica cu√°ntos valores faltantes hay en las columnas age, embarked y deck.\n",
    "‚úçÔ∏è Resultado esperado: un conteo por columna con la cantidad de valores nulos.\n",
    "\n",
    "\"\"\"\n",
    "## ‚úîÔ∏è SOLUCI√ìN\n",
    "df_uno = spark.sql(\"SELECT * FROM workspace.exercises.titanic\")\n",
    "# df_uno.show(5)\n",
    "cantidad_datos_nulos = df_uno.select([\n",
    "  sum(when(col(c).isNull(),1).otherwise(0)).alias(c)\n",
    "  for c in df_uno.columns\n",
    "])\n",
    "cantidad_datos_nulos.show() ## ‚û°Ô∏è Cantidad de datos nulos: age(177) - embarked(2) - deck(688)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "52bd62dc-6436-4eb7-8a94-8848908e2b5f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "2. Eliminaci√≥n de filas duplicadas\n",
    "\n",
    "üóÉÔ∏è Dataset: Diccionario\n",
    "üóíÔ∏è Enunciado: Elimina las filas duplicadas y conserva solo la primera aparici√≥n de cada registro.\n",
    "‚úçÔ∏è Resultado esperado: un DataFrame sin filas repetidas.\n",
    "\n",
    "\"\"\"\n",
    "## ‚úîÔ∏è Soluci√≥n\n",
    "dict_data = {\n",
    " \"id\": [1,2,2,3,4,4,5],\n",
    " \"nombre\": [\"Ana\",\"Luis\",\"Luis\",\"Mar√≠a\",\"Pedro\",\"Pedro\",\"Sof√≠a\"],\n",
    " \"edad\": [23,30,30,22,40,40,29]\n",
    "}\n",
    "df_dos = spark.createDataFrame(data=list(zip(*dict_data.values())),schema=list(dict_data.keys()))\n",
    "# df_dos.show()\n",
    "print(df_dos.count()) ## Cantidad de datos originales: 7\n",
    "df_dos_clean = df_dos.drop_duplicates(subset=[\"id\",\"nombre\",\"edad\"])\n",
    "# df_dos_clean.show()\n",
    "print(df_dos_clean.count()) ## Cantidad de datos despu√©s de eliminar nulos: 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f3794ff3-1222-4443-8da6-19838d6847fd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "3. Reemplazo simple de valores faltantes\n",
    "\n",
    "üóÉÔ∏è Dataset: PENGUINS\n",
    "üóíÔ∏è Enunciado: Reemplaza los valores nulos en la columna bill_length_mm con la media de esa misma columna.\n",
    "‚úçÔ∏è Resultado esperado: columna sin valores nulos en bill_length_mm.\n",
    "\n",
    "\"\"\"\n",
    "## ‚úîÔ∏è Soluci√≥n\n",
    "df_tres = spark.sql(\"SELECT * FROM workspace.exercises.penguins\")\n",
    "# df_tres.show(5)\n",
    "cantidad_nulos_df_tres = df_tres.select([\n",
    "    sum(when(col(c).isNull(),1).otherwise(0)).alias(c)\n",
    "    for c in df_tres.columns\n",
    "    if c=='bill_length_mm'\n",
    "])\n",
    "# cantidad_nulos_df_tres.show() ## Cantidad de nulos: 2\n",
    "media_columna_bill_length_mm = df_tres.select(round(mean(col(\"bill_length_mm\")),2)).collect()[0][0]\n",
    "media_columna_bill_length_mm ## Valor de la media: 43.92\n",
    "df_tres = df_tres.fillna(value=media_columna_bill_length_mm,subset=[\"bill_length_mm\"])\n",
    "\n",
    "cantidad_nulos_df_tres = df_tres.select([\n",
    "    sum(when(col(c).isNull(),1).otherwise(0)).alias(c)\n",
    "    for c in df_tres.columns\n",
    "    if c=='bill_length_mm'\n",
    "])\n",
    "cantidad_nulos_df_tres.show() ## Cantidad de nulos: 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "98c26d0a-42cc-44b5-8239-4700a606ca1f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### ü•à NIVEL INTERMEDIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33860441-6be8-4518-8e9e-a0ec71ac8a42",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\"\"\"\n",
    "4. Imputaci√≥n condicional de valores faltantes\n",
    "\n",
    "üóÉÔ∏è Dataset: TITANIC\n",
    "üóíÔ∏è Enunciado: Completa los valores faltantes de age con la edad promedio por clase (pclass).\n",
    "‚úçÔ∏è Resultado esperado: columna age sin valores nulos, imputada seg√∫n clase de pasajero.\n",
    "\n",
    "\"\"\"\n",
    "## ‚úîÔ∏è Soluci√≥n\n",
    "df_cuatro = spark.sql(\"SELECT * FROM workspace.exercises.titanic\")\n",
    "# df_cuatro.show()\n",
    "cantidad_datos_nulos_df_cuatro = df_cuatro.select([\n",
    "    sum(when(col(c).isNull(),1).otherwise(0)).alias(c)\n",
    "    for c in df_cuatro.columns\n",
    "])\n",
    "# cantidad_datos_nulos_df_cuatro.select(col(\"age\")).show() ## Cantidad de nulos: 177\n",
    "promedio_edad_pclass = df_cuatro.groupBy(col(\"pclass\")).agg(\n",
    "    round(avg(col(\"age\")),2).alias(\"avg_edad_pclass\")\n",
    ")\n",
    "# promedio_edad_pclass.show()\n",
    "def valor_avg_edad_pclass(pclass):\n",
    "    return promedio_edad_pclass.filter(\n",
    "        col(\"pclass\")==pclass\n",
    "    ).collect()[0][1]\n",
    "df_cuatro_clean = df_cuatro.withColumn(\n",
    "    \"age\",\n",
    "    when(\n",
    "        (col(\"age\").isNull()==True) & (col(\"pclass\")==1),lit(valor_avg_edad_pclass(1))\n",
    "    ).when(\n",
    "        (col(\"age\").isNull()==True) & (col(\"pclass\")==2),lit(valor_avg_edad_pclass(2))\n",
    "    ).when(\n",
    "        (col(\"age\").isNull()==True) & (col(\"pclass\")==3),lit(valor_avg_edad_pclass(3))\n",
    "    ).otherwise(col(\"age\"))\n",
    ")\n",
    "# df_cuatro_clean.show(5)\n",
    "cantidad_datos_nulos_df_cuatro_clean = df_cuatro_clean.select([\n",
    "    sum(when(col(c).isNull(),1).otherwise(0)).alias(c)\n",
    "    for c in df_cuatro_clean.columns\n",
    "])\n",
    "# cantidad_datos_nulos_df_cuatro_clean.show() ## Cantidad de nulos: 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b60910a-9452-4715-b378-2bae2427f9ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "5. Detecci√≥n de outliers usando IQR\n",
    "\n",
    "üóÉÔ∏è Dataset: Diccionario\n",
    "üóíÔ∏è Enunciado: Identifica los valores de ventas que son outliers seg√∫n el rango intercuart√≠lico (IQR).\n",
    "‚úçÔ∏è Resultado esperado: listado de los productos que presentan valores an√≥malos.\n",
    "\n",
    "\"\"\"\n",
    "## ‚úîÔ∏è Soluci√≥n\n",
    "diccionario_cinco = {\n",
    " \"producto\": [\"A\",\"B\",\"C\",\"D\",\"E\",\"F\"],\n",
    " \"ventas\": [120, 130, 115, 1000, 140, 135]\n",
    "}\n",
    "\n",
    "df_cinco = spark.createDataFrame(data=list(zip(*diccionario_cinco.values())),schema=list(diccionario_cinco.keys()))\n",
    "# df_cinco.show()\n",
    "df_cinco_quartiles = df_cinco.agg(\n",
    "    expr('percentile(ventas,array(0.25))')[0].alias(\"q1\"),\n",
    "    expr('percentile(ventas,array(0.75))')[0].alias(\"q3\")\n",
    ")\n",
    "# df_cinco_quartiles.show()\n",
    "q1_ventas = df_cinco_quartiles.select(col(\"q1\")).collect()[0][0]\n",
    "# q1_ventas\n",
    "q3_ventas = df_cinco_quartiles.select(col(\"q3\")).collect()[0][0]\n",
    "# q3_ventas\n",
    "iqr_ventas = q3_ventas - q1_ventas\n",
    "lower_bound_ventas = q1_ventas - 1.5 * iqr_ventas\n",
    "upper_bound_ventas = q3_ventas + 1.5 * iqr_ventas\n",
    "# print(lower_bound_ventas)\n",
    "# print(upper_bound_ventas)\n",
    "df_cinco_outliers = df_cinco.filter(\n",
    "    (col(\"ventas\")<lower_bound_ventas) |\n",
    "    (col(\"ventas\")>upper_bound_ventas) \n",
    ")\n",
    "# df_cinco_outliers.show() ## Valores outilers\n",
    "# df_cinco_outliers.count() ## Cantidad de valores outilers: 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5f10427-dffd-4ee0-b174-8c293117bf0d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "6. Eliminaci√≥n selectiva de duplicados\n",
    "\n",
    "üóÉÔ∏è Dataset: Diamonds\n",
    "üóíÔ∏è Enunciado: En el dataset de diamantes, elimina duplicados basados solo en las columnas carat y price.\n",
    "‚úçÔ∏è Resultado esperado: DataFrame sin duplicados en esas dos columnas, pero manteniendo el resto de filas.\n",
    "\n",
    "\"\"\"\n",
    "## ‚úîÔ∏è Soluci√≥n\n",
    "df_seis = spark.sql(\"SELECT * FROM workspace.exercises.df_diamonds\")\n",
    "# df_seis.show(5)\n",
    "# df_seis.count() ## Cantidad inicial de datos: 53940\n",
    "df_seis_clean = df_seis.dropDuplicates(subset=[\"carat\",\"price\"])\n",
    "# df_seis_clean.show(5)\n",
    "df_seis_clean.count() ## Cantidad despu√©s de eliminar duplicados: 28988"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "08765b5b-f587-41e4-80d5-3cc9455817bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "7. Relleno de valores faltantes con interpolaci√≥n\n",
    "\n",
    "üóÉÔ∏è Dataset: Diccionario\n",
    "üóíÔ∏è Enunciado: Rellena los valores nulos de la columna temperatura mediante la mediana de las temepraturas.\n",
    "‚úçÔ∏è Resultado esperado: columna completa sin valores nulos.\n",
    "\n",
    "\"\"\"\n",
    "## ‚úîÔ∏è Soluci√≥n\n",
    "## Utilizaremos pandas para generar fechas\n",
    "diccionario_siete  = {\n",
    " \"fecha\": [\"2024-01-01\",\"2024-01-02\",\"2024-01-03\",\"2024-01-04\",\"2024-01-05\",\"2024-01-06\",\"2024-01-07\",\"2024-01-08\",\"2024-01-09\",\"2024-01-10\"],\n",
    " \"temperatura\": [21,22,None,24,25,None,None,28,29,30]\n",
    "}\n",
    "df_siete = spark.createDataFrame(data=list(zip(*diccionario_siete.values())),schema=list(diccionario_siete.keys()))\n",
    "# df_siete.show()\n",
    "mediana_ventas = df_siete.select(median(col(\"temperatura\")).alias(\"mediana\")).collect()[0][0]\n",
    "# mediana_ventas\n",
    "df_siete = df_siete.fillna({\"temperatura\":mediana_ventas})\n",
    "df_siete.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "52cb0da3-7514-4180-9b4c-c23c2b156c54",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "8. Conteo de valores faltantes combinados\n",
    "\n",
    "üóÉÔ∏è Dataset: Penguins\n",
    "üóíÔ∏è Enunciado: Calcula el n√∫mero de registros que tienen valores nulos simult√°neamente\n",
    "    en las columnas bill_length_mm y bill_depth_mm.\n",
    "‚úçÔ∏è Resultado esperado: un n√∫mero entero que indique cu√°ntos registros cumplen esta condici√≥n.\n",
    "\n",
    "\"\"\"\n",
    "## ‚úîÔ∏è Soluci√≥n\n",
    "df_ocho = spark.sql(\"SELECT * FROM workspace.exercises.penguins\")\n",
    "# df_ocho.show(5)\n",
    "df_ocho_nulos_simultaneos = df_ocho.filter(\n",
    "    (col(\"bill_length_mm\").isNull() & col(\"bill_depth_mm\").isNull())\n",
    ")\n",
    "# df_ocho_nulos_simultaneos.show() ## Valores nulos simult√°neos en: \"bill_length_mm\" y \"bill_depth_mm\"\n",
    "df_ocho_nulos_simultaneos.count() ## Cantidad de datos que tengan valores nulos simult√°neos: 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e1812251-c0c1-4362-ab41-b4e96c24fb88",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### ü•á NIVEL AVANZADO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "23fff3bb-5b1f-49cf-ae6d-bad30bc271d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "9. Winsorizaci√≥n de outliers\n",
    "\n",
    "üóÉÔ∏è Dataset: Diccionario\n",
    "üóíÔ∏è Enunciado: Aplica winsorizaci√≥n al 5% superior e inferior en la\n",
    "   columna nota para reducir el impacto de valores extremos.\n",
    "‚úçÔ∏è Resultado esperado: columna nota ajustada, sin eliminar registros.\n",
    "\n",
    "üí° La winsorinizaci√≥n permite mejorar la integridad y confiabilidad de datos\n",
    "    evitando valores extremos a los limites de quartil que tiene cada columna.\n",
    "    Por ejemplo: valores mayores a 10, se establecen como 10 y valores\n",
    "    menores a 5, se establecen como 5.\n",
    "\n",
    "\"\"\"\n",
    "## ‚úîÔ∏è Soluci√≥n\n",
    "diccionario_nueve = {\n",
    " \"alumno\": [\"A\",\"B\",\"C\",\"D\",\"E\",\"F\",\"G\"],\n",
    " \"nota\": [10, 11, 75, 100, 11, 13, 200]\n",
    "}\n",
    "df_nueve = spark.createDataFrame(data=list(zip(*diccionario_nueve.values())),schema=list(diccionario_nueve.keys()))\n",
    "# df_nueve.show()\n",
    "df_nueve_quartiles = df_nueve.agg(\n",
    "    expr('percentile(nota,array(0.05))')[0].alias(\"q005\"),\n",
    "    round(expr('percentile(nota,array(0.95))')[0],2).alias(\"q95\")\n",
    ")\n",
    "# df_nueve_quartiles.show()\n",
    "limite_inferior_5_pct = df_nueve_quartiles.select(col(\"q005\")).collect()[0][0]\n",
    "limite_superior_95_pct = df_nueve_quartiles.select(col(\"q95\")).collect()[0][0]\n",
    "# limite_inferior_5_pct\n",
    "# limite_superior_95_pct\n",
    "\n",
    "df_nueve_final = df_nueve.withColumn(\n",
    "    \"nota\",\n",
    "    when(\n",
    "        col(\"nota\")<limite_inferior_5_pct,lit(limite_inferior_5_pct)\n",
    "    ).when(\n",
    "        col(\"nota\")>limite_superior_95_pct,lit(limite_superior_95_pct)\n",
    "    ).otherwise(col(\"nota\"))\n",
    ")\n",
    "df_nueve_final.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "283e103d-125f-4334-bf78-f3b086da9337",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "10. Detecci√≥n de inconsistencias de tipado en columnas\n",
    "\n",
    "üóÉÔ∏è Dataset: Diccionario\n",
    "üóíÔ∏è Enunciado: Identifica las filas con tipos incorrectos  y convi√©rtelas al tipo correcto.\n",
    "‚úçÔ∏è Resultado esperado: Un dataset con tipos de datos correcto en cada columna\n",
    "\n",
    "\"\"\"\n",
    "from pyspark.sql.types import LongType,FloatType\n",
    "## ‚úîÔ∏è Soluci√≥n\n",
    "diccionario_diez = {\n",
    "    \"ID\":[1,\"2\",3,\"004\",\"5\"],\n",
    "    \"Venta\":[\"12254\",1450,1200.00,\"300\",120.00]\n",
    "}\n",
    "df_diez = spark.createDataFrame(data=list(zip(*diccionario_diez.values())),schema=list(diccionario_diez.keys()))\n",
    "# df_diez.show()\n",
    "df_diez_estandarizado = df_diez.withColumns({\n",
    "    \"ID\":\n",
    "    col(\"ID\").cast(LongType()),\n",
    "    \"Venta\":\n",
    "    col(\"Venta\").cast(FloatType())\n",
    "})\n",
    "df_diez.printSchema()\n",
    "df_diez_estandarizado.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b62d21e-0710-4974-9134-fc4194bfb4b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "11. Identificaci√≥n de duplicados aproximados (fuzzy matching)\n",
    "\n",
    "üóÉÔ∏è Dataset: Diccionario\n",
    "üóíÔ∏è Enunciado: Detecta nombres de clientes que parecen duplicados\n",
    "    por errores tipogr√°ficos (ejemplo: \"Luis\" vs \"luiz\").\n",
    "‚úçÔ∏è Resultado esperado: listado de pares de valores sospechosos de ser duplicados.\n",
    "\n",
    "\"\"\"\n",
    "## ‚úîÔ∏è Soluci√≥n\n",
    "diccionario_once = {\n",
    " \"cliente\": [\"Ana\", \"Ana \", \"Luis\", \"Luz\", \"luiz\", \"Pedro\", \"pedroo\"]\n",
    "}\n",
    "import difflib\n",
    "lista_clientes = [i.replace(' ','') for i in diccionario_once[\"cliente\"]]\n",
    "# lista_clientes\n",
    "sospechosos_duplicados = {}\n",
    "for nombre in lista_clientes:\n",
    "    sospechosos = difflib.get_close_matches(nombre,lista_clientes,n=3,cutoff=0.8)\n",
    "    sospechosos_duplicados[nombre]=sospechosos\n",
    "sospechosos_duplicados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "edb42f36-653c-476c-8eb7-123749413d40",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "12. Validaci√≥n y limpieza de rangos v√°lidos\n",
    "\n",
    "üóÉÔ∏è Dataset: Titanic\n",
    "üóíÔ∏è Enunciado: Valida que la columna \"age\" est√© en un rango l√≥gico (0 a 100 a√±os).\n",
    "               Detecta y corrige/descarta valores fuera de rango.\n",
    "‚úçÔ∏è Resultado esperado: columna age sin valores inv√°lidos, garantizando integridad de negocio.\n",
    "\n",
    "\"\"\"\n",
    "## ‚úîÔ∏è Soluci√≥n\n",
    "df_doce = spark.sql(\"SELECT * FROM workspace.exercises.titanic\")\n",
    "# df_doce.show(5)\n",
    "df_doce_clean = df_doce.filter(\n",
    "    (col(\"age\")>0) &\n",
    "    (col(\"age\")<100) \n",
    ")\n",
    "# df_doce.count() ## Cantidad de datos originales: 891\n",
    "# df_doce_clean.count() ## Cantidad de datos filtrados: 714"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be92cbf0-3a8a-437e-bcc0-577692dba0f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "13. Normalizaci√≥n de categor√≠as inconsistentes\n",
    "\n",
    "üóÉÔ∏è Dataset: Diccionario\n",
    "üóíÔ∏è Enunciado: Detecta y unifica las categor√≠as inconsistentes aplicando reglas \n",
    "              de limpieza (case folding, correcci√≥n de errores).\n",
    "‚úçÔ∏è Resultado esperado: Dataset con categor√≠as √∫nicas y estandarizadas.\n",
    "\n",
    "\"\"\"\n",
    "## ‚úîÔ∏è Soluci√≥n\n",
    "diccionario_trece = {\n",
    "    \"Ciudad\":[\"Lima\",\"lima\",\"LIMA\",\"Lma\",]\n",
    "}\n",
    "df_trece = spark.createDataFrame(data=list(zip(*diccionario_trece.values())),schema=list(diccionario_trece.keys()))\n",
    "ciudades_validas = [\"Lima\"]\n",
    "import difflib\n",
    "def ciudad_estandarizada(ciudad):\n",
    "    similar = difflib.get_close_matches(ciudad,ciudades_validas,n=3,cutoff=0.6)\n",
    "    return similar[0] if similar else ciudad\n",
    "from pyspark.sql.types import StringType\n",
    "udf_ciudad = udf(ciudad_estandarizada,StringType())\n",
    "df_trece = df_trece.withColumn(\n",
    "    \"Ciudad_Estandarizada\",\n",
    "    initcap(udf_ciudad(col(\"Ciudad\")))\n",
    ")\n",
    "df_trece.show()\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "talleres_pyspark_databricks_de",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5a20393c-40f7-4656-a26f-ba04955d9b47",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 📘 Talleres de Ingeniería de Datos con Pyspark en Databricks 🐍🧱"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0cfb42a0-7319-463b-a13c-60f10cc2cd04",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "👨‍💻 Autor: Brayan Neciosup  \n",
    "📍 Portafolio: [brayanneciosup](https://bryanneciosup626.wixsite.com/brayandataanalitics)  \n",
    "🔗 LinkedIn: [linkedin.com/brayanneciosup](https://www.linkedin.com/in/brayan-rafael-neciosup-bola%C3%B1os-407a59246/)  \n",
    "💻 GitHub: [github.com/BrayanR03](https://github.com/BrayanR03)  \n",
    "📚 Serie: Fundamentos de Apache Spark - PySpark \n",
    "📓 Estos talleres constarán de 3 niveles (Básico-Intermedio-Avanzado)   \n",
    "🔍 Abarcará temas desde Fundamentos de Data Wrangling hacia Casos de Uso Avanzado   \n",
    "📝 Cada ejercicio presenta su enunciado, dataset, resultado esperado y solución.   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c4522c20-2e22-46d0-88c6-0f0bebfe08d6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### FUNDAMENTOS DE DATA WRANGLING (MANIPULACIÓN DE DATOS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25f1d0b0-f046-42d2-a32a-92e388f9a21d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "spark = SparkSession.builder.appName(\"TalleresPySparkDE\").getOrCreate()\n",
    "### 💡 CABE RESALTAR QUE LOS DATASETS UTILIZADOS ESTAN ALMACENADOS EN UNITY CATALOG."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d953466b-3ec1-4626-ae09-9f261d383d85",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### 🥉 NIVEL BÁSICO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73350ec6-4165-41c6-80d6-63f997fd376c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "1. Detección de valores nulos en columnas principales\n",
    "\n",
    "🗃️ Dataset: TITANIC\n",
    "🗒️ Enunciado: Identifica cuántos valores faltantes hay en las columnas age, embarked y deck.\n",
    "✍️ Resultado esperado: un conteo por columna con la cantidad de valores nulos.\n",
    "\n",
    "\"\"\"\n",
    "## ✔️ SOLUCIÓN\n",
    "df_uno = spark.sql(\"SELECT * FROM workspace.exercises.titanic\")\n",
    "# df_uno.show(5)\n",
    "cantidad_datos_nulos = df_uno.select([\n",
    "  sum(when(col(c).isNull(),1).otherwise(0)).alias(c)\n",
    "  for c in df_uno.columns\n",
    "])\n",
    "cantidad_datos_nulos.show() ## ➡️ Cantidad de datos nulos: age(177) - embarked(2) - deck(688)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "52bd62dc-6436-4eb7-8a94-8848908e2b5f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "2. Eliminación de filas duplicadas\n",
    "\n",
    "🗃️ Dataset: Diccionario\n",
    "🗒️ Enunciado: Elimina las filas duplicadas y conserva solo la primera aparición de cada registro.\n",
    "✍️ Resultado esperado: un DataFrame sin filas repetidas.\n",
    "\n",
    "\"\"\"\n",
    "## ✔️ Solución\n",
    "dict_data = {\n",
    " \"id\": [1,2,2,3,4,4,5],\n",
    " \"nombre\": [\"Ana\",\"Luis\",\"Luis\",\"María\",\"Pedro\",\"Pedro\",\"Sofía\"],\n",
    " \"edad\": [23,30,30,22,40,40,29]\n",
    "}\n",
    "df_dos = spark.createDataFrame(data=list(zip(*dict_data.values())),schema=list(dict_data.keys()))\n",
    "# df_dos.show()\n",
    "print(df_dos.count()) ## Cantidad de datos originales: 7\n",
    "df_dos_clean = df_dos.drop_duplicates(subset=[\"id\",\"nombre\",\"edad\"])\n",
    "# df_dos_clean.show()\n",
    "print(df_dos_clean.count()) ## Cantidad de datos después de eliminar nulos: 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f3794ff3-1222-4443-8da6-19838d6847fd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "3. Reemplazo simple de valores faltantes\n",
    "\n",
    "🗃️ Dataset: PENGUINS\n",
    "🗒️ Enunciado: Reemplaza los valores nulos en la columna bill_length_mm con la media de esa misma columna.\n",
    "✍️ Resultado esperado: columna sin valores nulos en bill_length_mm.\n",
    "\n",
    "\"\"\"\n",
    "## ✔️ Solución\n",
    "df_tres = spark.sql(\"SELECT * FROM workspace.exercises.penguins\")\n",
    "# df_tres.show(5)\n",
    "cantidad_nulos_df_tres = df_tres.select([\n",
    "    sum(when(col(c).isNull(),1).otherwise(0)).alias(c)\n",
    "    for c in df_tres.columns\n",
    "    if c=='bill_length_mm'\n",
    "])\n",
    "# cantidad_nulos_df_tres.show() ## Cantidad de nulos: 2\n",
    "media_columna_bill_length_mm = df_tres.select(round(mean(col(\"bill_length_mm\")),2)).collect()[0][0]\n",
    "media_columna_bill_length_mm ## Valor de la media: 43.92\n",
    "df_tres = df_tres.fillna(value=media_columna_bill_length_mm,subset=[\"bill_length_mm\"])\n",
    "\n",
    "cantidad_nulos_df_tres = df_tres.select([\n",
    "    sum(when(col(c).isNull(),1).otherwise(0)).alias(c)\n",
    "    for c in df_tres.columns\n",
    "    if c=='bill_length_mm'\n",
    "])\n",
    "cantidad_nulos_df_tres.show() ## Cantidad de nulos: 0"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "talleres_pyspark_databricks_de",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

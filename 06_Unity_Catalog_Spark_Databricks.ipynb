{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "805ef10c-0efd-445d-95ca-5bee03de707b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession # Puerta de entrada para trabajar con spark <-- SIEMPRE DEBEMOS IMPORTAR LA LLAVE MAESTRA QUE INICIA TODO.\n",
    "from pyspark.sql.functions import *  # Funciones propias del módulo SQL de Spark, para trabajar sobre Dataframes.\n",
    "from pyspark.sql.types import *\n",
    "spark = SparkSession.builder.appName(\"ApacheSparkDatabricksApuntes\").getOrCreate() \n",
    "\"\"\"\n",
    "^          ^__________^        ^_________^                               ^\n",
    "|                |                   |                                   | \n",
    "Variable   Constructor de Sesión   Nombre Aplicación       Evita conflicto del SparkSession\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f7dc4620-76e3-49d0-b788-75a9f71cfe56",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Explicación de Catálagos - Esquemas - Volumenes/Tablas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "96c2802f-80f4-405a-a1fc-d3f8ffd9e075",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### JERARQUÍA UNITY CATALOG (CREACIÓN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce1fa1d7-24f8-42bc-9b1f-4bce23a44425",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Crear un CATÁLAGO (carpeta o conjunto de carpetas que almacenará los archivos):\n",
    "# >> Usando SQL EDITOR (SINTAXIS): \n",
    "    ## CREATE CATALOG IF NOT EXISTS nombre_catalago; \n",
    "# >> Usando PySpark (SINTAXIS): \n",
    "    ## spark.sql(\"CREATE CATALOG IF NOT EXISTS nombre_catalago\")\n",
    "\n",
    "#### ➡️ Crearemos un catálago para los ejemplos que explicaré mas adelante\n",
    "spark.sql(\"CREATE CATALOG IF NOT EXISTS explicacion_unity_catalog\")\n",
    "print(\"Se creo el catálago para los ejemplos!!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce1ea131-9d5f-43a7-8224-b14b49f5b2ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Crear un ESQUEMA (agrupación de archivos basado en el objetivo del proyecto):\n",
    "# >> Usando SQL EDITOR (SINTAXIS): \n",
    "    ## CREATE SCHEMA IF NOT EXISTS nombre_catalago.NombreEsquema;\n",
    "# >> Usando PySpark (SINTAXIS): \n",
    "    ## spark.sql(\"CREATE SCHEMA IF NOT EXISTS nombre_catalago.NombreEsquema;\")\n",
    "\n",
    "#### ➡️ Crearemos un esquema para los ejemplos que explicaré mas adelante\n",
    "spark.sql(\"CREATE SCHEMA IF NOT EXISTS explicacion_unity_catalog.ejemplos\")\n",
    "print(\"Se creo el esquema para los ejemplos!!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dcc74272-ecdd-4323-b279-4d5b7e613050",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Crear un VOLUMEN (contenedores de archivos físicos sin esquema tabular):\n",
    "# >> Usando SQL EDITOR (SINTAXIS): \n",
    "    ## CREATE VOLUME IF NOT EXISTS nombre_catalago.nombre_esquema.NombreVolumen;\n",
    "# >> Usando PySpark (SINTAXIS): \n",
    "    ## spark.sql(\"CREATE VOLUME IF NOT EXISTS nombre_catalago.nombre_esquema.NombreVolumen;\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ad66e3ab-4f66-43ac-8266-90af4b671a5c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### ALMACENAR/LEER OBJETOS EN UNITY CATALOG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef644360-5a0b-4ab5-a224-53462b9cfbfa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# PATH PARA ALMACENAR OBJETOS EN VOLUMENES (RECOMENDADO POR GOBERNANZA DE UNITY CATALOG): \n",
    "# SINTAXIS:\n",
    "\"/Volumes/nombre_catalago/nombre_esquema/nombre_volumen\" \n",
    "# EJEMPLO:\n",
    "\"/Volumes/workspace/exercises/volumen_dataframe\"\n",
    "\n",
    "# USAREMOS DE EJEMPLO UN DATAFRAME (TITANIC):\n",
    "dataframe_titanic = spark.sql(\"SELECT * FROM workspace.exercises.titanic\") ##⬅️ Explicaremos más adelante"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d6b881b5-e2d0-4a1c-a3c0-86befb432fa0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### ARCHIVOS PARQUET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2563feb2-e266-457f-ac6c-40598834c82e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Parquet organiza los datos por columnas, lo que permite una mayor compresión y consultas más\n",
    "rápidas, especialmente cuando solo se requieren algunas columnas específicas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9921d624-8355-47c6-ad53-201839fc5423",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###### ALMACENAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "23a8a288-661e-4e24-ac6a-d6ac3d2896a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Almacenar en un archivo parquet el dataframe_titanic (sobrevivientes==1)\n",
    "df_sobrevivientes = dataframe_titanic.filter(col(\"survived\")==1)\n",
    "# df_sobrevivientes.show()\n",
    "\n",
    "#### ➡️ Crearemos un volumen para almacenar el archivo .parquet del dataframe_titanic\n",
    "spark.sql(\"CREATE VOLUME IF NOT EXISTS explicacion_unity_catalog.ejemplos.parquet_titanic\")\n",
    "print(\"Se creo el volumen para el archivo parquet del dataframe_titanic!!!\")\n",
    "\n",
    "#### 📌 Almacenamos en un .parquet el dataframe filtrado\n",
    "df_sobrevivientes.write.mode(\"overwrite\").parquet(\"/Volumes/explicacion_unity_catalog/ejemplos/parquet_titanic\")\n",
    "print(\"Archivo parquet creado y almacenado en el volúmen!!!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "47f309fd-9bc0-4711-b3a3-cf41adf0def6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###### LEER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c54587c-366f-41af-b331-63ba968c2510",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Leer el archivo parquet del dataframe_titanic filtrado creado anteriormente\n",
    "parquet_titanic = spark.read.parquet(\"/Volumes/explicacion_unity_catalog/ejemplos/parquet_titanic\")\n",
    "parquet_titanic.show(3)\n",
    "print(\"Lectura de archivo parquet correctamente\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fe2d699f-4ab9-4879-ad81-410e940d967f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### ARCHIVOS CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f1565feb-81d5-43ef-aa36-ba4558361c40",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Para este ejemplo leeremos una tabla desde Unity Catalog, lo almacenamos como CSV en el volumen\n",
    "y luego leemos ese CSV."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ed87ed0b-7d12-47be-82bc-9c62ad7de7f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###### ALMACENAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a06f1d4b-32b6-4682-a898-11ffaf3ab4b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Almacenar en un archivo csv el dataframe_titanic (sobrevivientes==1)\n",
    "df_sobrevivientes_table = dataframe_titanic.filter(col(\"survived\")==1)\n",
    "# df_sobrevivientes_table.show()\n",
    "\n",
    "#### ➡️ Convertir la tabla proveniente de Unity Catalog a un CSV (Solo para ejemplos didácticos)\n",
    "\n",
    "#### Creamos el volúmen que almacenará el archivo CSV \n",
    "spark.sql(\"CREATE VOLUME IF NOT EXISTS explicacion_unity_catalog.ejemplos.csv_titanic\")\n",
    "print(\"Se creó correctamente el volumen!!!\")\n",
    "\n",
    "#### 📌 Almacenar en un CSV \n",
    "df_sobrevivientes_table.write.mode(\"overwrite\").option(\"header\",\"true\").option(\"inferSchema\",\"true\").csv(\"/Volumes/explicacion_unity_catalog/ejemplos/csv_titanic\")\n",
    "print(\"Se almacenó correctamente el CSV en el volúmen\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "54b09bee-77a7-4f05-8a63-2198f6d73867",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###### LEER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "617435f1-a434-4e88-8205-851c46c78f87",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Leer el archivo CSV del dataframe_titanic filtrado creado anteriormente.\n",
    "csv_titanic = spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").csv(\"/Volumes/explicacion_unity_catalog/ejemplos/csv_titanic\")\n",
    "csv_titanic.show(3)\n",
    "print(\"Se leyó correctamente el archivo CSV!!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2724fddf-ec85-4922-ae1e-4d05e129ac20",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### ARCHIVOS DELTA / DELTA TABLES (MAYOR OPTIMIZACIÓN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "33fa5d28-89ab-4465-b64c-313eaa29c465",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Permiten una mejor optimización en la lectura de la información, debido que trabajan\n",
    "sobre delta lakes (motores de procesamiento muy eficientes basados en archivos parquet). Sin embargo, existe una diferencia en como\n",
    "se almacenan y leen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "46127735-43c7-4d8f-ab00-c19eb1b4a333",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###### ALMACENAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f609972-0c9a-4e3e-b228-ec4737de029f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#### ➡️ ARCHIVOS DELTA (NO SON ENTIDADES GOBERNADAS POR UNITY CATALOG, POR ENDE NO PUEDEN INTERACTUAR MEDIANTE SPARK SQL)\n",
    "## Es el modo por defecto que Databricks establece a cada archivo subido desde Catalog.\n",
    "\n",
    "###📌 Almacenamos en archivo delta\n",
    "df_sobrevivientes = dataframe_titanic.filter(col(\"survived\")==1)\n",
    "# df_sobrevivientes.show(3)\n",
    "\n",
    "## ➡️ Creamos el volumen\n",
    "spark.sql(\"CREATE VOLUME IF NOT EXISTS explicacion_unity_catalog.ejemplos.archivo_delta_titanic\")\n",
    "print(\"Volumen creado correctamente!!!\")\n",
    "df_sobrevivientes.write.format(\"delta\").mode(\"overwrite\").save(\"/Volumes/explicacion_unity_catalog/ejemplos/archivo_delta_titanic\")\n",
    "print(\"Archivo delta creado correctamente!!!\")\n",
    "\n",
    "\n",
    "###📌 Almacenamos en delta table\n",
    "df_sobrevivientes.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"explicacion_unity_catalog.ejemplos.delta_titanic\")\n",
    "print(\"Delta table creado correctamente!!!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2ef87dbe-2ead-4418-a751-d95d4846ae41",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###### LEER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "162d40ad-c4ef-4aae-8756-6ed21e6015b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "### ➡️ Leer el archivo delta y la delta table del dataframe_titanic filtrado creado anteriormente.\n",
    "\n",
    "#### 📌 LECTURA DE ARCHIVO DELTA\n",
    "archivo_delta_titanic = spark.read.format(\"delta\").load(\"/Volumes/explicacion_unity_catalog/ejemplos/archivo_delta_titanic/\")\n",
    "archivo_delta_titanic.show(3)\n",
    "\n",
    "#### 📌 LECTURA DE DELTA TABLE (RECOMENDADA)\n",
    "delta_table_titanic = spark.sql(\"SELECT * FROM explicacion_unity_catalog.ejemplos.delta_titanic\")\n",
    "delta_table_titanic.show(3)\n",
    "print(\"Lectura correcta de ambas formas delta!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1b1c4cbe-ad38-49dc-a8aa-e5144eba9546",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "06_Unity_Catalog_Spark_Databricks",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

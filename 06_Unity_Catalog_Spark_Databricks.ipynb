{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "805ef10c-0efd-445d-95ca-5bee03de707b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession # Puerta de entrada para trabajar con spark <-- SIEMPRE DEBEMOS IMPORTAR LA LLAVE MAESTRA QUE INICIA TODO.\n",
    "from pyspark.sql.functions import *  # Funciones propias del m칩dulo SQL de Spark, para trabajar sobre Dataframes.\n",
    "from pyspark.sql.types import *\n",
    "spark = SparkSession.builder.appName(\"ApacheSparkDatabricksApuntes\").getOrCreate() \n",
    "\"\"\"\n",
    "^          ^__________^        ^_________^                               ^\n",
    "|                |                   |                                   | \n",
    "Variable   Constructor de Sesi칩n   Nombre Aplicaci칩n       Evita conflicto del SparkSession\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f7dc4620-76e3-49d0-b788-75a9f71cfe56",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Explicaci칩n de Cat치lagos - Esquemas - Volumenes/Tablas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "96c2802f-80f4-405a-a1fc-d3f8ffd9e075",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### JERARQU칈A UNITY CATALOG (CREACI칍N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce1fa1d7-24f8-42bc-9b1f-4bce23a44425",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Crear un CAT츼LAGO (carpeta o conjunto de carpetas que almacenar치 los archivos):\n",
    "# >> Usando SQL EDITOR (SINTAXIS): \n",
    "    ## CREATE CATALOG IF NOT EXISTS nombre_catalago; \n",
    "# >> Usando PySpark (SINTAXIS): \n",
    "    ## spark.sql(\"CREATE CATALOG IF NOT EXISTS nombre_catalago\")\n",
    "\n",
    "#### 俱뫮잺 Crearemos un cat치lago para los ejemplos que explicar칠 mas adelante\n",
    "spark.sql(\"CREATE CATALOG IF NOT EXISTS explicacion_unity_catalog\")\n",
    "print(\"Se creo el cat치lago para los ejemplos!!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce1ea131-9d5f-43a7-8224-b14b49f5b2ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Crear un ESQUEMA (agrupaci칩n de archivos basado en el objetivo del proyecto):\n",
    "# >> Usando SQL EDITOR (SINTAXIS): \n",
    "    ## CREATE SCHEMA IF NOT EXISTS nombre_catalago.NombreEsquema;\n",
    "# >> Usando PySpark (SINTAXIS): \n",
    "    ## spark.sql(\"CREATE SCHEMA IF NOT EXISTS nombre_catalago.NombreEsquema;\")\n",
    "\n",
    "#### 俱뫮잺 Crearemos un esquema para los ejemplos que explicar칠 mas adelante\n",
    "spark.sql(\"CREATE SCHEMA IF NOT EXISTS explicacion_unity_catalog.ejemplos\")\n",
    "print(\"Se creo el esquema para los ejemplos!!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dcc74272-ecdd-4323-b279-4d5b7e613050",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Crear un VOLUMEN (contenedores de archivos f칤sicos sin esquema tabular):\n",
    "# >> Usando SQL EDITOR (SINTAXIS): \n",
    "    ## CREATE VOLUME IF NOT EXISTS nombre_catalago.nombre_esquema.NombreVolumen;\n",
    "# >> Usando PySpark (SINTAXIS): \n",
    "    ## spark.sql(\"CREATE VOLUME IF NOT EXISTS nombre_catalago.nombre_esquema.NombreVolumen;\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ad66e3ab-4f66-43ac-8266-90af4b671a5c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### ALMACENAR/LEER OBJETOS EN UNITY CATALOG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef644360-5a0b-4ab5-a224-53462b9cfbfa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# PATH PARA ALMACENAR OBJETOS EN VOLUMENES (RECOMENDADO POR GOBERNANZA DE UNITY CATALOG): \n",
    "# SINTAXIS:\n",
    "\"/Volumes/nombre_catalago/nombre_esquema/nombre_volumen\" \n",
    "# EJEMPLO:\n",
    "\"/Volumes/workspace/exercises/volumen_dataframe\"\n",
    "\n",
    "# USAREMOS DE EJEMPLO UN DATAFRAME (TITANIC):\n",
    "dataframe_titanic = spark.sql(\"SELECT * FROM workspace.exercises.titanic\") ##拘勇 Explicaremos m치s adelante"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d6b881b5-e2d0-4a1c-a3c0-86befb432fa0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### ARCHIVOS PARQUET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2563feb2-e266-457f-ac6c-40598834c82e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Parquet organiza los datos por columnas, lo que permite una mayor compresi칩n y consultas m치s\n",
    "r치pidas, especialmente cuando solo se requieren algunas columnas espec칤ficas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9921d624-8355-47c6-ad53-201839fc5423",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###### ALMACENAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "23a8a288-661e-4e24-ac6a-d6ac3d2896a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Almacenar en un archivo parquet el dataframe_titanic (sobrevivientes==1)\n",
    "df_sobrevivientes = dataframe_titanic.filter(col(\"survived\")==1)\n",
    "# df_sobrevivientes.show()\n",
    "\n",
    "#### 俱뫮잺 Crearemos un volumen para almacenar el archivo .parquet del dataframe_titanic\n",
    "spark.sql(\"CREATE VOLUME IF NOT EXISTS explicacion_unity_catalog.ejemplos.parquet_titanic\")\n",
    "print(\"Se creo el volumen para el archivo parquet del dataframe_titanic!!!\")\n",
    "\n",
    "#### 游늷 Almacenamos en un .parquet el dataframe filtrado\n",
    "df_sobrevivientes.write.mode(\"overwrite\").parquet(\"/Volumes/explicacion_unity_catalog/ejemplos/parquet_titanic\")\n",
    "print(\"Archivo parquet creado y almacenado en el vol칰men!!!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "47f309fd-9bc0-4711-b3a3-cf41adf0def6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###### LEER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c54587c-366f-41af-b331-63ba968c2510",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Leer el archivo parquet del dataframe_titanic filtrado creado anteriormente\n",
    "parquet_titanic = spark.read.parquet(\"/Volumes/explicacion_unity_catalog/ejemplos/parquet_titanic\")\n",
    "parquet_titanic.show(3)\n",
    "print(\"Lectura de archivo parquet correctamente\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fe2d699f-4ab9-4879-ad81-410e940d967f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### ARCHIVOS CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f1565feb-81d5-43ef-aa36-ba4558361c40",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Para este ejemplo leeremos una tabla desde Unity Catalog, lo almacenamos como CSV en el volumen\n",
    "y luego leemos ese CSV."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ed87ed0b-7d12-47be-82bc-9c62ad7de7f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###### ALMACENAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a06f1d4b-32b6-4682-a898-11ffaf3ab4b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Almacenar en un archivo csv el dataframe_titanic (sobrevivientes==1)\n",
    "df_sobrevivientes_table = dataframe_titanic.filter(col(\"survived\")==1)\n",
    "# df_sobrevivientes_table.show()\n",
    "\n",
    "#### 俱뫮잺 Convertir la tabla proveniente de Unity Catalog a un CSV (Solo para ejemplos did치cticos)\n",
    "\n",
    "#### Creamos el vol칰men que almacenar치 el archivo CSV \n",
    "spark.sql(\"CREATE VOLUME IF NOT EXISTS explicacion_unity_catalog.ejemplos.csv_titanic\")\n",
    "print(\"Se cre칩 correctamente el volumen!!!\")\n",
    "\n",
    "#### 游늷 Almacenar en un CSV \n",
    "df_sobrevivientes_table.write.mode(\"overwrite\").option(\"header\",\"true\").option(\"inferSchema\",\"true\").csv(\"/Volumes/explicacion_unity_catalog/ejemplos/csv_titanic\")\n",
    "print(\"Se almacen칩 correctamente el CSV en el vol칰men\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "54b09bee-77a7-4f05-8a63-2198f6d73867",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###### LEER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "617435f1-a434-4e88-8205-851c46c78f87",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Leer el archivo CSV del dataframe_titanic filtrado creado anteriormente.\n",
    "csv_titanic = spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").csv(\"/Volumes/explicacion_unity_catalog/ejemplos/csv_titanic\")\n",
    "csv_titanic.show(3)\n",
    "print(\"Se ley칩 correctamente el archivo CSV!!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2724fddf-ec85-4922-ae1e-4d05e129ac20",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### ARCHIVOS DELTA / DELTA TABLES (MAYOR OPTIMIZACI칍N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "33fa5d28-89ab-4465-b64c-313eaa29c465",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Permiten una mejor optimizaci칩n en la lectura de la informaci칩n, debido que trabajan\n",
    "sobre delta lakes (motores de procesamiento muy eficientes basados en archivos parquet). Sin embargo, existe una diferencia en como\n",
    "se almacenan y leen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "46127735-43c7-4d8f-ab00-c19eb1b4a333",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###### ALMACENAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f609972-0c9a-4e3e-b228-ec4737de029f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#### 俱뫮잺 ARCHIVOS DELTA (NO SON ENTIDADES GOBERNADAS POR UNITY CATALOG, POR ENDE NO PUEDEN INTERACTUAR MEDIANTE SPARK SQL)\n",
    "## Es el modo por defecto que Databricks establece a cada archivo subido desde Catalog.\n",
    "\n",
    "###游늷 Almacenamos en archivo delta\n",
    "df_sobrevivientes = dataframe_titanic.filter(col(\"survived\")==1)\n",
    "# df_sobrevivientes.show(3)\n",
    "\n",
    "## 俱뫮잺 Creamos el volumen\n",
    "spark.sql(\"CREATE VOLUME IF NOT EXISTS explicacion_unity_catalog.ejemplos.archivo_delta_titanic\")\n",
    "print(\"Volumen creado correctamente!!!\")\n",
    "df_sobrevivientes.write.format(\"delta\").mode(\"overwrite\").save(\"/Volumes/explicacion_unity_catalog/ejemplos/archivo_delta_titanic\")\n",
    "print(\"Archivo delta creado correctamente!!!\")\n",
    "\n",
    "\n",
    "###游늷 Almacenamos en delta table\n",
    "df_sobrevivientes.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"explicacion_unity_catalog.ejemplos.delta_titanic\")\n",
    "print(\"Delta table creado correctamente!!!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2ef87dbe-2ead-4418-a751-d95d4846ae41",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###### LEER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "162d40ad-c4ef-4aae-8756-6ed21e6015b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "### 俱뫮잺 Leer el archivo delta y la delta table del dataframe_titanic filtrado creado anteriormente.\n",
    "\n",
    "#### 游늷 LECTURA DE ARCHIVO DELTA\n",
    "archivo_delta_titanic = spark.read.format(\"delta\").load(\"/Volumes/explicacion_unity_catalog/ejemplos/archivo_delta_titanic/\")\n",
    "archivo_delta_titanic.show(3)\n",
    "\n",
    "#### 游늷 LECTURA DE DELTA TABLE (RECOMENDADA)\n",
    "delta_table_titanic = spark.sql(\"SELECT * FROM explicacion_unity_catalog.ejemplos.delta_titanic\")\n",
    "delta_table_titanic.show(3)\n",
    "print(\"Lectura correcta de ambas formas delta!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1b1c4cbe-ad38-49dc-a8aa-e5144eba9546",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "06_Unity_Catalog_Spark_Databricks",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

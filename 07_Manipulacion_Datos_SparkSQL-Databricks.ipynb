{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bdb171e4-c56f-4d12-89cd-a63089567db4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## MANIPULACIÃ“N DE DATOS (SPARK SQL EN DATABRICKS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e73c2979-6912-4eb3-8c88-8372565878c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession # Puerta de entrada para trabajar con spark <-- SIEMPRE DEBEMOS IMPORTAR LA LLAVE MAESTRA QUE INICIA TODO.\n",
    "from pyspark.sql.functions import *  # Funciones propias del mÃ³dulo SQL de Spark, para trabajar sobre Dataframes.\n",
    "\n",
    "spark = SparkSession.builder.appName(\"DatabricksSparkSQL\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "13c00770-281a-4559-93e1-ded0f0126c1c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### DATAFRAMES (VersiÃ³n Databricks Free Edition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cb3fe8f8-cb2c-4c2e-ba4f-25a579e89cad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Link de [Databrick Free Edition](https://dbc-89f542f8-2df6.cloud.databricks.com/?o=758509963140561)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7c9c736d-562b-46a1-9da2-1af1e2bb35f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Para abordar el tema de **ManipulaciÃ³n de Datos**, utilizaremos una de las principales herramientas que ofrece **Databricks: Spark SQL**.  \n",
    "Esta herramienta nos permite aplicar conceptos del lenguaje SQL sobre conjuntos de datos estructurados, ya sea que provengan directamente de archivos o estÃ©n gobernados a travÃ©s del servicio **Unity Catalog**.\n",
    "\n",
    "Gracias a Spark SQL, podremos consultar, transformar y analizar datos de forma eficiente, aprovechando tanto la potencia del motor distribuido de Spark como la organizaciÃ³n lÃ³gica que brinda Unity Catalog.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "977fe4c5-7621-4161-ac95-b38d326688d9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### FASE 1. FUENTES DE DATOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "24241b3b-f046-4c91-9e20-b3c7610058ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#### âž¡ï¸ Como se venÃ­a explicando, Unity Catalog trabaja bajo una jerarquÃ­a de CatÃ¡lagos, Esquemas, Volumenes y Tablas.\n",
    "####     Por ende, es diferente la manera de acceder a su contenido.\n",
    "\n",
    "#====== ðŸ“Œ LEER ARCHIVO CSV \n",
    "##ðŸ“ SINTAXIS:\n",
    "#   dataset_nombre = spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").csv(\"/Volumes/NombreCatalago/NombreEsquema/NombreArchivo\")\n",
    "\n",
    "#---> spark.read : Permite leer un archivo basado en una ruta. Siempre instanciando primero SparkSession.\n",
    "#---> .option(\"header\",\"true||false\") : Permite establecer si tomarÃ¡ en cuenta los encabezados del dataset. true(mostrar) | false(ocultar)\n",
    "#---> .option(\"inferSchema\",\"true||false\") : Permite reconocer el tipo de dato de cada columna del dataset. true(mostrar) | false(ocultar)\n",
    "#---> .csv(RutaArchivoCSV) : Tipo de archivo a leer (En este caso un CSV).\n",
    "\n",
    "## ðŸ“ EJEMPLO:\n",
    "df_titanic_csv = spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").csv(\"/Volumes/explicacion_unity_catalog/ejemplos/csv_titanic\")\n",
    "# df_titanic_csv.show(5)\n",
    "\n",
    "#====== ðŸ“Œ LEER ARCHIVO PARQUET\n",
    "##ðŸ“ SINTAXIS:\n",
    "#   dataset_nombre = spark.read.parquet(\"/Volumes/NombreCatalago/NombreEsquema/NombreArchivoPARQUET\")\n",
    "\n",
    "#---> spark.read : Permite leer un archivo basado en una ruta. Siempre instanciando primero SparkSession.\n",
    "#---> .parquet(RutaArchivoPARQUET) : Tipo de archivo a leer (En este caso un PARQUET).\n",
    "\n",
    "## ðŸ“ EJEMPLO\n",
    "df_titanic_parquet = spark.read.parquet(\"/Volumes/explicacion_unity_catalog/ejemplos/parquet_titanic\")\n",
    "# df_titanic_parquet.show(5)\n",
    "\n",
    "#====== ðŸ“Œ LEER ARCHIVO DELTA\n",
    "## ðŸ“ SINTAXIS:\n",
    "#   dataset_nombre = spark.read.format(\"delta\").load(\"/Volumes/NombreCatalago/NombreEsquema/NombreArchivoDELTA\")\n",
    "\n",
    "#---> spark.read : Permite leer un archivo basado en una ruta. Siempre instanciando primero SparkSession.\n",
    "#---> .format(\"delta\") : stablece el tipo de archivo a leer (En este caso aplica siempre para DELTA)\n",
    "#---> .load(RutaArchivoDelta) : Aplica solo a los archivos DELTA.\n",
    "\n",
    "## ðŸ“ EJEMPLO:\n",
    "df_titanic_archivo_delta = spark.read.format(\"delta\").load(\"/Volumes/explicacion_unity_catalog/ejemplos/archivo_delta_titanic\")\n",
    "# df_titanic_archivo_delta.show(5)\n",
    "\n",
    "#====== ðŸ“Œ LEER TABLA DELTA (RECOMENDADO Y NATIVO EN DATABRICKS-UNITY-CATALOG)\n",
    "## ðŸ“ SINTAXIS:\n",
    "#   dataset_nombre = spark.sql(\"SELECT * FROM NombreCatalago.NombreEsquema.NombreTablaDelta\")\n",
    "\n",
    "#---> spark.sql() : Permite leer mediante lenguaje SQL una tabla delta. Siempre instanciando primero SparkSession.\n",
    "#---> \"NombreCatalago.NombreEsquema.NombreTablaDelta\" : Ruta aplicable solo para las tablas delta y gobernadas por Unity Catalog\n",
    "\n",
    "## ðŸ“ EJEMPLO\n",
    "df_titanic_delta = spark.sql(\"SELECT * FROM explicacion_unity_catalog.ejemplos.delta_titanic\")\n",
    "# df_titanic_delta.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7cd4a063-2337-43a2-bc32-7e739882d27a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### FASE 2. EXPLORACIÃ“N INICIAL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "072cc5f7-fa2f-4206-959d-80b0914f90e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Utilizaremos de ejemplo el dataset de titanic ( en formato tabla delta y CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e253d9b-3aa8-43e0-b929-69872197e5f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#====== A). Quitar encabezados originales provenientes del Dataset (Archivos CSV).\n",
    "df_titanic_archivo_delta = spark.read.csv(\"/Volumes/explicacion_unity_catalog/ejemplos/csv_titanic\") ## â¬…ï¸ Archivo CSV\n",
    "df_titanic_archivo_delta.show(3)\n",
    "#### ðŸ§ ðŸ’¡ Importante: Cuando leamos un archivo CSV, si no establecemos .option(\"header\",\"true\")\n",
    "####                   el dataframe mostrarÃ¡ las columnas con un prefijo c_ seguido de una posiciÃ³n asignada.\n",
    "####                   (Y los nombres de columnas serÃ¡n parte de los registros del dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "38f7d025-9d30-4880-9c32-74776c068d0d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#====== B). Mostrar los N primeros registros de un dataset.\n",
    "\n",
    "df_titanic_delta = spark.sql(\"SELECT * FROM explicacion_unity_catalog.ejemplos.delta_titanic\") ## â¬…ï¸ Tabla delta\n",
    "\n",
    "#### ðŸ“ SINTAXIS: dataset_nombre.show(NÃºmeroDeRegistros)\n",
    "\n",
    "#### ðŸ“ EJEMPLO 1:\n",
    "# df_titanic_delta.show(10) ## â¬…ï¸ Mostrar los 10 primeros registros.\n",
    "\n",
    "#### ðŸ“ EJEMPLO 2:\n",
    "# df_titanic_delta.show(5) ## â¬…ï¸ Mostrar los 5 primeros registros.\n",
    "\n",
    "#### ðŸ“ EJEMPLO 3:\n",
    "df_titanic_delta.show(3) ## â¬…ï¸ Mostrar los 3 primeros registros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb5e834b-4a6d-4600-b862-da940d8ecb6e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#====== C). Mostrar los N Ãºltimos registros de un dataset (Retornado en una lista).\n",
    "\n",
    "df_titanic_delta = spark.sql(\"SELECT * FROM explicacion_unity_catalog.ejemplos.delta_titanic\") ## â¬…ï¸ Tabla delta\n",
    "\n",
    "#### ðŸ“ SINTAXIS: dataset_nombre.tail(NÃºmeroDeRegistros)\n",
    "\n",
    "#### ðŸ“ EJEMPLO 1:\n",
    "# df_titanic_delta.tail(10) ## â¬…ï¸ Mostrar los 10 primeros Ãºltimos.\n",
    "\n",
    "#### ðŸ“ EJEMPLO 2:\n",
    "# df_titanic_delta.tail(5) ## â¬…ï¸ Mostrar los 5 primeros Ãºltimos.\n",
    "\n",
    "#### ðŸ“ EJEMPLO 3:\n",
    "df_titanic_delta.tail(3) ## â¬…ï¸ Mostrar los 3 primeros Ãºltimos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "04482e74-468a-4215-8dad-45de1268436e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#====== D). Mostrar el volÃºmen del dataset (Cantidad de filas y columnas respectivamente)\n",
    "\n",
    "df_titanic_delta = spark.sql(\"SELECT * FROM explicacion_unity_catalog.ejemplos.delta_titanic\") ## â¬…ï¸ Tabla delta\n",
    "\n",
    "#### ðŸ“ SINTAXIS: \n",
    "#       dataset_nombre.count() â¬…ï¸ Cantidad de filas\n",
    "#       len([i for i in dataframe_nombre.columns]) â¬…ï¸ Cantidad de columnas\n",
    "\n",
    "#### ðŸ“ EJEMPLO 1:\n",
    "print(f\"Cantidad de filas: {df_titanic_delta.count()}\") ## â¬…ï¸ Cantidad de filas.\n",
    "\n",
    "#### ðŸ“ EJEMPLO 2:\n",
    "print(f\"Cantidad de columnas: {len([i for i in df_titanic_delta.columns])}\") ## â¬…ï¸ Cantidad de columnas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1fe9efb3-5cdf-4254-8128-9e29c0f4e0e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### FASE 3. EXPLORACIÃ“N INICIAL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6c871f58-4bdd-457b-b93a-10e79d83f6b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###### FASE 3.1. DATOS CUALITATIVOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "842aed67-f394-4048-a525-c7fe4d848771",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    A). CONVERTIR A MAYÃšSCULAS LOS DATOS CUALITATIVOS DE UNA COLUMNA EN UN DATASET\n",
    "    \n",
    "    ðŸ“ SINTAXIS:\n",
    "\n",
    "        dataframe_nombre = dataframe_nombre.withColumn(\n",
    "            \"NombreColumnaNueva\",\n",
    "            upper(col(\"NombreColumnaCualitativa\")) â¬…ï¸ FunciÃ³n upper() integrada en Spark SQL.\n",
    "        )\n",
    "    ### ðŸ§  En este caso, debemos almacenar en una variable los cambios a realizar en el dataframe\n",
    "    ### ðŸ’¡ Importante: Siempre .withColumn() trabajarÃ¡ con al menos una columna del dataset.\n",
    "\"\"\"\n",
    "\n",
    "# ðŸ’¡ EJEMPLO:\n",
    "df_titanic_delta = df_titanic_delta.withColumn(\n",
    "    \"embark_town\",\n",
    "    upper(col(\"embark_town\")) ## â¬…ï¸ Convertimos a mayÃºsculas los datos de la columna cualitativa \"\"embark_town\"\"\n",
    ")\n",
    "df_titanic_delta.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0100db6f-3ef4-4b75-91aa-160ef6ecdffa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    B). CONVERTIR A MINÃšSCULAS LOS DATOS CUALITATIVOS DE UNA COLUMNA EN UN DATASET\n",
    "    \n",
    "    ðŸ“ SINTAXIS:\n",
    "\n",
    "        dataframe_nombre = dataframe_nombre.withColumn(\n",
    "            \"NombreColumnaNueva\",\n",
    "            lower(col(\"NombreColumnaCualitativa\")) â¬…ï¸ FunciÃ³n lower() integrada en Spark SQL.\n",
    "        )\n",
    "    ### ðŸ§  En este caso, debemos almacenar en una variable los cambios a realizar en el dataframe\n",
    "    ### ðŸ’¡ Importante: Siempre .withColumn() trabajarÃ¡ con al menos una columna del dataset.\n",
    "\"\"\"\n",
    "\n",
    "# ðŸ’¡ EJEMPLO:\n",
    "df_titanic_delta = df_titanic_delta.withColumn(\n",
    "    \"embark_town\",\n",
    "    lower(col(\"embark_town\")) ## â¬…ï¸ Convertimos a minÃºsculas los datos de la columna cualitativa \"\"embark_town\"\"\n",
    ")\n",
    "df_titanic_delta.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "daf6de6e-361c-4e50-beb5-1be9c1b90553",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    C). CONVERTIR A MAYÃšSCULA LA PRIMERA LETRA DE CADA DATO CUALITATIVOS DE UNA COLUMNA EN UN DATASET\n",
    "    \n",
    "    ðŸ“ SINTAXIS:\n",
    "\n",
    "        dataframe_nombre = dataframe_nombre.withColumn(\n",
    "            \"NombreColumnaNueva\",\n",
    "            initcap(col(\"NombreColumnaCualitativa\")) â¬…ï¸ FunciÃ³n initcap() integrada en Spark SQL.\n",
    "        )\n",
    "    ### ðŸ§  En este caso, debemos almacenar en una variable los cambios a realizar en el dataframe\n",
    "    ### ðŸ’¡ Importante: Siempre .withColumn() trabajarÃ¡ con al menos una columna del dataset.\n",
    "\"\"\"\n",
    "\n",
    "# ðŸ’¡ EJEMPLO:\n",
    "df_titanic_delta = df_titanic_delta.withColumn(\n",
    "    \"embark_town\",\n",
    "    initcap(col(\"embark_town\")) ## â¬…ï¸ Convertimos la primera letra en mayÃºscula de cada dato en la columna cualitativa \"\"embark_town\"\"\n",
    ")\n",
    "df_titanic_delta.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58aab99e-657c-4821-8846-5c42ba6762b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    D). EXTRAER DE CADA DATO CUALITATIVO UNA CADENA EN ESPECÃFICO MEDIANTE >>EXPRESIONES REGULARES<< EN UN DATASET\n",
    "    \n",
    "    ðŸ“ SINTAXIS:\n",
    "\n",
    "        dataframe_nombre = dataframe_nombre.withColumn(\n",
    "            \"NombreColumnaNueva\",\n",
    "                regexp_extract(col(\"embark_town\"),r'([A-Z])',1) â¬…ï¸ FunciÃ³n regexp_extract() integrada en Spark SQL.\n",
    "        )\n",
    "    ### ðŸ§  En este caso, debemos almacenar en una variable los cambios a realizar en el dataframe\n",
    "    ### ðŸ’¡ Importante: Siempre .withColumn() trabajarÃ¡ con al menos una columna del dataset.\n",
    "\"\"\"\n",
    "\n",
    "# ðŸ’¡ EJEMPLO 1: â¬…ï¸ Extraemos primera letra en mayÃºscula \n",
    "# df_titanic_delta = df_titanic_delta.withColumn(\n",
    "#     \"PrimeraLetraEnMayÃºscula\",\n",
    "#     regexp_extract(col(\"embark_town\"),r'([A-Z])',1)\n",
    "# )\n",
    "# df_titanic_delta.show(3)\n",
    "\n",
    "# ðŸ’¡ EJEMPLO 2: â¬…ï¸ Extraemos primera letra en minÃºscula \n",
    "# df_titanic_delta = df_titanic_delta.withColumn(\n",
    "#     \"PrimeraLetraEnMinÃºscula\",\n",
    "#     regexp_extract(col(\"embark_town\"),r'([a-z])',0)\n",
    "# )\n",
    "# df_titanic_delta.show(3)\n",
    "\n",
    "# ðŸ’¡ EJEMPLO 3: â¬…ï¸ Retornamos todo despuÃ©s de una letra en mayÃºscula \n",
    "# df_titanic_delta = df_titanic_delta.withColumn(\n",
    "#     \"TodoDespuesDeUnaMayÃºscula\",\n",
    "#     regexp_extract(col(\"embark_town\"),r'[A-Z]([a-z]+)', 1)\n",
    "# )\n",
    "# df_titanic_delta.show(3)\n",
    "\n",
    "# ðŸ’¡ EJEMPLO 4: â¬…ï¸ Retornamos todo despuÃ©s de una letra en minÃºscula \n",
    "# df_titanic_delta = df_titanic_delta.withColumn(\n",
    "#     \"TodoDespuesDeUnaMinÃºscula\",\n",
    "#     regexp_extract(col(\"embark_town\"),r'[a-z]([a-z]+)', 1)\n",
    "# )\n",
    "# df_titanic_delta.show(3)\n",
    "\n",
    "# ðŸ’¡ EJEMPLO 5: â¬…ï¸ Retornamos todo despuÃ©s de un nÃºmero\n",
    "# df_titanic_delta = df_titanic_delta.withColumn(\n",
    "#     \"TodoDespuesDeUnNÃºmero\",\n",
    "#     regexp_extract(col(\"embark_town\"),r'([0-9]+)', 1)\n",
    "# )\n",
    "# df_titanic_delta.show(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "187aeed1-2d0d-4205-9597-093aec868b27",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    E). REEMPLAZAR VALORES DE DATOS CUALITATIVOS MEDIANTE >>EXPRESIONES REGULARES<< EN UN DATASET\n",
    "    \n",
    "    ðŸ“ SINTAXIS:\n",
    "    \n",
    "    \n",
    "    ### ðŸ§  En este caso, debemos almacenar en una variable los cambios a realizar en el dataframe\n",
    "    ### ðŸ’¡ Importante: Siempre .withColumn() trabajarÃ¡ con al menos una columna del dataset.\n",
    "\"\"\"\n",
    "### DATASET DE PRUEBA PARA ESTE EJEMPLO\n",
    "datos_sucios = {\n",
    "    \"nombre\": [\n",
    "        \"  JUAN  pÃ©rez  \", \n",
    "        \"MARÃA@@LÃ“PEZ\", \n",
    "        \"Ana--MartÃ­nez\", \n",
    "        \"carlos_rodrÃ­guez \", \n",
    "        \"SofÃ­a123 GarcÃ­a\"\n",
    "    ],\n",
    "    \"categoria_producto\": [\n",
    "        \"ELECTRÃ“NICA#1\", \n",
    "        \"ropa--Mujer\", \n",
    "        \"hogar_y_DECORACIÃ“N\", \n",
    "        \"LIBROS@@\", \n",
    "        \"Juguetes  niÃ±os\"\n",
    "    ],\n",
    "    \"codigo\": [\n",
    "        \"ID-0001\", \n",
    "        \"ID-0023\", \n",
    "        \"CL-1234\", \n",
    "        \"ID-abc123\", \n",
    "        \"id-9999\"\n",
    "    ]\n",
    "}\n",
    "## Dataset de prueba (sucio)\n",
    "df_test = spark.createDataFrame(data=list(zip(*datos_sucios.values())),schema=[\"nombre\",\"categoria_producto\",\"codigo\"])\n",
    "# df_test.show()\n",
    "\n",
    "## Dataset de prueba (limpio)\n",
    "df_test_clean = df_test \n",
    "# df_test_clean.show()\n",
    "\n",
    "## ðŸ’¡ EJEMPLO 1: â¬…ï¸ Eliminar caractÃ©res especiales (@@,#,--,_)\n",
    "df_test_clean = df_test_clean.withColumns({\n",
    "    \"categoria_producto\":\n",
    "    regexp_replace(col(\"categoria_producto\"),r'[^A-Za-z0-9ÃÃ‰ÃÃ“ÃšÃ¡Ã©Ã­Ã³ÃºÃ‘Ã±ÃœÃ¼]',\" \"),\n",
    "    \"nombre\":\n",
    "    regexp_replace(col(\"nombre\"),r'[^A-Za-z0-9ÃÃ‰ÃÃ“ÃšÃ¡Ã©Ã­Ã³ÃºÃ‘Ã±ÃœÃ¼]',' ')\n",
    "})\n",
    "# df_test_clean.show()\n",
    "\n",
    "## ðŸ’¡ EJEMPLO 2: â¬…ï¸ Normalizar espacios (Varios espacios en blanco a uno solo)\n",
    "df_test_clean = df_test_clean.withColumns({\n",
    "    \"categoria_producto\":\n",
    "    regexp_replace(col(\"categoria_producto\"),r'\\s+',\" \"),\n",
    "    \"nombre\":\n",
    "    regexp_replace(col(\"nombre\"),r'\\s+',' ')\n",
    "})\n",
    "# df_test_clean.show()\n",
    "\n",
    "## ðŸ’¡ EJEMPLO 3: â¬…ï¸ Reemplazar guiones por espacios en blanco\n",
    "df_test_clean = df_test_clean.withColumn(\n",
    "    \"nombre\",\n",
    "    regexp_replace(col(\"nombre\"),r'[-_]',' ')\n",
    ")\n",
    "# df_test_clean.show()\n",
    "\n",
    "## ðŸ’¡ EJEMPLO 4: â¬…ï¸ Eliminar prefijos (ID- || CL- || id- || Letras)\n",
    "df_test_clean = df_test_clean.withColumn(\n",
    "    \"codigo\",\n",
    "    regexp_replace(col(\"codigo\"),r'^ID-|^CL-|^id-|[a-zA-Z]','')\n",
    ")\n",
    "# df_test_clean.show()\n",
    "\n",
    "## ðŸ’¡ EJEMPLO 5: â¬…ï¸ Eliminar nÃºmeros dentro de cadenas\n",
    "df_test_clean = df_test_clean.withColumns({\n",
    "    \"nombre\":\n",
    "    regexp_replace(initcap(col(\"nombre\")),r'[0-9]',''),\n",
    "    \"categoria_producto\":\n",
    "    regexp_replace(initcap(col(\"categoria_producto\")),r'[0-9]','')\n",
    "})\n",
    "df_test_clean.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "00a1018e-ad0b-4fd5-9db5-228ce1db3ba0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    F). ELIMINAR VALORES DUPLICADOS EN DATOS CUALITATIVOS DE UN DATASET\n",
    "    \n",
    "    ðŸ“ SINTAXIS:\n",
    "    \n",
    "        dataset_nombre.dropDuplicates(subset=[NombreColumna1,NombreColumnaN])\n",
    "        \n",
    "        subset: Establece el o los nombres de las columnas a eliminar duplicados,\n",
    "                en caso no especifiquemos, los duplicados serÃ¡n eliminados de la\n",
    "                totalidadl de dataset.\n",
    "        \n",
    "    ### ðŸ§  En este caso, debemos trabajar sobre el mismo dataset/dataframe para eliminar los duplicados.\n",
    "\"\"\"\n",
    "### âœ…ðŸ—ƒï¸ Dataset a utilizar: Diamonds\n",
    "df_diamonds = spark.sql(\"SELECT * FROM workspace.exercises.df_diamonds\")\n",
    "# df_diamonds.show()\n",
    "# df_diamonds.count() ## â¬…ï¸ Cantidad de datos inicial: 53940\n",
    "\n",
    "## ðŸ’¡ EJEMPLO 1: ELIMINAR VALORES DUPLICADOS DE TODO EL DATASET\n",
    "# df_diamonds = df_diamonds.dropDuplicates()\n",
    "# df_diamonds.count() ## â¬…ï¸ Cantidad de datos inicial: 53794\n",
    "\n",
    "##ðŸ’¡ EJEMPLO 2: ELIMINAR VALORES DUPLICADOS DE UNA COLUMNA ESPECÃFICA\n",
    "# df_diamonds[[\"carat\"]].count() ## â¬…ï¸ Cantidad inicial de datos: 53940\n",
    "# df_diamonds = df_diamonds.dropDuplicates(subset=[\"carat\"]) \n",
    "# df_diamonds[[\"carat\"]].count() ## â¬…ï¸ Cantidad de datos restantes: 273   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "21c1932b-9e89-48d4-ad57-b697d4ba4a93",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###### FASE 3.2. DATOS CUANTITATIVOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e310c968-7c57-4567-ad5a-4db8645a06e5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_penguins = spark.sql(\"SELECT * FROM workspace.exercises.penguins\")\n",
    "df_penguins.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f80db7ef-7476-495b-b342-231f39348ab7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    A). EXPLORACIÃ“N BÃSICA Y RESUMEN ESTADÃSTICO\n",
    "        \n",
    "        ðŸ“ Para los datos cuantitativos podemos obtener su estadÃ­stica bÃ¡sica\n",
    "            mediante .decribe() el cuÃ¡l retornarÃ¡ el valor de las columnas float o int.\n",
    "            En los campos cualitativos se visualizarÃ¡ informaciÃ³n no creÃ­ble.\n",
    "\"\"\"\n",
    "df_penguins.describe().show()                            ## â¬…ï¸ EstadÃ­stica simple - Todas las columnas\n",
    "df_penguins.select(col(\"body_mass_g\")).describe().show() ## â¬…ï¸ EstadÃ­stica simple - Columna body_mass_g\n",
    "df_penguins.select(mean(col(\"body_mass_g\"))).show()      ## â¬…ï¸ Media estadÃ­stica - Columna body_mass_g\n",
    "df_penguins.select(median(col(\"body_mass_g\"))).show()    ## â¬…ï¸ Mediana estadÃ­stica - Columna body_mass_g\n",
    "df_penguins.select(min(col(\"body_mass_g\"))).show()       ## â¬…ï¸ Valor mÃ­nimo - Columna body_mass_g\n",
    "df_penguins.select(max(col(\"body_mass_g\"))).show()       ## â¬…ï¸ Valor mÃ¡ximo (IMC Gramos)\n",
    "df_penguins.select(sum(col(\"body_mass_g\"))).show()       ## â¬…ï¸ Suma total - Columna body_mass_g\n",
    "df_penguins.select(stddev(col(\"body_mass_g\"))).show()    ## â¬…ï¸ DesviaciÃ³n estÃ¡ndar - Columna body_mass_g\n",
    "df_penguins.select(variance(col(\"body_mass_g\"))).show()  ## â¬…ï¸ Varianza - Columna body_mass_g\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e87e6cb-fcb6-40f1-851b-f2b4f95257ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    B). VERIFICAR VALORES NULOS EN LA(S) COLUMNA(AS) DEL DATASET\n",
    "\"\"\"\n",
    "\n",
    "## ðŸ’¡ EJEMPLO 1: VERIFICAR CANTIDAD DE VALORES NULOS EN TODAS LAS COLUMNAS DEL DATASET\n",
    "## â¬…ï¸ Retorna la cantidad de datos nulos por columna.\n",
    "cantidad_nulos = df_penguins.select([\n",
    "    sum(when(col(i).isNull(),1).otherwise(0)).alias(i)\n",
    "    for i in df_penguins.columns\n",
    "])\n",
    "# cantidad_nulos.show()\n",
    "\n",
    "## ðŸ’¡ EJEMPLO 2: VERIFICAR CANTIDAD DE VALORES NULOS EN UNA COLUMNA ESPECÃFICA DEL DATASET\n",
    "##â¬…ï¸ Retorna la cantidad de datos nulos de una columna.\n",
    "cantidad_nulos_bmg = df_penguins.select([\n",
    "    sum(when(col(i).isNull(),1).otherwise(0)).alias(i)\n",
    "    for i in df_penguins.columns\n",
    "    if i==\"body_mass_g\"\n",
    "])\n",
    "cantidad_nulos_bmg.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b1bb0d4f-accc-4fbe-8706-93ab70f10eaa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    C). FILTRANDO VALORES EN COLUMNAS CUANTITATIVAS\n",
    "\"\"\"\n",
    "\n",
    "print(\"Total de datos registrados: \",df_penguins.count()) ## â¬…ï¸ Cantidad de datos del dataset : 344\n",
    "\n",
    "## ðŸ’¡ EJEMPLO 1: FILTRAR VALORES DE COLUMNA \"body_mass_g\" MAYOR A 3000\n",
    "df_penguins_bmg_mayor_3000 = df_penguins.filter(\n",
    "    col(\"body_mass_g\")>3000\n",
    ")\n",
    "# df_penguins_bmg_mayor_3000.show(3)\n",
    "print(\"Total de datos filtro 1: \",df_penguins_bmg_mayor_3000.count()) ## â¬…ï¸ Cantidad de datos del dataset : 331\n",
    "\n",
    "## ðŸ’¡ EJEMPLO 2: FILTRAR VALORES DE COLUMNA \"body_mass_g\" ENTRE 1000 y 3000\n",
    "df_penguins_bmg_entre_1000_3000 = df_penguins.filter(\n",
    "    (col(\"body_mass_g\")>=1000) &\n",
    "    (col(\"body_mass_G\")<=3000)\n",
    ")\n",
    "# df_penguins_bmg_entre_1000_3000.show(3)\n",
    "print(\"Total de datos filtro 2: \",df_penguins_bmg_entre_1000_3000.count()) ## â¬…ï¸ Cantidad de datos del dataset : 11\n",
    "\n",
    "## ðŸ’¡ EJEMPLO 3: FILTRAR VALORES NULOS EN COLUMNA \"body_mass_g\"\n",
    "df_penguins_nulos_body_mass_g = df_penguins.filter(\n",
    "    col(\"body_mass_g\").isNull()\n",
    ")\n",
    "# df_penguins_nulos_body_mass_g.show(3)\n",
    "print(\"Total de datos filtro 3: \",df_penguins_nulos_body_mass_g.count()) ## â¬…ï¸ Cantidad de datos del dataset : 2\n",
    "\n",
    "## ðŸ’¡ EJEMPLO 4: FILTRAR VALORES NO NULOS EN COLUMNA \"body_mass_g\"\n",
    "df_penguins_no_nulos_body_mass_g = df_penguins.filter(\n",
    "    col(\"body_mass_g\").isNotNull()\n",
    ")\n",
    "# df_penguins_no_nulos_body_mass_g.show(3)\n",
    "print(\"Total de datos filtro 4: \",df_penguins_no_nulos_body_mass_g.count()) ## â¬…ï¸ Cantidad de datos del dataset : 342\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5449ee6e-c753-4928-9c42-9dc485c6c3a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e261f97-fd33-4ee1-97f2-68d5d4c67e33",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    D). RELLENANDO DATOS Null DE LAS COLUMNAS DE UN DATASET\n",
    "    ðŸ“ SINTAXIS:\n",
    "        df_penguins = df_penguins.fillna(value=ValorARellenar,subset=NombreColumna)\n",
    "        ### ðŸ§  Importante: En este caso, los valores null deben rellenarse con datos\n",
    "                            del mismo tipo que la columna. Por ende, debemos llenar \n",
    "                            individualmante.\n",
    "\"\"\"\n",
    "### âž¡ï¸ Verificar datos nulos\n",
    "cantidad_nulos = df_penguins.select([\n",
    "    sum(when(col(i).isNull(),1).otherwise(0)).alias(i)\n",
    "    for i in df_penguins.columns\n",
    "])\n",
    "# cantidad_nulos.show()\n",
    "\n",
    "### âž¡ï¸ Verificar datos nulos que han sido rellenados.\n",
    "df_penguins = df_penguins.fillna(value=15.55,subset=[\"body_mass_g\"])\n",
    "# df_penguins.show()\n",
    "\n",
    "cantidad_nulos.show() ##  âœ…Datos nulos rellanados correctamente.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ca1e525f-40db-4358-bcff-570cd6d4fb79",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### FASE 4. AGRUPAMIENTO DE INFORMACIÃ“N"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b836edac-a8cf-4b22-b47a-abd9201f42ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "El agrupamiento de la informaciÃ³n nos brinda el resumen de la misma proveniente \n",
    "de un dataset, logrando encontrar ciertos patrones en cada grupo de informaciÃ³n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3864eede-5013-4d89-8a75-06d9d7ff4aac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    ðŸ“ SINTAXIS:\n",
    "\n",
    "        dataset_nuevo_agrupamiento= dataset_nombre.groupBy([\"NombreColumna1\",\"NombreColumna2]).agg(\n",
    "            funciÃ³nAgregaciÃ³n(col(\"NombreColumna\")).alias(\"NuevoNombreColumna\")\n",
    "        )\n",
    "        \n",
    "        ### ðŸ§  En este caso, debemos almacenar en una variable los cambios a realizar en el dataframe.\n",
    "\n",
    "âŽ En el agrupamiento de informaciÃ³n, podemos utilizar diversas funciones de agregaciÃ³n, tales como:\n",
    "\n",
    "ðŸ’¡.min():    â¬…ï¸ Permite obtener el mÃ­nimo valor de la informaciÃ³n agrupada.\n",
    "ðŸ’¡.max():    â¬…ï¸ Permite obtener el mÃ¡ximo valor de la informaciÃ³n agrupada.\n",
    "ðŸ’¡.sum():    â¬…ï¸ Permite sumar la informaciÃ³n de una columna cuantitativa por la informaciÃ³n agrupada.\n",
    "ðŸ’¡.count():  â¬…ï¸ Permite contar la cantidad de informaciÃ³n de una columna por la informaciÃ³n agrupada.\n",
    "ðŸ’¡.mean():   â¬…ï¸ Permite obtener la media de una columna cuantitativa por la informaciÃ³n agrupada.\n",
    "ðŸ’¡.median(): â¬…ï¸ Permite obtener la mediana de una columna cuantitativa por la informaciÃ³n agrupada.\n",
    "\"\"\"\n",
    "###âœ”ï¸ Usaremos el dataset de \"penguins\"\n",
    "df_penguins = spark.sql(\"SELECT * FROM workspace.exercises.penguins\") ## â¬…ï¸ Tabla Delta\n",
    "# df_penguins.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4bc7f87d-8b2f-401f-b435-4ae6193f6c62",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## ðŸ’¡ EJEMPLO 1: (AGRUPANDO POR UNA COLUMNA) \n",
    "df_agrupado_uno = df_penguins.groupBy([\"species\"]).agg(\n",
    "    round(mean(col(\"body_mass_g\")),2).alias(\"PromedioBMG\") ## â¬…ï¸ Hallamos la media de la columna \"body_mass_g\"\n",
    ")\n",
    "# df_agrupado_uno.show()\n",
    "\n",
    "## ðŸ’¡ EJEMPLO 2: (AGRUPAR POR DOS COLUMNAS)\n",
    "df_agrupado_dos = df_penguins.groupBy([\"species\",\"sex\"]).agg(\n",
    "    round(mean(col(\"body_mass_g\")),2).alias(\"Species_Sex_PromedioBMG\")\n",
    ")\n",
    "# df_agrupado_dos.show()\n",
    "\n",
    "## ðŸ’¡ EJEMPLO 3: (AGRUPAR LA INFORMACIÃ“N POR LA CANTIDAD DE LA MISMA COLUMNA)\n",
    "\n",
    "df_agrupado_tres = df_penguins.groupBy([\"species\"]).agg(\n",
    "    count(col(\"species\")).alias(\"Cantidad\") ## â¬…ï¸ .count() : funciÃ³n clave para traer la cantidad de datos de la columna.\n",
    ")\n",
    "df_agrupado_tres.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0e0519ec-0a67-4296-89d6-20828056d38b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### FASE 4. CÃLCULOS MÃ“VILES EN PYSPARK - DATABRICKS "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d842a223-5bd8-41d9-94f3-5aaf127b5dab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "AsÃ­ como SQL SERVER permite realizar cÃ¡lculos mÃ³viles basados en funciones de agregaciÃ³n\n",
    "(min,max,mean,entre otros), Pyspark lo realiza mediante el mÃ³dulo de pypspark.sql.function \n",
    "simulando el lenguaje T-SQL en su estructura."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a3abce2-23a7-4f5c-94cf-ec1acf9e3e0a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    ðŸ’¡ Importante: Para realizar el cÃ¡lculo movil utilizaremos la funciÃ³n .select() para\n",
    "                    crear la columna mÃ³vil. AdemÃ¡s, en Pyspark debemos definir una configuraciÃ³n\n",
    "                    de la ventana deslizante en el caso de cÃ¡lculos mÃ³viles mediante: \n",
    "                    >>> from pyspark.sql.window import Window <<< para utilizarlo en .over()\n",
    "    ðŸ“ SINTAXIS: \n",
    "        1. CÃ¡lculo MÃ³vil sin Particionamiento: Permite el cÃ¡lculo mÃ³vil de una columna que no es afectada por otra(s).\n",
    "\n",
    "            from pyspark.sql.window import Window\n",
    "            dataset_nombre_calculos_moviles = dataset_nombre_original ### â¬…ï¸ Copiamos el dataset original\n",
    "            â¬‡ï¸ ConfiguraciÃ³n de la ventana \n",
    "            window_configuracion = Window.orderBy(NombreColumna1,NombreColumnaN) ### â¬…ï¸ Ordenamos por una columna\n",
    "            dataset_nombre_calculos_moviles = dataset_nombre_calculos_moviles.select( ### â¬…ï¸ FunciÃ³n .select()\n",
    "                col(NombreColumna1),col(NombreColumna2), ### â¬…ï¸ Seleccionamos las columnas necesarias\n",
    "                funcionAgregacion(col(NombreColumnaCalculo)).over(window_configuracion).alias(NombreColumnaCalculoMovil)\n",
    "            )\n",
    "\n",
    "        2. CÃ¡lculo MÃ³vil con Particionamiento: Permite el cÃ¡lculo mÃ³vil de una columna que es afectada por otra(s).\n",
    "\n",
    "            from pyspark.sql.window import Window\n",
    "            dataset_nombre_calculos_moviles = dataset_nombre_original ### â¬…ï¸ Copiamos el dataset original\n",
    "            â¬‡ï¸ ConfiguraciÃ³n de la ventana \n",
    "            window_configuracion = Window.partitionBy(NombreColumna1,NombreColumnaN) ### â¬…ï¸ Particionamos por una columna\n",
    "                                         .orderBy(NombreColumna1,NombreColumnaN) ### â¬…ï¸ Ordenamos por una columna\n",
    "                                         .rowsBetween(CantidadFilasAtras,FilaActual) ### â¬…ï¸ Filas que formarÃ¡n\n",
    "                                                                                            parte del cÃ¡lculo mÃ³vil.\n",
    "                ### EXPLICACIÃ“N DE ROWSBETWEEN\n",
    "                .rowsBetween(-2,0): En este caso, el 0 indica utilizar la fina actual.\n",
    "                .rowsBetween(-2,1): Cualquier otro nÃºmero que no sea 0 indica que se utilizarÃ¡ la fina actual.\n",
    "\n",
    "            dataset_nombre_calculos_moviles = dataset_nombre_calculos_moviles.select( ### â¬…ï¸ FunciÃ³n .select()\n",
    "                col(NombreColumna1),col(NombreColumna2), ### â¬…ï¸ Seleccionamos las columnas necesarias\n",
    "                funcionAgregacion(col(NombreColumnaCalculo)).over(window_configuracion).alias(NombreColumnaCalculoMovil)\n",
    "            )\n",
    "\"\"\"\n",
    "### ðŸ’¹ USAREMOS EL SIGUIENTE DATASET DE EJEMPLO:\n",
    "df_ventas = spark.sql(\"SELECT * FROM workspace.default.ventas_sac\")\n",
    "df_ventas.show(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c3f3e335-0e59-4a90-9902-d0c8bb3656d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "### ðŸ’¡ EJEMPLO 1: CÃLCULO MÃ“VIL âž¡ï¸ VENTAS ACUMULATIVAS (SUMA ACUMULADA SIN PARTICIONES)\n",
    "from pyspark.sql.window import Window\n",
    "df_ventas_acumuladas = df_ventas\n",
    "window_ejemplo_1 = Window.orderBy(\"nro_venta\") ## â¬…ï¸ Como no particionamos informaciÃ³n, necesitamos ordenarlas.\n",
    "df_ventas_acumuladas = df_ventas_acumuladas.select(\n",
    "    \"*\", ## â¬…ï¸ De esta manera seleccionamos todas las columnas\n",
    "    round(sum(col(\"total\")).over(window_ejemplo_1),2).alias(\"ventas_acumuladas\")\n",
    ")\n",
    "# df_ventas_acumuladas.show(6)\n",
    "\n",
    "### ðŸ’¡ EJEMPLO 2: CÃLCULO MÃ“VIL âž¡ï¸ VENTAS ACUMULATIVAS (SUMA ACUMULADA CADA 3 DIAS) (âœ… INCLUYE FILA ACTUAL)\n",
    "df_ventas_acumuladas_3_dias = df_ventas\n",
    "window_ejemplo_2 = Window.orderBy(\"nro_venta\").rowsBetween(-2,0) ###â¬…ï¸ ConfiguraciÃ³n de la ventana a crear.\n",
    "df_ventas_acumuladas_3_dias = df_ventas_acumuladas_3_dias.select(\n",
    "    \"*\", ## â¬…ï¸ De esta manera seleccionamos todas las columnas\n",
    "    round(sum(col(\"total\")).over(window_ejemplo_2),2).alias(\"ventas_acumuladas_3_dias\")\n",
    ") \n",
    "# df_ventas_acumuladas_3_dias.show(6) ### (âœ… Incluye la fiLa actual - âŒNo particionamos).\n",
    "\n",
    "### ðŸ’¡ EJEMPLO 3: CÃLCULO MÃ“VIL âž¡ï¸ VENTAS ACUMULATIVAS (SUMA ACUMULADA CADA 3 DIAS) (âŒ NO INCLUYE FILA ACTUAL)\n",
    "df_ventas_acumuladas_3_dias_sin_fila_actual = df_ventas\n",
    "window_ejemplo_3 = Window.orderBy(\"nro_venta\").rowsBetween(-3,-1) ###â¬…ï¸ ConfiguraciÃ³n de la ventana a crear.\n",
    "df_ventas_acumuladas_3_dias_sin_fila_actual = df_ventas_acumuladas_3_dias_sin_fila_actual.select(\n",
    "    \"*\", ## â¬…ï¸ De esta manera seleccionamos todas las columnas\n",
    "    round(sum(col(\"total\")).over(window_ejemplo_3),2).alias(\"ventas_acumuladas_3_dias\")\n",
    ") \n",
    "# df_ventas_acumuladas_3_dias_sin_fila_actual.show(6) ### (âŒ No incluye la fiLa actual - âŒNo particionamos).\n",
    "\n",
    "### ðŸ’¡ EJEMPLO 4: CÃLCULO MÃ“VIL âž¡ï¸ VENTAS ACUMULATIVAS (SUMA ACUMULADA CON PARTICIONES)\n",
    "df_ventas_acumuladas_particionadas = df_ventas\n",
    "window_ejemplo_4 = Window.partitionBy(\"cliente\").rowsBetween(Window.unboundedPreceding,0) ## â¬…ï¸ ConfiguraciÃ³n ventana âœ…\n",
    "##window_ejemplo_4=Window.partitionBy(\"cliente\") ## â¬…ï¸ ConfiguraciÃ³n ventana âŒ (Arroja el total final acumulado en cada fila)\n",
    "df_ventas_acumuladas_particionadas = df_ventas_acumuladas_particionadas.select(\n",
    "    \"*\", ## â¬…ï¸ De esta manera seleccionamos todas las columnas\n",
    "    round(sum(col(\"total\")).over(window_ejemplo_4),2).alias(\"ventas_acumuladas\")\n",
    ")\n",
    "# df_ventas_acumuladas_particionadas.show(5) ## âœ… .over() permite el particionamiento junto a la window_especificaciÃ³n\n",
    "\n",
    "#==========================================================================================================================\n",
    "### ðŸ’¡ EJEMPLO 5: CÃLCULO MÃ“VIL âž¡ï¸ PROMEDIO MÃ“VIL (PROMEDIO CADA 3 DÃAS) (âœ… INCLUYE FILA ACTUAL)\n",
    "df_ventas_promedio_3_dias = df_ventas\n",
    "window_ejemplo_5 = Window.orderBy(\"nro_venta\").rowsBetween(-2,0)\n",
    "df_ventas_promedio_3_dias = df_ventas_promedio_3_dias.select(\n",
    "    \"*\", ## â¬…ï¸ De esta manera seleccionamos todas las columnas\n",
    "    round(avg(col(\"total\")).over(window_ejemplo_5),2).alias(\"promedio_ventas_3_dias\")\n",
    ")\n",
    "# df_ventas_promedio_3_dias.show(5)\n",
    "\n",
    "### ðŸ’¡ EJEMPLO 6: CÃLCULO MÃ“VIL âž¡ï¸ PROMEDIO MÃ“VIL (PROMEDIO CADA 3 DÃAS) (âŒ NO INCLUYE FILA ACTUAL)\n",
    "df_ventas_promedio_3_dias_sin_fila_actual = df_ventas\n",
    "window_ejemplo_6 = Window.orderBy(\"nro_venta\").rowsBetween(-3,-1)\n",
    "df_ventas_promedio_3_dias_sin_fila_actual = df_ventas_promedio_3_dias_sin_fila_actual.select(\n",
    "    \"*\", ## â¬…ï¸ De esta manera seleccionamos todas las columnas\n",
    "    round(avg(col(\"total\")).over(window_ejemplo_6),2).alias(\"promedio_ventas_3_dias\")\n",
    ")\n",
    "# df_ventas_promedio_3_dias_sin_fila_actual.show(5)\n",
    "\n",
    "#========================================================================================================================\n",
    "### ðŸ’¡ EJEMPLO 7: CÃLCULO MÃ“VIL âž¡ï¸ MÃXIMO MÃ“VIL (VALOR MÃXIMO CADA 3 DÃAS) (âœ… INCLUYE FILA ACTUAL)\n",
    "df_ventas_maximo_3_dias = df_ventas\n",
    "window_ejemplo_7 = Window.orderBy(\"nro_venta\").rowsBetween(-2,0)\n",
    "df_ventas_maximo_3_dias = df_ventas_maximo_3_dias.select(\n",
    "    \"*\", ## â¬…ï¸ De esta manera seleccionamos todas las columnas\n",
    "    max(col(\"total\")).over(window_ejemplo_7).alias(\"valor_maximo_3_dias\")\n",
    ")\n",
    "# df_ventas_maximo_3_dias.show(5)\n",
    "### ðŸ’¡ EJEMPLO 8: CÃLCULO MÃ“VIL âž¡ï¸ MÃXIMO MÃ“VIL (VALOR MÃXIMO CADA 3 DÃAS) (âŒ NO INCLUYE FILA ACTUAL)\n",
    "df_ventas_maximo_3_dias_sin_fila_actual = df_ventas\n",
    "window_ejemplo_8 = Window.orderBy(\"nro_venta\").rowsBetween(-3,-1)\n",
    "df_ventas_maximo_3_dias_sin_fila_actual = df_ventas_maximo_3_dias_sin_fila_actual.select(\n",
    "    \"*\", ## â¬…ï¸ De esta manera seleccionamos todas las columnas\n",
    "    max(col(\"total\")).over(window_ejemplo_8).alias(\"valor_maximo_3_dias\")\n",
    ")\n",
    "# df_ventas_maximo_3_dias_sin_fila_actual.show(5)\n",
    "\n",
    "#=======================================================================================================================\n",
    "### ðŸ’¡ EJEMPLO 9: CÃLCULO MÃ“VIL âž¡ï¸ MÃNIMO MÃ“VIL (VALOR MÃNIMO CADA 3 DÃAS) (âœ… INCLUYE FILA ACTUAL)\n",
    "df_ventas_minimo_3_dias = df_ventas\n",
    "window_ejemplo_9 = Window.orderBy(\"nro_venta\").rowsBetween(-2,0)\n",
    "df_ventas_minimo_3_dias = df_ventas_minimo_3_dias.select(\n",
    "    \"*\", ## â¬…ï¸ De esta manera seleccionamos todas las columnas\n",
    "    min(col(\"total\")).over(window_ejemplo_9).alias(\"valor_minimo_3_dias\")\n",
    ")\n",
    "# df_ventas_minimo_3_dias.show(5)\n",
    "\n",
    "### ðŸ’¡ EJEMPLO 10: CÃLCULO MÃ“VIL âž¡ï¸ MÃNIMO MÃ“VIL (VALOR MÃNIMO CADA 3 DÃAS) (âŒ NO INCLUYE FILA ACTUAL)\n",
    "df_ventas_minimo_3_dias_sin_fila_actual = df_ventas\n",
    "window_ejemplo_10 = Window.orderBy(\"nro_venta\").rowsBetween(-3,-1)\n",
    "df_ventas_minimo_3_dias_sin_fila_actual = df_ventas_minimo_3_dias_sin_fila_actual.select(\n",
    "    \"*\", ## â¬…ï¸ De esta manera seleccionamos todas las columnas\n",
    "    min(col(\"total\")).over(window_ejemplo_10).alias(\"valor_minimo_3_dias\")\n",
    ")\n",
    "df_ventas_minimo_3_dias_sin_fila_actual.show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b6061ff6-fcf5-49c2-903e-d6d49a7edb94",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### FASE 5. COMBINAR Y UNIR DATASETS/DATAFRAMES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9687c86d-a1be-4a09-a3e9-4d5b9d3dfcf7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "A diferencia de Pandas y Polars, PySpark permite la unificaciÃ³n de datasets/dataframes mediante tres formas: \n",
    "\n",
    "    - Utilizando Join  (UniÃ³n columnar - Horizontalmente)\n",
    "    - Utilizando union (UniÃ³n a nivel de filas - vertifcalmente) âœ… MÃ¡s riguroso\n",
    "    - Utilizando unionByName (UniÃ³n a nivel de filas - vertifcalmente) âœ… MÃ¡s flexible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ab3fcaba-1ccb-4788-b460-a3cb9bcb8a7e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###### FUNCIÃ“N JOIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a39de5cc-1e96-482f-9ece-b7fde7cb5c99",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    ðŸ“ SINTAXIS: \n",
    "\n",
    "        dataset_unificado_nombre=dataset_nombre_1.join(other=dataset_nombre_2,on='ColumnaEnComun',\n",
    "                                how='left || rigth || inner || full || semi || anti || cross')                                \n",
    "                                \n",
    "        ==================================================================================================  \n",
    "\n",
    "    ### ðŸ’¡Importante: Join permite unificar los datasets/dataframes a nivel columnar, es decir,\n",
    "                       unifica horizontalmente las columnas de un dataset/dataframe A; con las columnas\n",
    "                       de un dataset/dataframe B; gracias a una columna en comÃºn que tienen ambos \n",
    "                       conjuntos de datos.  \n",
    "\"\"\"\n",
    "### âœ… Utilizaremos este dataset de ejemplo:\n",
    "import numpy as np\n",
    "\n",
    "diccionario_uno = {\n",
    "    \"ID_Cliente\":[1,2,3,4,5],\n",
    "    \"Nombre\": [\"Pepito\",\"Juanito\",\"Pedrito\",\"Brayan\",\"Carlos\"],\n",
    "    \"Departamento\":[\"LAS QUINTANAS\",\"EL GOLF\",\"BUENOS AIRES\",\"SAN ANDRÃ‰S\",\"CALIFORNIA\"]\n",
    "}\n",
    "diccionario_dos = {\n",
    "    \"ID_Cliente\":[1,1,2,2,5,4,1,None],\n",
    "    \"Ventas\":np.random.uniform(low=1450.25,high=1980.30,size=8).round(2).tolist()\n",
    "}\n",
    "\n",
    "df_uno = spark.createDataFrame(data=list(zip(*diccionario_uno.values())),schema=list(diccionario_uno.keys()))\n",
    "df_dos = spark.createDataFrame(data=list(zip(*diccionario_dos.values())),schema=list(diccionario_dos.keys()))\n",
    "\n",
    "diccionario_tres = {\n",
    "    \"Cliente_ID\":[1,1,2,2,5,4,1,None],\n",
    "    \"Ventas\":np.random.uniform(low=1450.25,high=1980.30,size=8).round(2).tolist()\n",
    "}\n",
    "df_tres = spark.createDataFrame(data=list(zip(*diccionario_tres.values())),schema=list(diccionario_tres.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "79ac2144-5b5f-4d09-bafe-7ac285d62dbc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "#### ðŸ’¡ EJEMPLO 1 (UNIFICACIÃ“N DE DATAFRAMES [INNER]) \n",
    "df_ejemplo_1 = df_uno.join(other=df_dos,on=\"ID_Cliente\",how=\"inner\")\n",
    "# df_ejemplo_1.count() ## Cantidad de registros: 7\n",
    "# df_ejemplo_1.show(7)\n",
    "## [NO SE ENCUENTRA EL CLIENTE 3] âž¡ï¸ INNER MANTIENE LA INFORMACIÃ“N RELACIONADA Y EXISTENTE ENTRE AMBOS DATASETS\n",
    "\n",
    "#### ðŸ’¡ EJEMPLO 2 (UNIFICACIÃ“N DE DATAFRAMES [LEFT])\n",
    "df_ejemplo_2 = df_uno.join(other=df_dos,on=\"ID_Cliente\",how=\"left\")\n",
    "# df_ejemplo_2.count() ## Cantidad de registros: 8\n",
    "# df_ejemplo_2.show(8)\n",
    "## [SE ENCUENTRA EL CLIENTE 3] âž¡ï¸ LEFT MANTIENE LA INFORMACIÃ“N DEL LADO IZQUIERDO (df_uno) \n",
    "## [ASÃ NO EXISTA ALGÃšN REGISTRO EN EL LADO DERECHO (df_dos)]\n",
    "\n",
    "#### ðŸ’¡ EJEMPLO 3 (UNIFICACIÃ“N DE DATAFRAMES [RIGHT])\n",
    "df_ejemplo_3 = df_uno.join(other=df_dos,on=\"ID_Cliente\",how=\"right\")\n",
    "# df_ejemplo_3.count() ## Cantidad de registros: 8\n",
    "# df_ejemplo_3.show(8)\n",
    "## [SE ENCUENTRA EL CLIENTE 3] âž¡ï¸ RIGHT MANTIENE LA INFORMACIÃ“N DEL LADO DERECHO (df_dos) \n",
    "## [ASÃ NO EXISTA ALGÃšN REGISTRO EN EL LADO IZQUIERDO (df_uno)]\n",
    "\n",
    "#### ðŸ’¡ EJEMPLO 4 (UNIFICACIÃ“N DE DATAFRAMES [FULL] o [OUTER]) â¬…ï¸ AMBOS REALIZAN LO MISMO\n",
    "df_ejemplo_4 = df_uno.join(other=df_dos,on=\"ID_Cliente\",how=\"full\")\n",
    "# df_ejemplo_4.count() ## Cantidad de registros: 9\n",
    "# df_ejemplo_4.show(9)\n",
    "## âž¡ï¸ FULL u OUTER MANTIENE LA INFORMACIÃ“N DE AMBOS DATAFRAMES.\n",
    "\n",
    "#### ðŸ’¡ EJEMPLO 5 (UNIFICACIÃ“N DE DATAFRAMES [CROSS])\n",
    "df_ejemplo_5 = df_uno.join(other=df_dos,how=\"cross\")\n",
    "# df_ejemplo_5.count() ## Cantidad de registros: 40\n",
    "# df_ejemplo_5.show()\n",
    "## âž¡ï¸ CROSS GENERA TODAS LAS COMBINACIONES POSIBLES DE LAS FILAS DEL df_uno CON LAS FILAS DEL df_dos. \n",
    "##     (YA NO NECESITAMOS UTILIZAR EL PARÃMETRO \"on=\")\n",
    "\n",
    "# #### ðŸ’¡ EJEMPLO 6 (UNIFICACIÃ“N DE DATAFRAMES [INNER] CON DIFERENTES NOMBRES)\n",
    "##---- PREVIAMENTE DEBEMOS CAMBIAR EL NOMBRE DE LA COLUMNA DE UNO DE LOS DATAFRAMES\n",
    "#      PARA MANTENER LA INTEGRIDAD\n",
    "# df_tres = df_tres.withColumnRenamed(existing=\"Cliente_ID\",new=\"ID_Cliente\")\n",
    "# df_tres.show(5) â¬…ï¸ NOMBRE CAMBIADO CORRECTAMENTE\n",
    "df_ejemplo_6 = df_uno.join(other=df_tres,how=\"inner\",on=\"ID_Cliente\")\n",
    "# df_ejemplo_6.count() ## Cantidad de registros: 7\n",
    "# df_ejemplo_6.show(7)\n",
    "\n",
    "#### ðŸ’¡ EJEMPLO ANTI (PERMITE DEVOLVER LAS FILAS DE df_uno QUE NO SE ENCUENTRAN EN df_dos)\n",
    "df_ejemplo_anti = df_uno.join(other=df_dos,how=\"anti\",on=\"ID_Cliente\")\n",
    "# df_ejemplo_anti.count() ## Cantidad de registros: 1\n",
    "# df_ejemplo_anti.show()\n",
    "\n",
    "\n",
    "#### ðŸ’¡ EJEMPLO SEMI (PERMITE DEVOLVER LAS FILAS DE df_uno EXISTENTES EN df_dos)\n",
    "####        PERO DEVOLVIENDO SOLO LAS FILAS DE df_uno\n",
    "df_ejemplo_semi = df_uno.join(other=df_dos,how=\"semi\",on=\"ID_Cliente\")\n",
    "# df_ejemplo_semi.count() ## Cantidad de registros: 1\n",
    "# df_ejemplo_semi.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "49c773ce-2ef3-4db9-aec3-d1a7b33346ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###### FUNCIÃ“N UNION, UNIONALL Y UNIONBYNAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2fdc70a6-2958-4234-85f4-3ce9b50fc54c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    ðŸ“ SINTAXIS: \n",
    "\n",
    "        1. Union : Permite unificar datasets/dataframes a nivel de fila (Por posiciones de las columnas).\n",
    "\n",
    "            ðŸ“ SINTAXIS:\n",
    "\n",
    "                dataset_nombre_unificado_final = dataset_nombre_uno.union(other=dataset_nombre_dos) \n",
    "\n",
    "        2. UnionALL : Permite unificar datasets/dataframes a nivel de fila.\n",
    "\n",
    "            ðŸ“ SINTAXIS:\n",
    "\n",
    "                dataset_nombre_unificado_final = dataset_nombre_uno.unionAll(other=dataset_nombre_dos) \n",
    "\n",
    "        1. UnionByName : Permite unificar datasets/dataframes a nivel de fila (Por nombres de las columnas).\n",
    "\n",
    "            ðŸ“ SINTAXIS:\n",
    "\n",
    "                dataset_nombre_unificado_final = dataset_nombre_uno.unionByName(other=dataset_nombre_dos,\n",
    "                                                 allowMissingColumns=True|False)\n",
    "                ðŸ’¡ El parÃ¡metro \"allowMissingColumns\" si estÃ¡ en True, permite que las columnas de un \n",
    "                   dataset que no hagan match con las de otro dataset se vuelvan columnas independientes.\n",
    "                   Sin embargo, si se establece en False ambos datasets deben tener la misma cantidad,\n",
    "                   tipo y nombre en las columnas.\n",
    "        ==================================================================================================  \n",
    "\n",
    "    ### ðŸ’¡Importante: Union,UnionAll y UnionByName permite unificar los datasets/dataframes a nivel fila,\n",
    "                       es decir, unifica verticalmente las columnas de un dataset/dataframe A; con las \n",
    "                       columnas de un dataset/dataframe B; gracias a la(s) columna(s) en comÃºn que tienen\n",
    "                       ambos conjuntos de datos.\n",
    "                       \n",
    "                       Debemos tener en cuenta que en ambos datasets/dataframes se debe tener la misma cantidad\n",
    "                       tipos y orden en las columnas, evitando ciertas inconsistencias en el dataset final.\n",
    "                       AdemÃ¡s, en ninguna de as 3 funciones presentadas anteriormente elimina duplicados a menos\n",
    "                       que utilicemos .distinc() o .dropDuplicates() segÃºn la casuÃ­stica correspondiente. \n",
    "\"\"\"\n",
    "### âœ… Utilizaremos estos dataset de ejemplo:\n",
    "import numpy as np\n",
    "diccionario_uno = {\n",
    "    \"Categorias\":[\"ELECTRODOMÃ‰STICOS\",\"TECNOLOGÃA\",\"JUGUETES\",\"DEPORTE\",\"CULTURA\"],\n",
    "    \"AÃ±o\": [2022,2022,2023,2023,2023],\n",
    "    \"Ventas\":[3565.16, 3723.17, 3614.21, 3552.18, 3572.76]\n",
    "}\n",
    "\n",
    "diccionario_dos = {\n",
    "    \"Categorias\":[\"ELECTRODOMÃ‰STICOS\",\"TECNOLOGÃA\",\"JUGUETES\",\"DEPORTE\",\"CULTURA\"],\n",
    "    \"AÃ±o\": [2024,2024,2024,2025,2025],\n",
    "    \"Ventas\":np.random.uniform(low=2150.25,high=4580.30,size=5).round(2).tolist()\n",
    "}\n",
    "\n",
    "diccionario_tres = {\n",
    "    \"Cat\":[\"ELECTRODOMÃ‰STICOS\",\"TECNOLOGÃA\",\"JUGUETES\",\"DEPORTE\",\"CULTURA\"],\n",
    "    \"AÃ±os\": [2024,2024,2024,2025,2025],\n",
    "    \"Vent\":np.random.uniform(low=2150.25,high=4580.30,size=5).round(2).tolist()\n",
    "}\n",
    "\n",
    "diccionario_cuatro = {\n",
    "    \"Cat\":[\"ELECTRODOMÃ‰STICOS\",\"TECNOLOGÃA\",\"JUGUETES\",\"DEPORTE\",\"CULTURA\"],\n",
    "    \"AÃ±os\": [2024,2024,2024,2025,2025],\n",
    "    \"Vent\":np.random.uniform(low=2150.25,high=4580.30,size=5).round(2).tolist()\n",
    "}\n",
    "\n",
    "diccionario_cinco = {\n",
    "    \"Categorias\":[\"ELECTRODOMÃ‰STICOS\",\"TECNOLOGÃA\",\"JUGUETES\",\"DEPORTE\",\"CULTURA\"],\n",
    "    \"AÃ±o\": [2022,2022,2023,2023,2023],\n",
    "    \"Ventas\":[3565.16, 3723.17, 3614.21, 3552.18, 3572.76]\n",
    "}\n",
    "df_uno = spark.createDataFrame(data=list(zip(*diccionario_uno.values())),schema=list(diccionario_uno.keys()))\n",
    "df_dos = spark.createDataFrame(data=list(zip(*diccionario_dos.values())),schema=list(diccionario_dos.keys()))\n",
    "df_tres = spark.createDataFrame(data=list(zip(*diccionario_tres.values())),schema=list(diccionario_tres.keys()))\n",
    "df_cuatro = spark.createDataFrame(data=list(zip(*diccionario_cuatro.values())),schema=list(diccionario_cuatro.keys()))\n",
    "df_cinco = spark.createDataFrame(data=list(zip(*diccionario_cinco.values())),schema=list(diccionario_cinco.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ebd85a25-11b3-4749-ad0c-f76f68d08848",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "#### âž¡ï¸ UNION:\n",
    "## ------ Datasets con las mismas caracterÃ­sticas en las columnas (Nombre,Tipo,Cantidad) \n",
    "df_ejemplo_1 = df_uno.union(other=df_dos)\n",
    "# df_ejemplo_1.show()\n",
    "\n",
    "## ------ Datasets con las mismas caracterÃ­sticas en Tipo y Cantidad (Nombres diferentes) \n",
    "df_ejemplo_2 = df_uno.union(other=df_tres)\n",
    "# df_ejemplo_2.show()\n",
    "\n",
    "## ------ Datasets con datos duplicados y mismas caracterÃ­sticas en las columnas(Nombres,Tipo,Cantidad) \n",
    "df_ejemplo_3 = df_uno.union(other=df_cinco) ## NO ELIMINAMOS DUPLICADOS\n",
    "# df_ejemplo_3.show()\n",
    "\n",
    "## ------ Datasets con datos duplicados y mismas caracterÃ­sticas en las columnas(Nombres,Tipo,Cantidad) \n",
    "df_ejemplo_4 = df_uno.union(other=df_cinco).dropDuplicates()\n",
    "df_ejemplo_4.show() ## ELIMINAMOS DUPLICADOS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "939b56b6-c6e9-4cee-9499-15eb10d99155",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "#### âž¡ï¸ UNIONALL:\n",
    "## ------ Datasets con las mismas caracterÃ­sticas en las columnas (Nombre,Tipo,Cantidad) \n",
    "df_ejemplo_1 = df_uno.unionAll(other=df_dos)\n",
    "# df_ejemplo_1.show()\n",
    "\n",
    "## ------ Datasets con las mismas caracterÃ­sticas en Tipo y Cantidad (Nombres diferentes) \n",
    "df_ejemplo_2 = df_uno.unionAll(other=df_tres)\n",
    "# df_ejemplo_2.show()\n",
    "\n",
    "## ------ Datasets con datos duplicados y mismas caracterÃ­sticas en las columnas(Nombres,Tipo,Cantidad) \n",
    "df_ejemplo_3 = df_uno.unionAll(other=df_cinco) ## NO ELIMINAMOS DUPLICADOS\n",
    "# df_ejemplo_3.show()\n",
    "\n",
    "## ------ Datasets con datos duplicados y mismas caracterÃ­sticas en las columnas(Nombres,Tipo,Cantidad) \n",
    "df_ejemplo_4 = df_uno.unionAll(other=df_cinco).dropDuplicates()\n",
    "# df_ejemplo_4.show() ## ELIMINAMOS DUPLICADOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f2b8159-951a-4a5e-8d7a-477197beb090",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "#### âž¡ï¸ UNIONBYNAME:\n",
    "## ------ Datasets con las mismas caracterÃ­sticas en las columnas (Nombre,Tipo,Cantidad) \n",
    "df_ejemplo_1 = df_uno.unionByName(other=df_dos,allowMissingColumns=True)\n",
    "# df_ejemplo_1 = df_uno.unionByName(other=df_dos,allowMissingColumns=False) ## Mismo resultado por las caracteristicas de las columnas.\n",
    "# df_ejemplo_1.show()\n",
    "\n",
    "## ------ Datasets con las mismas caracterÃ­sticas en Tipo y Cantidad (Nombres diferentes) \n",
    "# df_ejemplo_2 = df_uno.unionByName(other=df_tres,allowMissingColumns=False) ## Error por los nombres diferentes de columnas.\n",
    "df_ejemplo_2 = df_uno.unionByName(other=df_tres,allowMissingColumns=True) ## Columnas diferentes se vuelven columnas independientes. \n",
    "# df_ejemplo_2.show()\n",
    "\n",
    "## ------ Datasets con datos duplicados y mismas caracterÃ­sticas en las columnas(Nombres,Tipo,Cantidad) \n",
    "df_ejemplo_3 = df_uno.unionByName(other=df_cinco,allowMissingColumns=True) ## NO ELIMINAMOS DUPLICADOS\n",
    "# df_ejemplo_3 = df_uno.unionByName(other=df_cinco,allowMissingColumns=False) ## Mismo resultado por las caracteristicas de las columnas.\n",
    "# df_ejemplo_3.show()\n",
    "\n",
    "## ------ Datasets con datos duplicados y mismas caracterÃ­sticas en las columnas(Nombres,Tipo,Cantidad) \n",
    "df_ejemplo_4 = df_uno.unionByName(other=df_cinco,allowMissingColumns=True).dropDuplicates()\n",
    "\n",
    "## Mismo resultado por las caracteristicas de las columnas.\n",
    "# df_ejemplo_4 = df_uno.unionByName(other=df_cinco,allowMissingColumns=False).dropDuplicates() \n",
    "# df_ejemplo_4.show() ## ELIMINAMOS DUPLICADOS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fd65bf07-aa85-403c-9e7f-925a0ad5b650",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### FASE 6. EXPORTACIÃ“N DE DATASETS/DATAFRAMES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3e037a62-2354-4234-a5a8-fac36575a5cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "En PySpark dentro de Databricks, la exportaciÃ³n de datos cumple el mismo rol crÃ­tico: cerrar el ciclo de manipulaciÃ³n y transformaciÃ³n de datos asegurando que la informaciÃ³n ya procesada quede disponible para su consumo, integraciÃ³n o almacenamiento.Sin embargo, la diferencia clave es que, bajo Unity Catalog, toda la gestiÃ³n de datos se centraliza en un catÃ¡logo/esquema/volumen-tabla que permite una seguridad y gobernanza en la informaciÃ³n. Los siguientes formatos para exportar los datasets serÃ¡n: CSV, PARQUET, DELTA TABLE y ARCHIVO DELTA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85dd3ce0-1602-4df0-ac4b-380e5e16cf81",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "### ðŸ’¡ UTLIZAREMOS DE EJEMPLO ESTE DATASET\n",
    "data = {\n",
    "    \"id\": [1, 2, 3, 3, None],\n",
    "    \"nombre\": [\"Ana\", \"Luis\", \"Karla\", \"Karla\", \"Pedro\"],\n",
    "    \"edad\": [23, 35, 29, 29, None]\n",
    "}\n",
    "df = spark.createDataFrame(data=list(zip(*data.values())),schema=list(data.keys()))\n",
    "# df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5c299c3-8f15-4a78-94b9-8881fa57b738",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "### âœ… Realizaremos una Limpieza bÃ¡sica\n",
    "df = df.dropna()                 # eliminar filas con valores nulos\n",
    "df = df.drop_duplicates()        # eliminar duplicados\n",
    "df = df.withColumn(\n",
    "    \"id\",\n",
    "    col(\"id\").cast(dataType=IntegerType()) # castear columna a entero\n",
    ")\n",
    "df = df.withColumnsRenamed({\"nombre\":\"Nombre\",\"edad\":\"Edad\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62b4b327-d9fd-49ca-8874-bcf13d001fc6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "### ðŸ’¡CREAREMOS UN CATALAGO y UN ESQUEMA PARA ALMACENAR LOS ARCHIVOS A EXPORTAR.\n",
    "##----1. CREAMOS EL CATALAGO\n",
    "spark.sql(\"CREATE CATALOG IF NOT EXISTS catalago_exportacion_archivos\")\n",
    "print(\"Catalago creado correctamente.\")\n",
    "##----2. CREAMOS EL ESQUEMA\n",
    "spark.sql(\"CREATE SCHEMA IF NOT EXISTS catalago_exportacion_archivos.esquema_exportacion_archivos\")  \n",
    "print(\"Esquema creado correctamente.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ca29874-ce90-4b47-82b6-17926e024efd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "##---- âœ…ðŸ—ƒï¸ EXPORTANDO A FORMATO CSV (COMMA SEPARATED VALUES) [CREAMOS UN VOLUMEN INDEPENDIENTE]\n",
    "## 1. CREAMOS EL VOLUMEN:\n",
    "spark.sql(\"CREATE VOLUME IF NOT EXISTS catalago_exportacion_archivos.esquema_exportacion_archivos.volumen_csv\")\n",
    "print(\"Volumen CSV creado correctamente.\")\n",
    "## 2. EXPORTAMOS/ALMACENAMOS EL ARCHIVO EN EL VOLUMEN:\n",
    "df.write.option(\"header\",\"true\").option(\"inferSchema\",\"true\").mode('overwrite').csv(\"/Volumes/catalago_exportacion_archivos/esquema_exportacion_archivos/volumen_csv\")\n",
    "print(\"Se exportÃ³ el dataframe en modo CSV correctamente.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37fe2b7a-3514-4c2a-9ad7-f6cb0ff9e9c3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "##---- âœ…ðŸ—ƒï¸ EXPORTANDO A FORMATO PARQUET (FORMATO COLUMNAR OPTIMIZADO) [CREAMOS UN VOLUMEN INDEPENDIENTE]\n",
    "## 1. CREAMOS EL VOLUMEN:\n",
    "spark.sql(\"CREATE VOLUME IF NOT EXISTS catalago_exportacion_archivos.esquema_exportacion_archivos.volumen_parquet\")\n",
    "print(\"Volumen PARQUET creado correctamente.\")\n",
    "## 2. EXPORTAMOS/ALMACENAMOS EL ARCHIVO EN EL VOLUMEN:\n",
    "df.write.mode('overwrite').parquet(\"/Volumes/catalago_exportacion_archivos/esquema_exportacion_archivos/volumen_parquet\")\n",
    "print(\"Se exportÃ³ el dataframe en modo PARQUET correctamente.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "34d20ae7-d19c-4e4a-8f2c-7280a753d39f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "##---- âœ…ðŸ—ƒï¸ EXPORTANDO A FORMATO ARCHIVO DELTA (ARCHIVO FÃSICO DE DATOS QUE NO ES GOBERNADO POR UNITY CATALOG COMO ENTIDAD) [CREAMOS UN VOLUMEN INDEPENDIENTE]\n",
    "## 1. CREAMOS EL VOLUMEN:\n",
    "spark.sql(\"CREATE VOLUME IF NOT EXISTS catalago_exportacion_archivos.esquema_exportacion_archivos.volumen_archivo_delta\")\n",
    "print(\"Volumen de ARCHIVO DELTA creado correctamente.\")\n",
    "## 2. EXPORTAMOS/ALMACENAMOS EL ARCHIVO EN EL VOLUMEN:\n",
    "df.write.format('delta').mode(\"overwrite\").save(\"/Volumes/catalago_exportacion_archivos/esquema_exportacion_archivos/volumen_archivo_delta\")\n",
    "print(\"Se exportÃ³ el dataframe en modo ARCHIVO DELTA correctamente.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b50fba8e-ddb1-4207-8929-1dd9df1866fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "##---- âœ…ðŸ—ƒï¸ EXPORTANDO A FORMATO DELTA TABLE (TABLA OPTIMIZADA PARA SU USO EFICIENTE EN LECTURA/ESCRITURA DE DATOS)\n",
    "#  [CREAMOS DIRECTAMENTE LA TABLA RELACIONADA A UN CATÃLAGO Y ESQUEMA]\n",
    "## 1. EXPORTAMOS/ALMACENAMOS EL ARCHIVO DIRECTAMENTE EN UNA TABLA\n",
    "df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"catalago_exportacion_archivos.esquema_exportacion_archivos.delta_table\")\n",
    "print(\"Se exportÃ³ el dataframe a una DELTA TABLE correctamente.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d3e68eae-2be1-4a80-9ca4-b19ff1f171ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###### PUEDES REVISAR LA IMAGEN OutPutExportacionDatosDatabricks.png PARA VER LA ESTRUCTURA DE ESTA FASE."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "07_Manipulacion_Datos_SparkSQL-Databricks",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bdb171e4-c56f-4d12-89cd-a63089567db4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## MANIPULACI√ìN DE DATOS (SPARK SQL EN DATABRICKS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e73c2979-6912-4eb3-8c88-8372565878c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession # Puerta de entrada para trabajar con spark <-- SIEMPRE DEBEMOS IMPORTAR LA LLAVE MAESTRA QUE INICIA TODO.\n",
    "from pyspark.sql.functions import *  # Funciones propias del m√≥dulo SQL de Spark, para trabajar sobre Dataframes.\n",
    "\n",
    "spark = SparkSession.builder.appName(\"DatabricksSparkSQL\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "13c00770-281a-4559-93e1-ded0f0126c1c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### DATAFRAMES (Versi√≥n Databricks Free Edition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cb3fe8f8-cb2c-4c2e-ba4f-25a579e89cad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Link de [Databrick Free Edition](https://dbc-89f542f8-2df6.cloud.databricks.com/?o=758509963140561)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7c9c736d-562b-46a1-9da2-1af1e2bb35f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Para abordar el tema de **Manipulaci√≥n de Datos**, utilizaremos una de las principales herramientas que ofrece **Databricks: Spark SQL**.  \n",
    "Esta herramienta nos permite aplicar conceptos del lenguaje SQL sobre conjuntos de datos estructurados, ya sea que provengan directamente de archivos o est√©n gobernados a trav√©s del servicio **Unity Catalog**.\n",
    "\n",
    "Gracias a Spark SQL, podremos consultar, transformar y analizar datos de forma eficiente, aprovechando tanto la potencia del motor distribuido de Spark como la organizaci√≥n l√≥gica que brinda Unity Catalog.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "977fe4c5-7621-4161-ac95-b38d326688d9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### FASE 1. FUENTES DE DATOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "24241b3b-f046-4c91-9e20-b3c7610058ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#### ‚û°Ô∏è Como se ven√≠a explicando, Unity Catalog trabaja bajo una jerarqu√≠a de Cat√°lagos, Esquemas, Volumenes y Tablas.\n",
    "####     Por ende, es diferente la manera de acceder a su contenido.\n",
    "\n",
    "#====== üìå LEER ARCHIVO CSV \n",
    "##üìù SINTAXIS:\n",
    "#   dataset_nombre = spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").csv(\"/Volumes/NombreCatalago/NombreEsquema/NombreArchivoCSV\")\n",
    "\n",
    "#---> spark.read : Permite leer un archivo basado en una ruta. Siempre instanciando primero SparkSession.\n",
    "#---> .option(\"header\",\"true||false\") : Permite establecer si tomar√° en cuenta los encabezados del dataset. true(mostrar) | false(ocultar)\n",
    "#---> .option(\"inferSchema\",\"true||false\") : Permite reconocer el tipo de dato de cada columna del dataset. true(mostrar) | false(ocultar)\n",
    "#---> .csv(RutaArchivoCSV) : Tipo de archivo a leer (En este caso un CSV).\n",
    "\n",
    "## üìù EJEMPLO:\n",
    "df_titanic_csv = spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").csv(\"/Volumes/explicacion_unity_catalog/ejemplos/csv_titanic\")\n",
    "# df_titanic_csv.show(5)\n",
    "\n",
    "#====== üìå LEER ARCHIVO PARQUET\n",
    "##üìù SINTAXIS:\n",
    "#   dataset_nombre = spark.read.parquet(\"/Volumes/NombreCatalago/NombreEsquema/NombreArchivoPARQUET\")\n",
    "\n",
    "#---> spark.read : Permite leer un archivo basado en una ruta. Siempre instanciando primero SparkSession.\n",
    "#---> .parquet(RutaArchivoPARQUET) : Tipo de archivo a leer (En este caso un PARQUET).\n",
    "\n",
    "## üìù EJEMPLO\n",
    "df_titanic_parquet = spark.read.parquet(\"/Volumes/explicacion_unity_catalog/ejemplos/parquet_titanic\")\n",
    "# df_titanic_parquet.show(5)\n",
    "\n",
    "#====== üìå LEER ARCHIVO DELTA\n",
    "## üìù SINTAXIS:\n",
    "#   dataset_nombre = spark.read.format(\"delta\").load(\"/Volumes/NombreCatalago/NombreEsquema/NombreArchivoDELTA\")\n",
    "\n",
    "#---> spark.read : Permite leer un archivo basado en una ruta. Siempre instanciando primero SparkSession.\n",
    "#---> .format(\"delta\") : stablece el tipo de archivo a leer (En este caso aplica siempre para DELTA)\n",
    "#---> .load(RutaArchivoDelta) : Aplica solo a los archivos DELTA.\n",
    "\n",
    "## üìù EJEMPLO:\n",
    "df_titanic_archivo_delta = spark.read.format(\"delta\").load(\"/Volumes/explicacion_unity_catalog/ejemplos/archivo_delta_titanic\")\n",
    "# df_titanic_archivo_delta.show(5)\n",
    "\n",
    "#====== üìå LEER TABLA DELTA (RECOMENDADO Y NATIVO EN DATABRICKS-UNITY-CATALOG)\n",
    "## üìù SINTAXIS:\n",
    "#   dataset_nombre = spark.sql(\"SELECT * FROM NombreCatalago.NombreEsquema.NombreTablaDelta\")\n",
    "\n",
    "#---> spark.sql() : Permite leer mediante lenguaje SQL una tabla delta. Siempre instanciando primero SparkSession.\n",
    "#---> \"NombreCatalago.NombreEsquema.NombreTablaDelta\" : Ruta aplicable solo para las tablas delta y gobernadas por Unity Catalog\n",
    "\n",
    "## üìù EJEMPLO\n",
    "df_titanic_delta = spark.sql(\"SELECT * FROM explicacion_unity_catalog.ejemplos.delta_titanic\")\n",
    "# df_titanic_delta.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7cd4a063-2337-43a2-bc32-7e739882d27a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### FASE 2. EXPLORACI√ìN INICIAL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "072cc5f7-fa2f-4206-959d-80b0914f90e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Utilizaremos de ejemplo el dataset de titanic ( en formato tabla delta y CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e253d9b-3aa8-43e0-b929-69872197e5f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#====== A). Quitar encabezados originales provenientes del Dataset (Archivos CSV).\n",
    "df_titanic_archivo_delta = spark.read.csv(\"/Volumes/explicacion_unity_catalog/ejemplos/csv_titanic\") ## ‚¨ÖÔ∏è Archivo CSV\n",
    "df_titanic_archivo_delta.show(3)\n",
    "#### üß†üí° Importante: Cuando leamos un archivo CSV, si no establecemos .option(\"header\",\"true\")\n",
    "####                   el dataframe mostrar√° las columnas con un prefijo c_ seguido de una posici√≥n asignada.\n",
    "####                   (Y los nombres de columnas ser√°n parte de los registros del dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "38f7d025-9d30-4880-9c32-74776c068d0d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#====== B). Mostrar los N primeros registros de un dataset.\n",
    "\n",
    "df_titanic_delta = spark.sql(\"SELECT * FROM explicacion_unity_catalog.ejemplos.delta_titanic\") ## ‚¨ÖÔ∏è Tabla delta\n",
    "\n",
    "#### üìù SINTAXIS: dataset_nombre.show(N√∫meroDeRegistros)\n",
    "\n",
    "#### üìù EJEMPLO 1:\n",
    "# df_titanic_delta.show(10) ## ‚¨ÖÔ∏è Mostrar los 10 primeros registros.\n",
    "\n",
    "#### üìù EJEMPLO 2:\n",
    "# df_titanic_delta.show(5) ## ‚¨ÖÔ∏è Mostrar los 5 primeros registros.\n",
    "\n",
    "#### üìù EJEMPLO 3:\n",
    "df_titanic_delta.show(3) ## ‚¨ÖÔ∏è Mostrar los 3 primeros registros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb5e834b-4a6d-4600-b862-da940d8ecb6e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#====== C). Mostrar los N √∫ltimos registros de un dataset (Retornado en una lista).\n",
    "\n",
    "df_titanic_delta = spark.sql(\"SELECT * FROM explicacion_unity_catalog.ejemplos.delta_titanic\") ## ‚¨ÖÔ∏è Tabla delta\n",
    "\n",
    "#### üìù SINTAXIS: dataset_nombre.tail(N√∫meroDeRegistros)\n",
    "\n",
    "#### üìù EJEMPLO 1:\n",
    "# df_titanic_delta.tail(10) ## ‚¨ÖÔ∏è Mostrar los 10 primeros √∫ltimos.\n",
    "\n",
    "#### üìù EJEMPLO 2:\n",
    "# df_titanic_delta.tail(5) ## ‚¨ÖÔ∏è Mostrar los 5 primeros √∫ltimos.\n",
    "\n",
    "#### üìù EJEMPLO 3:\n",
    "df_titanic_delta.tail(3) ## ‚¨ÖÔ∏è Mostrar los 3 primeros √∫ltimos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "04482e74-468a-4215-8dad-45de1268436e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#====== D). Mostrar el vol√∫men del dataset (Cantidad de filas y columnas respectivamente)\n",
    "\n",
    "df_titanic_delta = spark.sql(\"SELECT * FROM explicacion_unity_catalog.ejemplos.delta_titanic\") ## ‚¨ÖÔ∏è Tabla delta\n",
    "\n",
    "#### üìù SINTAXIS: \n",
    "#       dataset_nombre.count() ‚¨ÖÔ∏è Cantidad de filas\n",
    "#       len([i for i in dataframe_nombre.columns]) ‚¨ÖÔ∏è Cantidad de columnas\n",
    "\n",
    "#### üìù EJEMPLO 1:\n",
    "print(f\"Cantidad de filas: {df_titanic_delta.count()}\") ## ‚¨ÖÔ∏è Cantidad de filas.\n",
    "\n",
    "#### üìù EJEMPLO 2:\n",
    "print(f\"Cantidad de columnas: {len([i for i in df_titanic_delta.columns])}\") ## ‚¨ÖÔ∏è Cantidad de columnas."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "07_Manipulacion_Datos_SparkSQL-Databricks",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

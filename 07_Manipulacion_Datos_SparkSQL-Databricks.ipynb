{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bdb171e4-c56f-4d12-89cd-a63089567db4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## MANIPULACI√ìN DE DATOS (SPARK SQL EN DATABRICKS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e73c2979-6912-4eb3-8c88-8372565878c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession # Puerta de entrada para trabajar con spark <-- SIEMPRE DEBEMOS IMPORTAR LA LLAVE MAESTRA QUE INICIA TODO.\n",
    "from pyspark.sql.functions import *  # Funciones propias del m√≥dulo SQL de Spark, para trabajar sobre Dataframes.\n",
    "\n",
    "spark = SparkSession.builder.appName(\"DatabricksSparkSQL\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "13c00770-281a-4559-93e1-ded0f0126c1c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### DATAFRAMES (Versi√≥n Databricks Free Edition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cb3fe8f8-cb2c-4c2e-ba4f-25a579e89cad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Link de [Databrick Free Edition](https://dbc-89f542f8-2df6.cloud.databricks.com/?o=758509963140561)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7c9c736d-562b-46a1-9da2-1af1e2bb35f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Para abordar el tema de **Manipulaci√≥n de Datos**, utilizaremos una de las principales herramientas que ofrece **Databricks: Spark SQL**.  \n",
    "Esta herramienta nos permite aplicar conceptos del lenguaje SQL sobre conjuntos de datos estructurados, ya sea que provengan directamente de archivos o est√©n gobernados a trav√©s del servicio **Unity Catalog**.\n",
    "\n",
    "Gracias a Spark SQL, podremos consultar, transformar y analizar datos de forma eficiente, aprovechando tanto la potencia del motor distribuido de Spark como la organizaci√≥n l√≥gica que brinda Unity Catalog.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "977fe4c5-7621-4161-ac95-b38d326688d9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### FASE 1. FUENTES DE DATOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "24241b3b-f046-4c91-9e20-b3c7610058ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#### ‚û°Ô∏è Como se ven√≠a explicando, Unity Catalog trabaja bajo una jerarqu√≠a de Cat√°lagos, Esquemas, Volumenes y Tablas.\n",
    "####     Por ende, es diferente la manera de acceder a su contenido.\n",
    "\n",
    "#====== üìå LEER ARCHIVO CSV \n",
    "##üìù SINTAXIS:\n",
    "#   dataset_nombre = spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").csv(\"/Volumes/NombreCatalago/NombreEsquema/NombreArchivo\")\n",
    "\n",
    "#---> spark.read : Permite leer un archivo basado en una ruta. Siempre instanciando primero SparkSession.\n",
    "#---> .option(\"header\",\"true||false\") : Permite establecer si tomar√° en cuenta los encabezados del dataset. true(mostrar) | false(ocultar)\n",
    "#---> .option(\"inferSchema\",\"true||false\") : Permite reconocer el tipo de dato de cada columna del dataset. true(mostrar) | false(ocultar)\n",
    "#---> .csv(RutaArchivoCSV) : Tipo de archivo a leer (En este caso un CSV).\n",
    "\n",
    "## üìù EJEMPLO:\n",
    "df_titanic_csv = spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").csv(\"/Volumes/explicacion_unity_catalog/ejemplos/csv_titanic\")\n",
    "# df_titanic_csv.show(5)\n",
    "\n",
    "#====== üìå LEER ARCHIVO PARQUET\n",
    "##üìù SINTAXIS:\n",
    "#   dataset_nombre = spark.read.parquet(\"/Volumes/NombreCatalago/NombreEsquema/NombreArchivoPARQUET\")\n",
    "\n",
    "#---> spark.read : Permite leer un archivo basado en una ruta. Siempre instanciando primero SparkSession.\n",
    "#---> .parquet(RutaArchivoPARQUET) : Tipo de archivo a leer (En este caso un PARQUET).\n",
    "\n",
    "## üìù EJEMPLO\n",
    "df_titanic_parquet = spark.read.parquet(\"/Volumes/explicacion_unity_catalog/ejemplos/parquet_titanic\")\n",
    "# df_titanic_parquet.show(5)\n",
    "\n",
    "#====== üìå LEER ARCHIVO DELTA\n",
    "## üìù SINTAXIS:\n",
    "#   dataset_nombre = spark.read.format(\"delta\").load(\"/Volumes/NombreCatalago/NombreEsquema/NombreArchivoDELTA\")\n",
    "\n",
    "#---> spark.read : Permite leer un archivo basado en una ruta. Siempre instanciando primero SparkSession.\n",
    "#---> .format(\"delta\") : stablece el tipo de archivo a leer (En este caso aplica siempre para DELTA)\n",
    "#---> .load(RutaArchivoDelta) : Aplica solo a los archivos DELTA.\n",
    "\n",
    "## üìù EJEMPLO:\n",
    "df_titanic_archivo_delta = spark.read.format(\"delta\").load(\"/Volumes/explicacion_unity_catalog/ejemplos/archivo_delta_titanic\")\n",
    "# df_titanic_archivo_delta.show(5)\n",
    "\n",
    "#====== üìå LEER TABLA DELTA (RECOMENDADO Y NATIVO EN DATABRICKS-UNITY-CATALOG)\n",
    "## üìù SINTAXIS:\n",
    "#   dataset_nombre = spark.sql(\"SELECT * FROM NombreCatalago.NombreEsquema.NombreTablaDelta\")\n",
    "\n",
    "#---> spark.sql() : Permite leer mediante lenguaje SQL una tabla delta. Siempre instanciando primero SparkSession.\n",
    "#---> \"NombreCatalago.NombreEsquema.NombreTablaDelta\" : Ruta aplicable solo para las tablas delta y gobernadas por Unity Catalog\n",
    "\n",
    "## üìù EJEMPLO\n",
    "df_titanic_delta = spark.sql(\"SELECT * FROM explicacion_unity_catalog.ejemplos.delta_titanic\")\n",
    "# df_titanic_delta.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7cd4a063-2337-43a2-bc32-7e739882d27a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### FASE 2. EXPLORACI√ìN INICIAL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "072cc5f7-fa2f-4206-959d-80b0914f90e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Utilizaremos de ejemplo el dataset de titanic ( en formato tabla delta y CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e253d9b-3aa8-43e0-b929-69872197e5f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#====== A). Quitar encabezados originales provenientes del Dataset (Archivos CSV).\n",
    "df_titanic_archivo_delta = spark.read.csv(\"/Volumes/explicacion_unity_catalog/ejemplos/csv_titanic\") ## ‚¨ÖÔ∏è Archivo CSV\n",
    "df_titanic_archivo_delta.show(3)\n",
    "#### üß†üí° Importante: Cuando leamos un archivo CSV, si no establecemos .option(\"header\",\"true\")\n",
    "####                   el dataframe mostrar√° las columnas con un prefijo c_ seguido de una posici√≥n asignada.\n",
    "####                   (Y los nombres de columnas ser√°n parte de los registros del dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "38f7d025-9d30-4880-9c32-74776c068d0d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#====== B). Mostrar los N primeros registros de un dataset.\n",
    "\n",
    "df_titanic_delta = spark.sql(\"SELECT * FROM explicacion_unity_catalog.ejemplos.delta_titanic\") ## ‚¨ÖÔ∏è Tabla delta\n",
    "\n",
    "#### üìù SINTAXIS: dataset_nombre.show(N√∫meroDeRegistros)\n",
    "\n",
    "#### üìù EJEMPLO 1:\n",
    "# df_titanic_delta.show(10) ## ‚¨ÖÔ∏è Mostrar los 10 primeros registros.\n",
    "\n",
    "#### üìù EJEMPLO 2:\n",
    "# df_titanic_delta.show(5) ## ‚¨ÖÔ∏è Mostrar los 5 primeros registros.\n",
    "\n",
    "#### üìù EJEMPLO 3:\n",
    "df_titanic_delta.show(3) ## ‚¨ÖÔ∏è Mostrar los 3 primeros registros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb5e834b-4a6d-4600-b862-da940d8ecb6e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#====== C). Mostrar los N √∫ltimos registros de un dataset (Retornado en una lista).\n",
    "\n",
    "df_titanic_delta = spark.sql(\"SELECT * FROM explicacion_unity_catalog.ejemplos.delta_titanic\") ## ‚¨ÖÔ∏è Tabla delta\n",
    "\n",
    "#### üìù SINTAXIS: dataset_nombre.tail(N√∫meroDeRegistros)\n",
    "\n",
    "#### üìù EJEMPLO 1:\n",
    "# df_titanic_delta.tail(10) ## ‚¨ÖÔ∏è Mostrar los 10 primeros √∫ltimos.\n",
    "\n",
    "#### üìù EJEMPLO 2:\n",
    "# df_titanic_delta.tail(5) ## ‚¨ÖÔ∏è Mostrar los 5 primeros √∫ltimos.\n",
    "\n",
    "#### üìù EJEMPLO 3:\n",
    "df_titanic_delta.tail(3) ## ‚¨ÖÔ∏è Mostrar los 3 primeros √∫ltimos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "04482e74-468a-4215-8dad-45de1268436e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#====== D). Mostrar el vol√∫men del dataset (Cantidad de filas y columnas respectivamente)\n",
    "\n",
    "df_titanic_delta = spark.sql(\"SELECT * FROM explicacion_unity_catalog.ejemplos.delta_titanic\") ## ‚¨ÖÔ∏è Tabla delta\n",
    "\n",
    "#### üìù SINTAXIS: \n",
    "#       dataset_nombre.count() ‚¨ÖÔ∏è Cantidad de filas\n",
    "#       len([i for i in dataframe_nombre.columns]) ‚¨ÖÔ∏è Cantidad de columnas\n",
    "\n",
    "#### üìù EJEMPLO 1:\n",
    "print(f\"Cantidad de filas: {df_titanic_delta.count()}\") ## ‚¨ÖÔ∏è Cantidad de filas.\n",
    "\n",
    "#### üìù EJEMPLO 2:\n",
    "print(f\"Cantidad de columnas: {len([i for i in df_titanic_delta.columns])}\") ## ‚¨ÖÔ∏è Cantidad de columnas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1fe9efb3-5cdf-4254-8128-9e29c0f4e0e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### FASE 3. EXPLORACI√ìN INICIAL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6c871f58-4bdd-457b-b93a-10e79d83f6b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###### FASE 3.1. DATOS CUALITATIVOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "842aed67-f394-4048-a525-c7fe4d848771",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    A). CONVERTIR A MAY√öSCULAS LOS DATOS CUALITATIVOS DE UNA COLUMNA EN UN DATASET\n",
    "    \n",
    "    üìù SINTAXIS:\n",
    "\n",
    "        dataframe_nombre = dataframe_nombre.withColumn(\n",
    "            \"NombreColumnaNueva\",\n",
    "            upper(col(\"NombreColumnaCualitativa\")) ‚¨ÖÔ∏è Funci√≥n upper() integrada en Spark SQL.\n",
    "        )\n",
    "    ### üß† En este caso, debemos almacenar en una variable los cambios a realizar en el dataframe\n",
    "    ### üí° Importante: Siempre .withColumn() trabajar√° con al menos una columna del dataset.\n",
    "\"\"\"\n",
    "\n",
    "# üí° EJEMPLO:\n",
    "df_titanic_delta = df_titanic_delta.withColumn(\n",
    "    \"embark_town\",\n",
    "    upper(col(\"embark_town\")) ## ‚¨ÖÔ∏è Convertimos a may√∫sculas los datos de la columna cualitativa \"\"embark_town\"\"\n",
    ")\n",
    "df_titanic_delta.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0100db6f-3ef4-4b75-91aa-160ef6ecdffa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    B). CONVERTIR A MIN√öSCULAS LOS DATOS CUALITATIVOS DE UNA COLUMNA EN UN DATASET\n",
    "    \n",
    "    üìù SINTAXIS:\n",
    "\n",
    "        dataframe_nombre = dataframe_nombre.withColumn(\n",
    "            \"NombreColumnaNueva\",\n",
    "            lower(col(\"NombreColumnaCualitativa\")) ‚¨ÖÔ∏è Funci√≥n lower() integrada en Spark SQL.\n",
    "        )\n",
    "    ### üß† En este caso, debemos almacenar en una variable los cambios a realizar en el dataframe\n",
    "    ### üí° Importante: Siempre .withColumn() trabajar√° con al menos una columna del dataset.\n",
    "\"\"\"\n",
    "\n",
    "# üí° EJEMPLO:\n",
    "df_titanic_delta = df_titanic_delta.withColumn(\n",
    "    \"embark_town\",\n",
    "    lower(col(\"embark_town\")) ## ‚¨ÖÔ∏è Convertimos a min√∫sculas los datos de la columna cualitativa \"\"embark_town\"\"\n",
    ")\n",
    "df_titanic_delta.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "daf6de6e-361c-4e50-beb5-1be9c1b90553",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    C). CONVERTIR A MAY√öSCULA LA PRIMERA LETRA DE CADA DATO CUALITATIVOS DE UNA COLUMNA EN UN DATASET\n",
    "    \n",
    "    üìù SINTAXIS:\n",
    "\n",
    "        dataframe_nombre = dataframe_nombre.withColumn(\n",
    "            \"NombreColumnaNueva\",\n",
    "            initcap(col(\"NombreColumnaCualitativa\")) ‚¨ÖÔ∏è Funci√≥n initcap() integrada en Spark SQL.\n",
    "        )\n",
    "    ### üß† En este caso, debemos almacenar en una variable los cambios a realizar en el dataframe\n",
    "    ### üí° Importante: Siempre .withColumn() trabajar√° con al menos una columna del dataset.\n",
    "\"\"\"\n",
    "\n",
    "# üí° EJEMPLO:\n",
    "df_titanic_delta = df_titanic_delta.withColumn(\n",
    "    \"embark_town\",\n",
    "    initcap(col(\"embark_town\")) ## ‚¨ÖÔ∏è Convertimos la primera letra en may√∫scula de cada dato en la columna cualitativa \"\"embark_town\"\"\n",
    ")\n",
    "df_titanic_delta.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58aab99e-657c-4821-8846-5c42ba6762b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    D). EXTRAER DE CADA DATO CUALITATIVO UNA CADENA EN ESPEC√çFICO MEDIANTE >>EXPRESIONES REGULARES<< EN UN DATASET\n",
    "    \n",
    "    üìù SINTAXIS:\n",
    "\n",
    "        dataframe_nombre = dataframe_nombre.withColumn(\n",
    "            \"NombreColumnaNueva\",\n",
    "                regexp_extract(col(\"embark_town\"),r'([A-Z])',1) ‚¨ÖÔ∏è Funci√≥n regexp_extract() integrada en Spark SQL.\n",
    "        )\n",
    "    ### üß† En este caso, debemos almacenar en una variable los cambios a realizar en el dataframe\n",
    "    ### üí° Importante: Siempre .withColumn() trabajar√° con al menos una columna del dataset.\n",
    "\"\"\"\n",
    "\n",
    "# üí° EJEMPLO 1: ‚¨ÖÔ∏è Extraemos primera letra en may√∫scula \n",
    "# df_titanic_delta = df_titanic_delta.withColumn(\n",
    "#     \"PrimeraLetraEnMay√∫scula\",\n",
    "#     regexp_extract(col(\"embark_town\"),r'([A-Z])',1)\n",
    "# )\n",
    "# df_titanic_delta.show(3)\n",
    "\n",
    "# üí° EJEMPLO 2: ‚¨ÖÔ∏è Extraemos primera letra en min√∫scula \n",
    "# df_titanic_delta = df_titanic_delta.withColumn(\n",
    "#     \"PrimeraLetraEnMin√∫scula\",\n",
    "#     regexp_extract(col(\"embark_town\"),r'([a-z])',0)\n",
    "# )\n",
    "# df_titanic_delta.show(3)\n",
    "\n",
    "# üí° EJEMPLO 3: ‚¨ÖÔ∏è Retornamos todo despu√©s de una letra en may√∫scula \n",
    "# df_titanic_delta = df_titanic_delta.withColumn(\n",
    "#     \"TodoDespuesDeUnaMay√∫scula\",\n",
    "#     regexp_extract(col(\"embark_town\"),r'[A-Z]([a-z]+)', 1)\n",
    "# )\n",
    "# df_titanic_delta.show(3)\n",
    "\n",
    "# üí° EJEMPLO 4: ‚¨ÖÔ∏è Retornamos todo despu√©s de una letra en min√∫scula \n",
    "# df_titanic_delta = df_titanic_delta.withColumn(\n",
    "#     \"TodoDespuesDeUnaMin√∫scula\",\n",
    "#     regexp_extract(col(\"embark_town\"),r'[a-z]([a-z]+)', 1)\n",
    "# )\n",
    "# df_titanic_delta.show(3)\n",
    "\n",
    "# üí° EJEMPLO 5: ‚¨ÖÔ∏è Retornamos todo despu√©s de un n√∫mero\n",
    "# df_titanic_delta = df_titanic_delta.withColumn(\n",
    "#     \"TodoDespuesDeUnN√∫mero\",\n",
    "#     regexp_extract(col(\"embark_town\"),r'([0-9]+)', 1)\n",
    "# )\n",
    "# df_titanic_delta.show(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "187aeed1-2d0d-4205-9597-093aec868b27",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    E). REEMPLAZAR VALORES DE DATOS CUALITATIVOS MEDIANTE >>EXPRESIONES REGULARES<< EN UN DATASET\n",
    "    \n",
    "    üìù SINTAXIS:\n",
    "    \n",
    "    \n",
    "    ### üß† En este caso, debemos almacenar en una variable los cambios a realizar en el dataframe\n",
    "    ### üí° Importante: Siempre .withColumn() trabajar√° con al menos una columna del dataset.\n",
    "\"\"\"\n",
    "### DATASET DE PRUEBA PARA ESTE EJEMPLO\n",
    "datos_sucios = {\n",
    "    \"nombre\": [\n",
    "        \"  JUAN  p√©rez  \", \n",
    "        \"MAR√çA@@L√ìPEZ\", \n",
    "        \"Ana--Mart√≠nez\", \n",
    "        \"carlos_rodr√≠guez \", \n",
    "        \"Sof√≠a123 Garc√≠a\"\n",
    "    ],\n",
    "    \"categoria_producto\": [\n",
    "        \"ELECTR√ìNICA#1\", \n",
    "        \"ropa--Mujer\", \n",
    "        \"hogar_y_DECORACI√ìN\", \n",
    "        \"LIBROS@@\", \n",
    "        \"Juguetes  ni√±os\"\n",
    "    ],\n",
    "    \"codigo\": [\n",
    "        \"ID-0001\", \n",
    "        \"ID-0023\", \n",
    "        \"CL-1234\", \n",
    "        \"ID-abc123\", \n",
    "        \"id-9999\"\n",
    "    ]\n",
    "}\n",
    "## Dataset de prueba (sucio)\n",
    "df_test = spark.createDataFrame(data=list(zip(*datos_sucios.values())),schema=[\"nombre\",\"categoria_producto\",\"codigo\"])\n",
    "# df_test.show()\n",
    "\n",
    "## Dataset de prueba (limpio)\n",
    "df_test_clean = df_test \n",
    "# df_test_clean.show()\n",
    "\n",
    "## üí° EJEMPLO 1: ‚¨ÖÔ∏è Eliminar caract√©res especiales (@@,#,--,_)\n",
    "df_test_clean = df_test_clean.withColumns({\n",
    "    \"categoria_producto\":\n",
    "    regexp_replace(col(\"categoria_producto\"),r'[^A-Za-z0-9√Å√â√ç√ì√ö√°√©√≠√≥√∫√ë√±√ú√º]',\" \"),\n",
    "    \"nombre\":\n",
    "    regexp_replace(col(\"nombre\"),r'[^A-Za-z0-9√Å√â√ç√ì√ö√°√©√≠√≥√∫√ë√±√ú√º]',' ')\n",
    "})\n",
    "# df_test_clean.show()\n",
    "\n",
    "## üí° EJEMPLO 2: ‚¨ÖÔ∏è Normalizar espacios (Varios espacios en blanco a uno solo)\n",
    "df_test_clean = df_test_clean.withColumns({\n",
    "    \"categoria_producto\":\n",
    "    regexp_replace(col(\"categoria_producto\"),r'\\s+',\" \"),\n",
    "    \"nombre\":\n",
    "    regexp_replace(col(\"nombre\"),r'\\s+',' ')\n",
    "})\n",
    "# df_test_clean.show()\n",
    "\n",
    "## üí° EJEMPLO 3: ‚¨ÖÔ∏è Reemplazar guiones por espacios en blanco\n",
    "df_test_clean = df_test_clean.withColumn(\n",
    "    \"nombre\",\n",
    "    regexp_replace(col(\"nombre\"),r'[-_]',' ')\n",
    ")\n",
    "# df_test_clean.show()\n",
    "\n",
    "## üí° EJEMPLO 4: ‚¨ÖÔ∏è Eliminar prefijos (ID- || CL- || id- || Letras)\n",
    "df_test_clean = df_test_clean.withColumn(\n",
    "    \"codigo\",\n",
    "    regexp_replace(col(\"codigo\"),r'^ID-|^CL-|^id-|[a-zA-Z]','')\n",
    ")\n",
    "# df_test_clean.show()\n",
    "\n",
    "## üí° EJEMPLO 5: ‚¨ÖÔ∏è Eliminar n√∫meros dentro de cadenas\n",
    "df_test_clean = df_test_clean.withColumns({\n",
    "    \"nombre\":\n",
    "    regexp_replace(initcap(col(\"nombre\")),r'[0-9]',''),\n",
    "    \"categoria_producto\":\n",
    "    regexp_replace(initcap(col(\"categoria_producto\")),r'[0-9]','')\n",
    "})\n",
    "df_test_clean.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "21c1932b-9e89-48d4-ad57-b697d4ba4a93",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###### FASE 3.2. DATOS CUANTITATIVOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e310c968-7c57-4567-ad5a-4db8645a06e5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_penguins = spark.sql(\"SELECT * FROM workspace.exercises.penguins\")\n",
    "df_penguins.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f80db7ef-7476-495b-b342-231f39348ab7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    A). EXPLORACI√ìN B√ÅSICA Y RESUMEN ESTAD√çSTICO\n",
    "        \n",
    "        üìù Para los datos cuantitativos podemos obtener su estad√≠stica b√°sica\n",
    "            mediante .decribe() el cu√°l retornar√° el valor de las columnas float o int.\n",
    "            En los campos cualitativos se visualizar√° informaci√≥n no cre√≠ble.\n",
    "\"\"\"\n",
    "df_penguins.describe().show()                            ## ‚¨ÖÔ∏è Estad√≠stica simple - Todas las columnas\n",
    "df_penguins.select(col(\"body_mass_g\")).describe().show() ## ‚¨ÖÔ∏è Estad√≠stica simple - Columna body_mass_g\n",
    "df_penguins.select(mean(col(\"body_mass_g\"))).show()      ## ‚¨ÖÔ∏è Media estad√≠stica - Columna body_mass_g\n",
    "df_penguins.select(median(col(\"body_mass_g\"))).show()    ## ‚¨ÖÔ∏è Mediana estad√≠stica - Columna body_mass_g\n",
    "df_penguins.select(min(col(\"body_mass_g\"))).show()       ## ‚¨ÖÔ∏è Valor m√≠nimo - Columna body_mass_g\n",
    "df_penguins.select(max(col(\"body_mass_g\"))).show()       ## ‚¨ÖÔ∏è Valor m√°ximo (IMC Gramos)\n",
    "df_penguins.select(sum(col(\"body_mass_g\"))).show()       ## ‚¨ÖÔ∏è Suma total - Columna body_mass_g\n",
    "df_penguins.select(stddev(col(\"body_mass_g\"))).show()    ## ‚¨ÖÔ∏è Desviaci√≥n est√°ndar - Columna body_mass_g\n",
    "df_penguins.select(variance(col(\"body_mass_g\"))).show()  ## ‚¨ÖÔ∏è Varianza - Columna body_mass_g\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e87e6cb-fcb6-40f1-851b-f2b4f95257ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    B). VERIFICAR VALORES NULOS EN LA(S) COLUMNA(AS) DEL DATASET\n",
    "\"\"\"\n",
    "\n",
    "## üí° EJEMPLO 1: VERIFICAR CANTIDAD DE VALORES NULOS EN TODAS LAS COLUMNAS DEL DATASET\n",
    "## ‚¨ÖÔ∏è Retorna la cantidad de datos nulos por columna.\n",
    "cantidad_nulos = df_penguins.select([\n",
    "    sum(when(col(i).isNull(),1).otherwise(0)).alias(i)\n",
    "    for i in df_penguins.columns\n",
    "])\n",
    "# cantidad_nulos.show()\n",
    "\n",
    "## üí° EJEMPLO 2: VERIFICAR CANTIDAD DE VALORES NULOS EN UNA COLUMNA ESPEC√çFICA DEL DATASET\n",
    "##‚¨ÖÔ∏è Retorna la cantidad de datos nulos de una columna.\n",
    "cantidad_nulos_bmg = df_penguins.select([\n",
    "    sum(when(col(i).isNull(),1).otherwise(0)).alias(i)\n",
    "    for i in df_penguins.columns\n",
    "    if i==\"body_mass_g\"\n",
    "])\n",
    "cantidad_nulos_bmg.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b1bb0d4f-accc-4fbe-8706-93ab70f10eaa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    C). FILTRANDO VALORES EN COLUMNAS CUANTITATIVAS\n",
    "\"\"\"\n",
    "\n",
    "print(\"Total de datos registrados: \",df_penguins.count()) ## ‚¨ÖÔ∏è Cantidad de datos del dataset : 344\n",
    "\n",
    "## üí° EJEMPLO 1: FILTRAR VALORES DE COLUMNA \"body_mass_g\" MAYOR A 3000\n",
    "df_penguins_bmg_mayor_3000 = df_penguins.filter(\n",
    "    col(\"body_mass_g\")>3000\n",
    ")\n",
    "# df_penguins_bmg_mayor_3000.show(3)\n",
    "print(\"Total de datos filtro 1: \",df_penguins_bmg_mayor_3000.count()) ## ‚¨ÖÔ∏è Cantidad de datos del dataset : 331\n",
    "\n",
    "## üí° EJEMPLO 2: FILTRAR VALORES DE COLUMNA \"body_mass_g\" ENTRE 1000 y 3000\n",
    "df_penguins_bmg_entre_1000_3000 = df_penguins.filter(\n",
    "    (col(\"body_mass_g\")>=1000) &\n",
    "    (col(\"body_mass_G\")<=3000)\n",
    ")\n",
    "# df_penguins_bmg_entre_1000_3000.show(3)\n",
    "print(\"Total de datos filtro 2: \",df_penguins_bmg_entre_1000_3000.count()) ## ‚¨ÖÔ∏è Cantidad de datos del dataset : 11\n",
    "\n",
    "## üí° EJEMPLO 3: FILTRAR VALORES NULOS EN COLUMNA \"body_mass_g\"\n",
    "df_penguins_nulos_body_mass_g = df_penguins.filter(\n",
    "    col(\"body_mass_g\").isNull()\n",
    ")\n",
    "# df_penguins_nulos_body_mass_g.show(3)\n",
    "print(\"Total de datos filtro 3: \",df_penguins_nulos_body_mass_g.count()) ## ‚¨ÖÔ∏è Cantidad de datos del dataset : 2\n",
    "\n",
    "## üí° EJEMPLO 4: FILTRAR VALORES NO NULOS EN COLUMNA \"body_mass_g\"\n",
    "df_penguins_no_nulos_body_mass_g = df_penguins.filter(\n",
    "    col(\"body_mass_g\").isNotNull()\n",
    ")\n",
    "# df_penguins_no_nulos_body_mass_g.show(3)\n",
    "print(\"Total de datos filtro 4: \",df_penguins_no_nulos_body_mass_g.count()) ## ‚¨ÖÔ∏è Cantidad de datos del dataset : 342\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5449ee6e-c753-4928-9c42-9dc485c6c3a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e261f97-fd33-4ee1-97f2-68d5d4c67e33",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    D). RELLENANDO DATOS Null DE LAS COLUMNAS DE UN DATASET\n",
    "    üìù SINTAXIS:\n",
    "        df_penguins = df_penguins.fillna(value=ValorARellenar,subset=NombreColumna)\n",
    "        ### üß† Importante: En este caso, los valores null deben rellenarse con datos\n",
    "                            del mismo tipo que la columna. Por ende, debemos llenar \n",
    "                            individualmante.\n",
    "\"\"\"\n",
    "### ‚û°Ô∏è Verificar datos nulos\n",
    "cantidad_nulos = df_penguins.select([\n",
    "    sum(when(col(i).isNull(),1).otherwise(0)).alias(i)\n",
    "    for i in df_penguins.columns\n",
    "])\n",
    "# cantidad_nulos.show()\n",
    "\n",
    "### ‚û°Ô∏è Verificar datos nulos que han sido rellenados.\n",
    "df_penguins = df_penguins.fillna(value=15.55,subset=[\"body_mass_g\"])\n",
    "# df_penguins.show()\n",
    "\n",
    "cantidad_nulos.show() ##  ‚úÖDatos nulos rellanados correctamente.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ca1e525f-40db-4358-bcff-570cd6d4fb79",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### FASE 4. AGRUPAMIENTO DE INFORMACI√ìN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b836edac-a8cf-4b22-b47a-abd9201f42ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "El agrupamiento de la informaci√≥n nos brinda el resumen de la misma proveniente \n",
    "de un dataset, logrando encontrar ciertos patrones en cada grupo de informaci√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3864eede-5013-4d89-8a75-06d9d7ff4aac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    üìù SINTAXIS:\n",
    "\n",
    "        dataset_nuevo_agrupamiento= dataset_nombre.groupBy([\"NombreColumna1\",\"NombreColumna2]).agg(\n",
    "            funci√≥nAgregaci√≥n(col(\"NombreColumna\")).alias(\"NuevoNombreColumna\")\n",
    "        )\n",
    "        \n",
    "        ### üß† En este caso, debemos almacenar en una variable los cambios a realizar en el dataframe.\n",
    "\n",
    "‚ùé En el agrupamiento de informaci√≥n, podemos utilizar diversas funciones de agregaci√≥n, tales como:\n",
    "\n",
    "üí°.min():    ‚¨ÖÔ∏è Permite obtener el m√≠nimo valor de la informaci√≥n agrupada.\n",
    "üí°.max():    ‚¨ÖÔ∏è Permite obtener el m√°ximo valor de la informaci√≥n agrupada.\n",
    "üí°.sum():    ‚¨ÖÔ∏è Permite sumar la informaci√≥n de una columna cuantitativa por la informaci√≥n agrupada.\n",
    "üí°.count():  ‚¨ÖÔ∏è Permite contar la cantidad de informaci√≥n de una columna por la informaci√≥n agrupada.\n",
    "üí°.mean():   ‚¨ÖÔ∏è Permite obtener la media de una columna cuantitativa por la informaci√≥n agrupada.\n",
    "üí°.median(): ‚¨ÖÔ∏è Permite obtener la mediana de una columna cuantitativa por la informaci√≥n agrupada.\n",
    "\"\"\"\n",
    "###‚úîÔ∏è Usaremos el dataset de \"penguins\"\n",
    "df_penguins = spark.sql(\"SELECT * FROM workspace.exercises.penguins\") ## ‚¨ÖÔ∏è Tabla Delta\n",
    "# df_penguins.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4bc7f87d-8b2f-401f-b435-4ae6193f6c62",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## üí° EJEMPLO 1: (AGRUPANDO POR UNA COLUMNA) \n",
    "df_agrupado_uno = df_penguins.groupBy([\"species\"]).agg(\n",
    "    round(mean(col(\"body_mass_g\")),2).alias(\"PromedioBMG\") ## ‚¨ÖÔ∏è Hallamos la media de la columna \"body_mass_g\"\n",
    ")\n",
    "# df_agrupado_uno.show()\n",
    "\n",
    "## üí° EJEMPLO 2: (AGRUPAR POR DOS COLUMNAS)\n",
    "df_agrupado_dos = df_penguins.groupBy([\"species\",\"sex\"]).agg(\n",
    "    round(mean(col(\"body_mass_g\")),2).alias(\"Species_Sex_PromedioBMG\")\n",
    ")\n",
    "# df_agrupado_dos.show()\n",
    "\n",
    "## üí° EJEMPLO 3: (AGRUPAR LA INFORMACI√ìN POR LA CANTIDAD DE LA MISMA COLUMNA)\n",
    "\n",
    "df_agrupado_tres = df_penguins.groupBy([\"species\"]).agg(\n",
    "    count(col(\"species\")).alias(\"Cantidad\") ## ‚¨ÖÔ∏è .count() : funci√≥n clave para traer la cantidad de datos de la columna.\n",
    ")\n",
    "df_agrupado_tres.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8a3abce2-23a7-4f5c-94cf-ec1acf9e3e0a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "07_Manipulacion_Datos_SparkSQL-Databricks",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

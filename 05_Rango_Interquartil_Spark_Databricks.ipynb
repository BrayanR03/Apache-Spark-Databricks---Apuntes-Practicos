{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "68d2b41a-218c-47c8-b9e2-e339dd6f5b96",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession # Puerta de entrada para trabajar con spark <-- SIEMPRE DEBEMOS IMPORTAR LA LLAVE MAESTRA QUE INICIA TODO.\n",
    "from pyspark.sql.functions import *  # Funciones propias del módulo SQL de Spark, para trabajar sobre Dataframes.\n",
    "from pyspark.sql.types import *\n",
    "spark = SparkSession.builder.appName(\"RangoInterquartilApacheSparkDatabricks\").getOrCreate() \n",
    "\"\"\"\n",
    "^          ^__________^        ^_________^                               ^\n",
    "|                |                   |                                   | \n",
    "Variable   Constructor de Sesión   Nombre Aplicación       Evita conflicto del SparkSession\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0e48cd00-4a67-47a4-8df3-c97aa7118c2c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### DATAFRAMES (Versión Databricks Free Edition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d262d08b-a9cf-40c2-9b77-16a578f6b44c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Link de [Databrick Free Edition](https://dbc-89f542f8-2df6.cloud.databricks.com/?o=758509963140561)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b58b13d9-8196-4fa7-b6d2-d01292f010af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### VALORES ATÍPICOS Y NO ATÍPICOS (RANGO INTERQUARTIL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d42d249b-612e-4373-a28a-eebdea9056c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 🔍 Análisis de valores atípicos usando el rango intercuartil (IQR)\n",
    "# Dataset utilizado: 🐧 Penguins (cargado previamente como Data Table en Unity Catalog)\n",
    "# ⚠️ Próximamente explicaré cómo cargar datasets al Unity Catalog en Databricks Free Edition...\n",
    "\n",
    "df_penguins = spark.sql(\"SELECT * FROM workspace.exercises.penguins\") #📊LENGUAJE SQL¿?....\n",
    "# df_penguins.show()\n",
    "\n",
    "# ESTADÍSTICA DESCRIPTIVA BÁSICA DE UN DATASET (PENGUINS)\n",
    "# df_penguins.describe().show()\n",
    "\n",
    "# Rango Interquartil (IQR)\n",
    "# Permite hallar valores atípicos dentro de nuestro dataset.\n",
    "# 💡Rango interquartil aplicado a la columna body_mass_g\n",
    "quartiles_body_mass_g = df_penguins.agg(\n",
    "expr('percentile(body_mass_g,array(0.25))')[0].alias(\"q1\"), # 💡Importante: expr() viene integrado dentro\n",
    "expr('percentile(body_mass_g,array(0.50))')[0].alias(\"q2\"), #    de las funciones sql en pyspark.\n",
    "expr('percentile(body_mass_g,array(0.75))')[0].alias(\"q3\"), # 💡 Cada cuartil es una porción del dataset completo\n",
    "expr('percentile(body_mass_g,array(1.00))')[0].alias(\"q4\")  #    (0.25,0.50,0.75,1) \n",
    ")\n",
    "# quartiles_body_mass_g.show()\n",
    "\n",
    "q1 = quartiles_body_mass_g[[\"q1\"]].first() # Quartil 1 ➡️ método .first() trae el primer valor de la columna\n",
    "## print(q1.q1) # Accede al valor de la columna\n",
    "q3 = quartiles_body_mass_g[[\"q3\"]].first() # Quartil 3 ➡️ método .first() trae el primer valor de la columna\n",
    "## print(q3.q3)# Accede al valor de la columna\n",
    "rango_iqr = q3.q3-q1.q1 # Cálculo de rango intercuartil\n",
    "## print(rango_iqr)\n",
    "lower_bound_body = q1.q1 - 1.5 *rango_iqr # Límite superior\n",
    "upper_bound_body = q3.q3 + 1.5 * rango_iqr # Límite inferior\n",
    "## print(lower_bound_body)\n",
    "## print(upper_bound_body)\n",
    "df_penguins_atipicos = df_penguins.filter(  \n",
    "    (col(\"body_mass_g\")<lower_bound_body) |\n",
    "    (col(\"body_mass_g\")>upper_bound_body)\n",
    ")# ⬅️ Filtramos los valores atípicos (fuera de los umbrales calculados en los límites superior e inferior)\n",
    "## df_penguins_atipicos.show()\n",
    "df_penguins_normales = df_penguins.filter(\n",
    "    (col(\"body_mass_g\")>=lower_bound_body) &\n",
    "    (col(\"body_mass_g\")<=upper_bound_body)\n",
    ")# ⬅️ Filtramos los valores limpios(dentro de los umbrales calculados en los límites superior e inferior)\n",
    "df_penguins_normales.show() # Dataset limpio"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "05_Rango_Interquartil_Spark_Databricks",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

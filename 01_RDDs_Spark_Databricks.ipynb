{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7afd87c5-3661-405d-9d0b-3da4a600541e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession # Puerta de entrada para trabajar con spark <-- SIEMPRE DEBEMOS IMPORTAR LA LLAVE MAESTRA QUE INICIA TODO.\n",
    "from pyspark.sql.functions import *  # Funciones propias del módulo SQL de Spark, para trabajar sobre Dataframes.\n",
    "from pyspark.sql.types import *\n",
    "spark = SparkSession.builder.appName(\"RDDsApacheSparkDatabricks\").getOrCreate() \n",
    "\"\"\"\n",
    "^          ^__________^        ^_________^                               ^\n",
    "|                |                   |                                   | \n",
    "Variable   Constructor de Sesión   Nombre Aplicación       Evita conflicto del SparkSession\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "428bf576-688b-4a30-8c33-1daa8b71b76a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### RDDs (Versión Databricks Community Edition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4f0df90c-f53b-4a36-bfc9-67fd8b99526f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Link de Databrick Community (Solo para el ejercicio de RDDs): [Databrick Community](https://community.cloud.databricks.com/?o=2956217321084781)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fa2a3466-24b5-454d-9058-a20cbae2f797",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Un RDD es una colección distribuida e inmutable de objetos, que se puede procesar en paralelo a través de un clúster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "64c7617e-697b-4a60-8bf3-124dccb247c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### EXPLICACIÓN RDDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1cddafcd-6621-4735-a6fd-d83fe7a4ab0a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "sc = spark.sparkContext # INSTANCIAMOS SPARK CONTEXT PARA EL USO DE RDDs\n",
    "# Crear RDD desde lista\n",
    "lista_rdd_final = sc.parallelize([1, 2, 3, 4, 5]) #<-- Usamos .parallelize() para definir SOLO RDDs\n",
    "print(lista_rdd_final.collect()) #<-- Al imprimir una lista, siempre haremos referencia a collect()\n",
    "\n",
    "# # DICCIONARIOS EN SPARK - USANDO RDDS --> parallelize()\n",
    "diccionario = spark.sparkContext.parallelize({\"Nombre\":\"Brayan\",\"Edad\":21}.items())\n",
    "print(diccionario.collect())\n",
    "\n",
    "# # # SETS EN SPARK - USANDO RDDS --> parallelize()\n",
    "sets = spark.sparkContext.parallelize({1,2,3,3,4,4,5})\n",
    "print(sets.collect())\n",
    "\n",
    "# # # TUPLAS EN SPARK - USANDO RDDS --> parallelize()\n",
    "tupla = spark.sparkContext.parallelize((1,2,3,4,5,\"BRAYAN\"))\n",
    "print(tupla.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e40bb737-31bc-485f-8a04-caca5fecdd57",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### FUNCIONES DE ACCIONES RDDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9418274a-39a7-4c4c-a998-b01b338bc627",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# FUNCIONES DE ACCIONES DE RDDs:\n",
    "# **Tomaremos de referencia la lista definida:\n",
    "print(lista_rdd_final.collect()) # Retorna todos los elementos del RDD\n",
    "print(lista_rdd_final.count()) # Retorna la cantidad de elementos del RDD\n",
    "print(lista_rdd_final.first()) # Retorna el primer elemento del RDD\n",
    "print(lista_rdd_final.take(3)) # Retorna los N primeros elementos del RDD\n",
    "\n",
    "# FUNCIONES DE TRANSFORMACIONES DE RDDs:\n",
    "# ** Tomaremos de referencia la lisa definida:\n",
    "\n",
    "# *FILTER\n",
    "print(lista_rdd_final.filter(lambda x:x%2==0).collect()) # Filtrar elementos del RDD basados en una función\n",
    "\n",
    "# *MAP\n",
    "print(lista_rdd_final.map(lambda x:x+2).collect()) # Retorna misma cantidad de elementos del RDD, pero, modificados.\n",
    "\n",
    "# *FLATMAP\n",
    "texto = spark.sparkContext.parallelize([\"Hola como estas?\",\"Espero muy bien\"])\n",
    "print(texto.flatMap(lambda x: x.split(\" \")).collect()) # Retorna una cantidad diferente de elementos del RDD y modificados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2ee6a496-50c1-4d88-a1af-07ad22d06987",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### FUNCIONES DE AGRUPACIÓN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "925abf4e-42cf-4745-9255-cd1c1461d464",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# FUNCIONES DE AGRUPACIÓN:            \n",
    "# * REDUCE\n",
    "print(lista_rdd_final.reduce(lambda x,y:x+y)) # Función que permite reducir a la mínima expresión.\n",
    "\n",
    "# * REDUCE BY KEY\n",
    "ventas = [\n",
    "    (\"Manzana\",10),\n",
    "    (\"Pera\",15),\n",
    "    (\"Manzana\",5),\n",
    "    (\"Platano\",50)\n",
    "]\n",
    "diccionario_rdd = spark.sparkContext.parallelize(ventas)\n",
    "print(diccionario_rdd.reduceByKey(lambda x,y:x+y).collect()) # Permite retornar datos agrupados"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "01_RDDs_Spark_Databricks",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

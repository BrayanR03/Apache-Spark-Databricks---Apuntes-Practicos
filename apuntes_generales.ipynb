{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "805ef10c-0efd-445d-95ca-5bee03de707b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession # Puerta de entrada para trabajar con spark <-- SIEMPRE DEBEMOS IMPORTAR LA LLAVE MAESTRA QUE INICIA TODO.\n",
    "from pyspark.sql.functions import *  # Funciones propias del m√≥dulo SQL de Spark, para trabajar sobre Dataframes.\n",
    "from pyspark.sql.types import *\n",
    "spark = SparkSession.builder.appName(\"ApacheSparkDatabricksApuntes\").getOrCreate() \n",
    "\"\"\"\n",
    "^          ^__________^        ^_________^                               ^\n",
    "|                |                   |                                   | \n",
    "Variable   Constructor de Sesi√≥n   Nombre Aplicaci√≥n       Evita conflicto del SparkSession\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "445a9068-289e-4d60-b0e7-c3169093dfe2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### RDDs (Versi√≥n Databricks Community Edition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "881fd059-2f9f-43ce-98bd-4a35e8e080e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Link de Databrick Community (Solo para el ejercicio de RDDs): [Databrick Community](https://community.cloud.databricks.com/?o=2956217321084781)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7e7b95f0-310f-403d-8ae7-986d1bbdcc90",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### EXPLICACI√ìN RDDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7dfdda75-6127-4f19-a1d4-c6293b7fd06e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "sc = spark.sparkContext # INSTANCIAMOS SPARK CONTEXT PARA EL USO DE RDDs\n",
    "# Crear RDD desde lista\n",
    "lista_rdd_final = sc.parallelize([1, 2, 3, 4, 5]) #<-- Usamos .parallelize() para definir SOLO RDDs\n",
    "print(lista_rdd_final.collect()) #<-- Al imprimir una lista, siempre haremos referencia a collect()\n",
    "\n",
    "# # DICCIONARIOS EN SPARK - USANDO RDDS --> parallelize()\n",
    "diccionario = spark.sparkContext.parallelize({\"Nombre\":\"Brayan\",\"Edad\":21}.items())\n",
    "print(diccionario.collect())\n",
    "\n",
    "# # # SETS EN SPARK - USANDO RDDS --> parallelize()\n",
    "sets = spark.sparkContext.parallelize({1,2,3,3,4,4,5})\n",
    "print(sets.collect())\n",
    "\n",
    "# # # TUPLAS EN SPARK - USANDO RDDS --> parallelize()\n",
    "tupla = spark.sparkContext.parallelize((1,2,3,4,5,\"BRAYAN\"))\n",
    "print(tupla.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "56ca5e31-7345-43b0-b2c7-317a8c97c20e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# FUNCIONES DE ACCIONES DE RDDs:\n",
    "# **Tomaremos de referencia la lista definida:\n",
    "print(lista_rdd_final.collect()) # Retorna todos los elementos del RDD\n",
    "print(lista_rdd_final.count()) # Retorna la cantidad de elementos del RDD\n",
    "print(lista_rdd_final.first()) # Retorna el primer elemento del RDD\n",
    "print(lista_rdd_final.take(3)) # Retorna los N primeros elementos del RDD\n",
    "\n",
    "# FUNCIONES DE TRANSFORMACIONES DE RDDs:\n",
    "# ** Tomaremos de referencia la lisa definida:\n",
    "\n",
    "# *FILTER\n",
    "print(lista_rdd_final.filter(lambda x:x%2==0).collect()) # Filtrar elementos del RDD basados en una funci√≥n\n",
    "\n",
    "# *MAP\n",
    "print(lista_rdd_final.map(lambda x:x+2).collect()) # Retorna misma cantidad de elementos del RDD, pero, modificados.\n",
    "\n",
    "# *FLATMAP\n",
    "texto = spark.sparkContext.parallelize([\"Hola como estas?\",\"Espero muy bien\"])\n",
    "print(texto.flatMap(lambda x: x.split(\" \")).collect()) # Retorna una cantidad diferente de elementos del RDD y modificados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2c9fd874-ea35-4e89-8dd8-036b376bbb4b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# FUNCIONES DE AGRUPACI√ìN:            \n",
    "# * REDUCE\n",
    "print(lista_rdd_final.reduce(lambda x,y:x+y)) # Funci√≥n que permite reducir a la m√≠nima expresi√≥n.\n",
    "\n",
    "# * REDUCE BY KEY\n",
    "ventas = [\n",
    "    (\"Manzana\",10),\n",
    "    (\"Pera\",15),\n",
    "    (\"Manzana\",5),\n",
    "    (\"Platano\",50)\n",
    "]\n",
    "diccionario_rdd = spark.sparkContext.parallelize(ventas)\n",
    "print(diccionario_rdd.reduceByKey(lambda x,y:x+y).collect()) # Permite retornar datos agrupados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0e0a4bde-9df9-454e-ba36-34b107679411",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### EJERICICIOS RDDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e39f34f1-e929-4713-84cb-46d29ef3745b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# EJERCICIOS B√ÅSICOS DE APACHE SPARK - RDDs\n",
    "\n",
    "# 1. \n",
    "dias_semana = [\"Lunes\", \"Martes\", \"Mi√©rcoles\", \"Jueves\", \"Viernes\", \"S√°bado\", \"Domingo\"]\n",
    "rdd_dias_semana = spark.sparkContext.parallelize(dias_semana)\n",
    "print(rdd_dias_semana.collect())\n",
    "\n",
    "# 2.\n",
    "numeros = [1,2,3,4,5,6,7,8,9,10]\n",
    "rdd_numeros = spark.sparkContext.parallelize(numeros)\n",
    "print(rdd_numeros.count())\n",
    "\n",
    "# 3. \n",
    "ciudades = [\"Paris\",\"Shangai\",\"Roma\",\"Buenos Aires\"]\n",
    "rdd_ciudades = spark.sparkContext.parallelize(ciudades)\n",
    "print(rdd_ciudades.first())\n",
    "\n",
    "# 4.\n",
    "numeros = [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20]\n",
    "numeros_rdd = spark.sparkContext.parallelize(numeros)\n",
    "print(numeros_rdd.take(5))\n",
    "\n",
    "# 5.\n",
    "numeros = [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20]\n",
    "rdd_numeros = spark.sparkContext.parallelize(numeros)\n",
    "print(rdd_numeros.filter(lambda x:x%2==0).collect())\n",
    "\n",
    "# 6.\n",
    "numeros = [1,2,3,4,5]\n",
    "rdd_numeros = spark.sparkContext.parallelize(numeros)\n",
    "rdd_cuadrados = rdd_numeros.map(lambda x:x**2)\n",
    "print(rdd_cuadrados.collect()) \n",
    "\n",
    "# 7.\n",
    "frases = [\"Hola mundo\", \"Spark es genial\", \"RDDs son poderosos\"]\n",
    "rdd_frases = spark.sparkContext.parallelize(frases)\n",
    "print(rdd_frases.flatMap(lambda x:x.split(\" \")).collect())\n",
    "\n",
    "# 8.\n",
    "numeros = [1,2,3,4,5,6,7,8,9,10]\n",
    "rdd_numeros = spark.sparkContext.parallelize(numeros)\n",
    "print(rdd_numeros.reduce(lambda x,y:x+y))\n",
    "\n",
    "# 9.\n",
    "lista_tuplas = [(\"a\", 1), (\"b\", 2), (\"a\", 3), (\"b\", 4), (\"c\", 5)]\n",
    "rdd_lista_tuplas = spark.sparkContext.parallelize(lista_tuplas)\n",
    "print(rdd_lista_tuplas.reduceByKey(lambda x,y:x+y).collect())\n",
    "\n",
    "# 10.\n",
    "frases = [\"Hola mundo\", \"Spark es genial\", \"RDDs son poderosos\"]\n",
    "rdd_frases = spark.sparkContext.parallelize(frases)\n",
    "flat_map_frases = rdd_frases.flatMap(lambda x:x.split(\" \"))\n",
    "print(f\"Cantidad de  palabras mayor a 4 letras: {flat_map_frases.filter(lambda x:len(x)>4).count()}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a3299fe1-9128-4854-9958-e62191974a32",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### DATAFRAMES (Versi√≥n Databricks Free Edition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5cef166a-1485-4da8-bc17-1aa239298bd4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Link de [Databrick Free Edition](https://dbc-89f542f8-2df6.cloud.databricks.com/?o=758509963140561)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c0282fae-58d1-4fde-9b9b-bb6f81e0f595",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### DEFINICI√ìN DE UN DATAFRAMES EN DATABRICKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "50773efc-88ac-4628-8c87-bbe4f1417e92",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1Ô∏è‚É£ DEFINIR DATAFRAMES DESDE CERO EN APACHE SPARK - DATABRICKS:\n",
    "\n",
    "    #‚ú¥Ô∏è Par√°metros de createDataframe(): data= *Datos en formato de tupla* y schema= *Nombre de las columnas*\n",
    "\n",
    "datos = [(\"Brayan\",21),(\"Rafael\",22)] # ‚¨ÖÔ∏è Lista de tuplas (Cada tupla es un registro en el Dataframe)\n",
    "df = spark.createDataFrame(data=datos,schema=[\"Nombre\",\"Edad\"]) #‚¨ÖÔ∏è Definimos el Dataframe (spark.createDataframe)\n",
    "# df.head()  ‚¨ÖÔ∏è Muestra el primer registro del Dataframe\n",
    "# df.show()   #‚¨ÖÔ∏è Muestra todo los registros del Dataframe (L√≠mite de 20 filas)\n",
    "# df.tail(3)  #‚¨ÖÔ∏è Muestra los 3 √∫ltimos registros del Dataframe en una sola fila.\n",
    "\n",
    "# 2Ô∏è‚É£ DEFINIR DATAFRAMES EN APACHE SPARK MEDIANTE UN DICCIONARIO:\n",
    "\n",
    "diccionario = {\"Nombre\":[\"Brayan\",\"Rafael\"],\"Edad\":[21,22]} # ‚¨ÖÔ∏è Diccionario\n",
    "df_dict = spark.createDataFrame(data=list(zip(*diccionario.values())),schema=list(diccionario.keys()))\n",
    "\"\"\"\n",
    "    Vamos descomponiendo y explicando todo lo que sucede en data de este dataframe.\n",
    "    Paso 1. diccionario.values(): Retorna todos los valores asignados a una llave en el diccionario. \n",
    "                                  ‚û°Ô∏è [\"Brayan\",\"Rafael\"];[21,22]\n",
    "    2. * : Este asterisco desempaqueta cada valor retornado del Paso 1. \n",
    "                                  ‚û°Ô∏è \"Brayan\",\"Rafael\"   ||   21,22\n",
    "    3. zip(): Unifica los valores desempaquetados del Paso 2. en una tupla.\n",
    "                                  ‚û°Ô∏è (\"Brayan\",\"Rafael\"),(21,22)\n",
    "    4. list(): Convierte cada tupla retornada del Paso 3. y lo asigan como un elemento ed una lista final.\n",
    "                                  ‚û°Ô∏è[(\"Brayan\",\"Rafael\"),(21,22)]\n",
    "    5. data: El par√°metro de createDataFrame() lo toma como valores correctamente organizados para ser parte de los registros del dataframe.                                 \n",
    "\"\"\" \n",
    "df_dict.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "14789894-4304-4d75-8f15-9f9fecd275f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### OPERACIONES B√ÅSICAS CON DATAFRAMES EN DATABRICKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3cc57571-b3c0-4171-9354-abec86a912f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Definici√≥n del dataframe\n",
    "diccionario = {\n",
    "  \"Nombre\":[\"Pepe\",\"Juan\",\"Pedro\",\"Carlos\",\"Raul\"],\n",
    "  \"Edad\":[20,25,26,24,21],\n",
    "  \"Nota\":[10,12,15,14,10]\n",
    "}\n",
    "# En caso los tipos de datos de cada columna sean variados,\n",
    "# los definimos aparte y luego lo integramos en createDataFrame(schema=**)\n",
    "from pyspark.sql.types import * ## Importamos esta librer√≠a para la creaci√≥n de estructuras desde cero.\n",
    "schema_dataframe = StructType([ #‚¨ÖÔ∏è M√©todo para definir nuevos tipos de estructuras, en este caso, columnas de un DF.\n",
    "  StructField(\"Nombre\",StringType(),True), #‚¨ÖÔ∏è M√©todo para definir columna: StructField(NombreColumna,TipoDato,True)\n",
    "  StructField(\"Edad\",LongType(),True),\n",
    "  StructField(\"Nota\",LongType(),True)\n",
    "])\n",
    "df_example = spark.createDataFrame(data=list(zip(*diccionario.values())),schema=schema_dataframe) \n",
    "df_example.show()\n",
    "\n",
    "#### Operaciones b√°sicas con dataframes\n",
    "##============== Obtener valor o valores de una columna en un Dataframe\n",
    "df_example[[\"Edad\"]].show() # Obtenemos el valor de una columna\n",
    "df_example[[\"Nombre\",\"Edad\"]].show() # Obtenemos valores de varias columnas\n",
    "\n",
    "##============== Fitrar valores en una columna espec√≠fica: m√©todo ‚û°Ô∏è.filter(col(NombreColumna)&|Condici√≥n)\n",
    "# Filtramos los registros con la edad mayor a 21...\n",
    "df_example.filter(col(\"Edad\")>21).show() # Mediante la funci√≥n col(NombreColumna), accedemos a la columna y sus filas. \n",
    "# Filtrar valores en dos columnas espec√≠ficas\n",
    "df_example.filter(\n",
    "  (col(\"Edad\")==25) & (col(\"Nombre\")=='Juan')\n",
    ").show() # La condici√≥n es separada en tuplas y con un operador l√≥gico.\n",
    "\n",
    "##============== Agregar nuevas columnas al Dataframe: m√©todo ‚û°Ô∏è.withColumn(NombreColumna,Condici√≥n/Valor)\n",
    "# Utilizaremos lit para valores literales y/o asignar valores\n",
    "df_example = df_example.withColumn( #‚¨ÖÔ∏è Agregamos una sola columna\n",
    "  \"Estado\", # Nuevo nombre de la columna\n",
    "  lit(\"Activo\") # Agregar el mismo valor para la N cantidad de filas\n",
    ") \n",
    "df_example.show()\n",
    "\n",
    "df_example = df_example.withColumns({ #‚¨ÖÔ∏è Agregamos varias columnas: m√©todo ‚û°Ô∏è.withColumns({NombreColumna:Condici√≥n/Valor})\n",
    "  \"Estado\": # Nuevo nombre de la columna 1\n",
    "  lit(\"Activo\"), # Agregar el mismo valor para la N cantidad de filas\n",
    "  \"Fecha Registro\": # Nuevo nombre de la columna 2\n",
    "  lit(\"2025-07-15\"), # Agregar el mismo valor para la N cantidad de filas\n",
    "}) \n",
    "df_example.show()\n",
    "\n",
    "##=============== Agregar nueva columna al Dataframe en base a una condici√≥n: m√©todo ‚û°Ô∏è.withColumn(NombreColumna,Condici√≥n)\n",
    "df_example = df_example.withColumn(\n",
    "  \"Categorid_Edad\",                                        # Nombre de nueva columna\n",
    "  when(col(\"Edad\")<18,lit(\"Menor\"))                        # Primera condici√≥n\n",
    "  .when((col(\"Edad\")>=18) & (col(\"Edad\")<25),lit(\"Joven\")) # Segunda condici√≥n\n",
    "  .otherwise(lit(\"Adulto\"))                                # √öltima condici√≥n\n",
    ")\n",
    "df_example.show()\n",
    "\n",
    "##=============== Seleccionamos una o m√°s columnas del dataframe: m√©todo ‚û°Ô∏è.select(col(NombreColumna))\n",
    "df_example = df_example.select(\n",
    "  col(\"Nombre\"),                   # Seleccionamos columna Nombre\n",
    "  col(\"Edad\"),                     # Seleccionamos columna Edad\n",
    ")\n",
    "df_example.show()\n",
    "\n",
    "##=============== Renombrar una columna de un DataFrame: \n",
    "# ‚û°Ô∏èm√©todo .withColumnRenamed(existing=NombreColumnaActual,new=NombreNuevoColumna) ||\n",
    "# ‚û°Ô∏èm√©todo .withColumnsRenamed({NombreAntiguoExistente1:NombreNuevoColumna1,NombreAntiguoExistente2:NombreNuevoColumna2}) \n",
    "\n",
    "df_example = df_example.withColumnRenamed(existing=\"Nombre\",new=\"Name\") #‚¨ÖÔ∏è Renombra columnas individuales\n",
    "df_example = df_example.withColumnsRenamed({\"Nombre\":\"Name\",\"Edad\":\"Age\"}) #‚¨ÖÔ∏è Renombra varias columnas\n",
    "# df_example = df_example.selectExpr(\"Nombre AS Name\",\"Edad as Age\") \n",
    "#‚¨ÖÔ∏è .selectExpr() renombra la o las columnas, sin embargo, selecciona esas renombradas y omite el resto (No recomendado) \n",
    "df_example.show()\n",
    "\n",
    "##=============== Eliminar columnas de un Dataframe: ‚û°Ô∏è m√©todo .drop(col(NombreColumna)) \n",
    "df_example = df_example.drop(col(\"Nota\")) # Ingresar nombre de columna a eliminar\n",
    "df_example.show()\n",
    "\n",
    "##=============== Ordenar Dataframe por una columna en espec√≠fico: ‚û°Ô∏è m√©todo .sort(col(NombreColumna).asc() || .desc())\n",
    "df_example.orderBy(col(\"Edad\").desc()).show() # Forma descendente por la columna Edad\n",
    "df_example.orderBy(col(\"Edad\").asc()).show() # Forma ascendente por la columna Edad\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "70610a79-7dce-4064-abec-a1ab6aee01d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### VALORES AT√çPICOS Y NO AT√çPICOS (RANGO INTERQUARTIL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d4f11899-ab9a-467f-bdde-40010b43bd0a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# üîç An√°lisis de valores at√≠picos usando el rango intercuartil (IQR)\n",
    "# Dataset utilizado: üêß Penguins (cargado previamente como Data Table en Unity Catalog)\n",
    "# ‚ö†Ô∏è Pr√≥ximamente explicar√© c√≥mo cargar datasets al Unity Catalog en Databricks Free Edition...\n",
    "\n",
    "df_penguins = spark.sql(\"SELECT * FROM workspace.exercises.penguins\") #üìäLENGUAJE SQL¬ø?....\n",
    "# df_penguins.show()\n",
    "\n",
    "# ESTAD√çSTICA DESCRIPTIVA B√ÅSICA DE UN DATASET (PENGUINS)\n",
    "# df_penguins.describe().show()\n",
    "\n",
    "# Rango Interquartil (IQR)\n",
    "# Permite hallar valores at√≠picos dentro de nuestro dataset.\n",
    "# üí°Rango interquartil aplicado a la columna body_mass_g\n",
    "quartiles_body_mass_g = df_penguins.agg(\n",
    "expr('percentile(body_mass_g,array(0.25))')[0].alias(\"q1\"), # üí°Importante: expr() viene integrado dentro\n",
    "expr('percentile(body_mass_g,array(0.50))')[0].alias(\"q2\"), #    de las funciones sql en pyspark.\n",
    "expr('percentile(body_mass_g,array(0.75))')[0].alias(\"q3\"), # üí° Cada cuartil es una porci√≥n del dataset completo\n",
    "expr('percentile(body_mass_g,array(1.00))')[0].alias(\"q4\")  #    (0.25,0.50,0.75,1) \n",
    ")\n",
    "# quartiles_body_mass_g.show()\n",
    "\n",
    "q1 = quartiles_body_mass_g[[\"q1\"]].first() # Quartil 1 ‚û°Ô∏è m√©todo .first() trae el primer valor de la columna\n",
    "## print(q1.q1) # Accede al valor de la columna\n",
    "q3 = quartiles_body_mass_g[[\"q3\"]].first() # Quartil 3 ‚û°Ô∏è m√©todo .first() trae el primer valor de la columna\n",
    "## print(q3.q3)# Accede al valor de la columna\n",
    "rango_iqr = q3.q3-q1.q1 # C√°lculo de rango intercuartil\n",
    "## print(rango_iqr)\n",
    "lower_bound_body = q1.q1 - 1.5 *rango_iqr # L√≠mite superior\n",
    "upper_bound_body = q3.q3 + 1.5 * rango_iqr # L√≠mite inferior\n",
    "## print(lower_bound_body)\n",
    "## print(upper_bound_body)\n",
    "df_penguins_atipicos = df_penguins.filter(  \n",
    "    (col(\"body_mass_g\")<lower_bound_body) |\n",
    "    (col(\"body_mass_g\")>upper_bound_body)\n",
    ")# ‚¨ÖÔ∏è Filtramos los valores at√≠picos (fuera de los umbrales calculados en los l√≠mites superior e inferior)\n",
    "## df_penguins_atipicos.show()\n",
    "df_penguins_normales = df_penguins.filter(\n",
    "    (col(\"body_mass_g\")>=lower_bound_body) &\n",
    "    (col(\"body_mass_g\")<=upper_bound_body)\n",
    ")# ‚¨ÖÔ∏è Filtramos los valores limpios(dentro de los umbrales calculados en los l√≠mites superior e inferior)\n",
    "df_penguins_normales.show() # Dataset limpio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f7dc4620-76e3-49d0-b788-75a9f71cfe56",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Explicaci√≥n de Cat√°lagos - Esquemas - Volumenes/Tablas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "96c2802f-80f4-405a-a1fc-d3f8ffd9e075",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### JERARQU√çA UNITY CATALOG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ce1fa1d7-24f8-42bc-9b1f-4bce23a44425",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "Crear el cat√°lago (carpeta o conjunto de carpetas que almacenar√° los archivos):\n",
    ">> Usando SQL EDITOR (SINTAXIS): \n",
    "    CREATE CATALOG IF NOT EXISTS nombre_catalago;\n",
    ">> Usando PySpark (SINTAXIS): \n",
    "    spark.sql(\"CREATE CATALOG IF NOT EXISTS nombre_catalago\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ce1ea131-9d5f-43a7-8224-b14b49f5b2ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "** Crear el esquema (agrupaci√≥n de archivos basado en el objetivo del proyecto):\n",
    ">> Usando SQL EDITOR (SINTAXIS): \n",
    "    CREATE SCHEMA IF NOT EXISTS nombre_catalago.NombreEsquema;\n",
    ">> Usando PySpark (SINTAXIS): \n",
    "    spark.sql(\"CREATE SCHEMA IF NOT EXISTS nombre_catalago.NombreEsquema;\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dcc74272-ecdd-4323-b279-4d5b7e613050",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "** Crear el volumen (contenedores de archivos f√≠sicos sin esquema tabular):\n",
    ">> Usando SQL EDITOR (SINTAXIS): \n",
    "    CREATE VOLUME IF NOT EXISTS nombre_catalago.nombre_esquema.NombreVolumen;\n",
    ">> Usando PySpark (SINTAXIS): \n",
    "    spark.sql(\"CREATE VOLUME IF NOT EXISTS nombre_catalago.nombre_esquema.NombreVolumen;\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ad66e3ab-4f66-43ac-8266-90af4b671a5c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### ALMACENAR OBJETOS EN UNITY CATALOG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ef644360-5a0b-4ab5-a224-53462b9cfbfa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# PATH PARA ACCEDER/ALMACENAR OBJETOS EN VOLUMENES (RECOMENDADO POR GOBERNANZA DE UNITY CATALOG): \n",
    "SINTAXIS:\n",
    "\"/Volumes/nombre_catalago/nombre_esquema/nombre_volumen\" \n",
    "EJEMPLO:\n",
    "\"/Volumes/workspace/exercises/volumen_dataframe\""
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "apuntes_generales",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
